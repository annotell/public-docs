---
id: zod-data
title: Upload Zod data
slug: /upload-zod-data
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

This tutorial will guide you trhough uploading different [scene types](docs/kognic-io/overview.md#different-types-of-scenes) using the zenseact open dataset.
The purpose of this page is to show you some of the steps that might be needed to convert recordings into Kognic scenes.
To follow along in this guide you need to download the data from [zenseact open dataset](https://zod.zenseact.com/download/).
The data should be structured like this:

```
zod
├── sequences
│   ├── 000000
│   ├── 000002
│   ├── ...
└── trainval-sequences-mini.json
```

<Tabs>
  <TabItem value="cameras-sequence" label="Cameras Sequence" default>
    Firsly we need to set up our KognicIOClient, if you have not set it up before checkout [quickstart](quickstart.mdx)
     ```python
    from kognic.io.client import KognicIOClient
    client = KognicIOClient()
    ```

    Next we need to get the Zod seqeunces
    ```python
    from zod.constants import Version as ZodVersion
    from zod.zod_sequences import ZodSequences

    zod_path = <absolute path to your zod folder>
    zod_version = ZodVersion.<The zod version you downloaded>
    sequences = ZodSequences(str(zod_path), version=zod_version)
    ```

    given that we have the frames it's very easy to create a camera sequence
    ```python
    id = sorted(sequences.get_all_ids())[1] # just creating 1 scene for the sake of the example
    # The external id is you human readble reference, it has to be unique within your organization/workspace?
    external_id = "sequences_" + id + "_" + str(uuid4())[:8]
    sequence = sequences[id]

    frames = convert_frames(zod_sequence, max_nr_frames)
    scene = CamerasSequence(external_id=external_id, frames=frames)
    created_scene = client.cameras_sequence.create(scene, dryrun=dryrun)
    scene_uuid = created_scene.scene_uuid if not dryrun else None
    print(f"Created scene with UUID: {scene_uuid}")
    ```

    Ok, but how do we convert the frames?
    For each frame we need to add all the sensors that we are intrested in. In this case we are only intrested in the FRONT camera.

    ```python
    from kognic.io.model.scene.cameras_sequence import Frame as CSFrame
    from zod.constants import Camera as ZodCamera
    from zod.zod_sequences import ZodSequenc

    def convert_frames(sequence: ZodSequence, max_nr_frames: int) -> list[CSFrame]:
        frames, seen_frame_timestamps = list(), set()
        zod_frames = sequence.info.get_camera_frames(camera=ZodCamera.FRONT)
        start_ts_ms = ns_to_ms(seconds_to_ns(zod_frames[0].time.timestamp()))

        for camera_frame in zod_frames[:max_nr_frames]:
            frame_ts_s = camera_frame.time.timestamp()

            if frame_ts_s in seen_frame_timestamps:
                continue

            seen_frame_timestamps.add(frame_ts_s)
            frame_ts_ns = seconds_to_ns(frame_ts_s)

            image = Image(
              filename=camera_frame.filepath,
              sensor_name=ZodCamera.FRONT,
              metadata=ImageMetadata(
                  shutter_time_start_ns=frame_ts_ns,
                  shutter_time_end_ns=frame_ts_ns + 1,
              )
            )

            frames.append(
              CSFrame(
                  relative_timestamp=ns_to_ms(frame_ts_ns) - start_ts_ms,
                  frame_id=str(frame_ts_ns),
                  images=[image]
              )
            )
        return frames
    ```

  </TabItem>
  <TabItem
    value="lidars-and-cameras-sequence"
    label="Lidars and Cameras Sequence"
    default
  >
   Firsly we need to set up our KognicIOClient, if you have not set it up before checkout [quickstart](quickstart.mdx)
  
    ```python
    from kognic.io.client import KognicIOClient
    client = KognicIOClient()
    ```

    Next we need to get a ZOD sequence
    ```python
    from zod.constants import Version as ZodVersion
    from zod.zod_sequences import ZodSequences

    zod_path = <absolute path to your zod folder>
    zod_version = ZodVersion.<The zod version you downloaded>
    sequences = ZodSequences(str(zod_path), version=zod_version)

    id = sorted(sequences.get_all_ids())[1] # just creating 1 scene for the sake of the example
    # The external id is you human readble reference, it has to be unique within your organization/workspace?
    external_id = "sequences_" + id + "_" + str(uuid4())[:8]
    sequence = sequences[id]
    ```

    When using both camera and lidar sensor we need calibrations to relate them to each other (Read more [here](/docs/kognic-io/calibrations/overview)).
    ZOD provides calibrations but, to use them we need to convert them to kognics format.
    In this example we only use the FRONT camera and the VELODYNE lidar.

    ```python
    from kognic.io.model.calibration import KannalaCalibration, LidarCalibration, Position, RotationQuaternion, SensorCalibration
    from kognic.io.model.calibration.camera.common import CameraMatrix
    from kognic.io.model.calibration.camera.kannala_calibration import KannalaDistortionCoefficients, UndistortionCoefficients

    from zod.constants import Camera as ZodCamera
    from zod.constants import Lidar as ZodLidar

    zod_calibration = sequence.calibration

    # Lidar calibration
    zod_lidar_calibration = zod_calibration.lidars[ZodLidar.VELODYNE]
    lidar_calib = LidarCalibration(
        position=Position(x=zod_lidar_calibration.extrinsics.translation[0], y=zod_lidar_calibration.extrinsics.translation[1], z=zod_lidar_calibration.extrinsics.translation[2]),
        rotation_quaternion=RotationQuaternion(w=zod_lidar_calibration.extrinsics.rotation.w, x=zod_lidar_calibration.extrinsics.rotation.x, y=zod_lidar_calibration.extrinsics.rotation.y, z=zod_lidar_calibration.extrinsics.rotation.z),
    )

    # Camera calibration
    zod_camera_calibration = zod_calibration.cameras[ZodCamera.FRONT]
    dist = zod_camera_calibration.distortion
    undist = zod_camera_calibration.undistortion
    intrinsics = zod_camera_calibration.intrinsics
    dist_coeff = KannalaDistortionCoefficients(k1=dist[0], k2=dist[1], p1=dist[2], p2=dist[3])
    undist_coeff = UndistortionCoefficients(l1=undist[0], l2=undist[1], l3=undist[2], l4=undist[3])
    camera_matrix = CameraMatrix(fx=intrinsics[0][0], fy=intrinsics[1][1], cx=intrinsics[0][2], cy=intrinsics[1][2])
    camera_calib = KannalaCalibration(
        position=Position(x=zod_camera_calibration.extrinsics.translation[0], y=zod_camera_calibration.extrinsics.translation[1], z=zod_camera_calibration.extrinsics.translation[2]),
        rotation_quaternion=RotationQuaternion(w=zod_camera_calibration.extrinsics.rotation.w, x=zod_camera_calibration.extrinsics.rotation.x, y=zod_camera_calibration.extrinsics.rotation.y, z=zod_camera_calibration.extrinsics.rotation.z),
        camera_matrix=camera_matrix,
        image_width=calibration.image_dimensions[0],
        image_height=calibration.image_dimensions[1],
        distortion_coefficients=dist_coeff,
        undistortion_coefficients=undist_coeff,
    )

    calibrations = {
        ZodLidar.VELODYNE: lidar_calib,
        ZodCamera.FRONT: camera_calib,
    }
    external_id = "zod_calibration_" + str(uuid4())[:8]
    sensor_calibrations = SensorCalibration(calibration=calibrations, external_id=external_id)
    # Note that you only need to create your calibration once so take note of you calibration id.
    created_calibration = client.calibration.create_calibration(calibration)
    print(f"calibration id: {calibration.id}")
    ```

    Next we need to create the frames, each frame conists of a list of lidars and a list of cameras, where each element in the list is a sensor at that point in time.

    Before we can add the zod data to a Kognic frame we need to convert it so that it matches our format.

    To convert a lidar we need this function, the main thing to note here is that Kognic expects the timestamp column to be called `ts_gps`.

    ```python
    def convert_zod_lidar_frame_to_point_cloud(lidar_frame: ZodSensorFrame, timestamp_ns: int) -> PointCloud:

      pc = np.load(lidar_frame.filepath)
      cols = ["ts_gps", "x", "y", "z", "intensity"]
      df = DataFrame(pc, columns=["x", "y", "z", "timestamp", "intensity", "diode_index"])
      df["timestamp"] = df["timestamp"] + timestamp_ns
      df.columns = df.columns.str.replace("timestamp", "ts_gps")
      pc_bytes= df[cols].to_csv(index=False).encode()

      return PointCloud(
          filename=lidar_frame.filepath,
          sensor_name=ZodLidar.VELODYNE,
          file_data=FileData(format=FileData.Format.CSV, data=pc_bytes),
      )

    ```

    To convert a camera we need the following function

    ```python
    def convert_zod_camera_frame_to_image(camera_frame: ZodCameraFrame) -> Image:
      camera_timestamp_ns = seconds_to_ns(camera_frame.time.timestamp())
      return Image(
          filename=camera_frame.filepath,
          sensor_name=ZodCamera.FRONT,
          # TODO: Add a comment to explain this, is it necessary is it a nice to have?
          metadata=ImageMetadata(
              shutter_time_start_ns=camera_timestamp_ns,
              shutter_time_end_ns=camera_timestamp_ns + 1,
          ),
      )

    ```

    and finally we need to be able to convert the imu data

    ```python
    frames, seen_frame_timestamps = list(), set()
    start_ts_ms = ns_to_ms(seconds_to_ns(sequence.info.get_lidar_frames()[0].time.timestamp()))

    lidar_calibration = sequence.calibration.lidars[ZodLidar.VELODYNE]

    for camera_frame, lidar_frame in list(sequence.info.get_camera_lidar_map())[:max_nr_frames]:
        frame_ts_s = lidar_frame.time.timestamp()

        if frame_ts_s in seen_frame_timestamps:
            continue

        seen_frame_timestamps.add(frame_ts_s)
        frame_ts_ns = seconds_to_ns(frame_ts_s)

        point_cloud = convert_zod_lidar_frame_to_point_cloud(lidar_frame, frame_ts_ns)

        ego_pose_ref = sequence.ego_motion.get_poses(frame_ts_s)  # reference coordinate system
        # transform to lidar coordinate system, since single-lidar
        ego_pose = np.matmul(ego_pose_ref, lidar_calibration.extrinsics.transform)

        frames.append(
            LCSFrame(
                relative_timestamp=ns_to_ms(frame_ts_ns) - start_ts_ms,
                frame_id=str(frame_ts_ns),
                images=[convert_zod_camera_frame_to_image(camera_frame)],
                point_clouds=[point_cloud],
                ego_vehicle_pose=convert_to_ego_vehicle_pose(ego_pose),
                unix_timestamp=frame_ts_ns,
            )
        )
    ```

  </TabItem>
  <TabItem
    value="aggregated-lidars-and-cameras-sequence"
    label="Aggregated Lidars and Cameras Sequence"
    default
  >
    ```python reference
    https://github.com/annotell/kognic-io-examples-python/blob/master/examples/calibration/create_pinhole_calibration.py#L14-L23
    ```
  </TabItem>
</Tabs>
