---
title: Data requirements
id: data-reqs
description: Kognic Platform Data requirements
---

The Kognic platform supports multiple types of data and information to enable an efficient annotation process, namely **images**, **point clouds**, **calibrations** and **ego vehicle poses/IMU data**. In this section we describe the supported formats for each type.

## Images

We currently support the following image formats: **png**, **jpg**, **jpeg**, **webp** and **avif**.

## Point clouds

Kognic uses a potree format internally to represent and present point clouds, this means that uploaded point cloud data needs to be converted into this format before it can be used as scene in the system. 
We currently support automatic conversion of the following formats: **pcd**, **csv** and **las**. The converter does not however exhaustively support all possible versions of these formats, see below for details of each format.

A timestamp field must always be present in point clouds, both in single-frame and sequence scenes, but the values are irrelevant [if motion compensation is not enabled](./scenes/lidars_with_imu_data.md#enabledisable-motion-compensation).
An intensity field may be provided in point clouds and will be preserved during conversion. If omitted, the intensity for all points will be zero.
Color and other auxiliary data that is not used in the platform is currently discarded in the conversion to potree.

[Click here for a more detailed description of the point cloud formats](../kognic-io/file_formats.md)


## Calibrations

Scenes with 2D and 3D data across various [coordinate systems](../kognic-io/coordinate_systems.md) need **calibrations** to align sensors by location and orientation.
Both an extrinsic calibration that maps the position and rotation in 3D relative to the reference system and an intrinsic camera calibration that projects the 3D points to camera's image plane.
All extrinsic calibrations shall represent the transformation from the sensor to the reference system.

<details>
  <summary>
    <h2 style={{margin: '0px'}}>Types of calibrations</h2>
  </summary>
All calibrations detail a sensor’s 3D position and orientation relative to the reference system, the calibrations shall map the transformation from the sensor to the reference system.
They also map 3D points to the camera’s image plane.

- For LiDAR/RADAR, there is only one type of calibration available, [read more here](../kognic-io/calibrations/lidars.md)
- For cameras, we support different types of [standard camera calibrations](../kognic-io/calibrations/cameras-standard.md), where you have to provide the intrinsic parameters of the camera.
All camera calibration are implemented using the openCV coordinate system.


:::tip Unsupported camera model
If your camera model is not supported, you can also provide
a [custom camera calibration](../kognic-io/calibrations/cameras-custom.md) where you provide the implementation in the form of a WebAssembly module.
:::

</details>

## Ego vehicle poses
An ego vehicle pose can optionally be added to each frame which describes the relative pose. 
It is highly recommended for 3D sequence annotations as it enables more efficient workflows and functions in the Kognic platform, especially for static objects.

The pose is represented using a 3D position and a quaternion in the local coordinate system. 
For a single lidar input the poses shall be on the lidar coordinate system. For multi-lidar inputs the poses shall be on the reference coordinate system
| Key                        | Value                         | Parameters                                                 |
|:---------------------------|:------------------------------|:-----------------------------------------------------------|
| `rotation_quaternion`      | A `RotationQuaternion` object | `w`, `x`, `y`, `z`                                         |
| `position`                 | A `Position` object           | `x`, `y`, `z`                                              |


In addition to the frame poses, there is also the option to upload higher frequency IMU data to enable motion compensation. 
More details on motion compensation can be found [here](../kognic-io/scenes/lidars_with_imu_data.md)


## FAQ

### How do I check the if the calibration is correct
Follow the instructions in the [link](../upload-data/view-uploaded-scene.mdx) to check your calibration.
You can also quickly check the orientation of your camera quickly by opening a scene and go to the 3D viewer. 
At the ego vehicle there is a small yellow circle representing the position and a red arrow representing the orientation of the currently selected camera. Check that the arrow is pointing in the direction that you expect, for example in front of the ego vehicle.


