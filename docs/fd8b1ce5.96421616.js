(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{115:function(e,n,t){"use strict";t.r(n),t.d(n,"MDXContext",(function(){return m})),t.d(n,"MDXProvider",(function(){return u})),t.d(n,"mdx",(function(){return b})),t.d(n,"useMDXComponents",(function(){return p})),t.d(n,"withMDXComponents",(function(){return c}));var a=t(0),i=t.n(a);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(){return(o=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e}).apply(this,arguments)}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function d(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var m=i.a.createContext({}),c=function(e){return function(n){var t=p(n.components);return i.a.createElement(e,o({},n,{components:t}))}},p=function(e){var n=i.a.useContext(m),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=p(e.components);return i.a.createElement(m.Provider,{value:n},e.children)},f={inlineCode:"code",wrapper:function(e){var n=e.children;return i.a.createElement(i.a.Fragment,{},n)}},h=i.a.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,o=e.parentName,s=d(e,["components","mdxType","originalType","parentName"]),m=p(t),c=a,u=m["".concat(o,".").concat(c)]||m[c]||f[c]||r;return t?i.a.createElement(u,l(l({ref:n},s),{},{components:t})):i.a.createElement(u,l({ref:n},s))}));function b(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,o=new Array(r);o[0]=h;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var d=2;d<r;d++)o[d]=t[d];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,t)}h.displayName="MDXCreateElement"},60:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return o})),t.d(n,"metadata",(function(){return s})),t.d(n,"rightToc",(function(){return l})),t.d(n,"default",(function(){return m}));var a=t(3),i=t(8),r=(t(0),t(115)),o={title:"Overview"},s={unversionedId:"kognic-io/overview",id:"kognic-io/overview",isDocsHomePage:!1,title:"Overview",description:"Different types of inputs",source:"@site/docs/kognic-io/overview.md",slug:"/kognic-io/overview",permalink:"/docs/kognic-io/overview",editUrl:"https://github.com/annotell/public-docs/docs-src/docs/kognic-io/overview.md",version:"current",sidebar:"docs",previous:{title:"Projects",permalink:"/docs/kognic-io/project"},next:{title:"Annotation Types",permalink:"/docs/kognic-io/annotation_types"}},l=[{value:"Different types of inputs",id:"different-types-of-inputs",children:[{value:"Sequential vs non-sequential",id:"sequential-vs-non-sequential",children:[]}]},{value:"Input Fields",id:"input-fields",children:[{value:"External Id",id:"external-id",children:[]},{value:"Sensor Specification",id:"sensor-specification",children:[]},{value:"Calibration Id",id:"calibration-id",children:[]},{value:"Metadata",id:"metadata",children:[]},{value:"Frame (non-sequential inputs)",id:"frame-non-sequential-inputs",children:[]},{value:"Frames (sequential inputs)",id:"frames-sequential-inputs",children:[]}]},{value:"Image &amp; Pointcloud Resources",id:"image--pointcloud-resources",children:[{value:"Local File",id:"local-file",children:[]},{value:"Bytes in Memory",id:"bytes-in-memory",children:[]},{value:"Bytes from Callback",id:"bytes-from-callback",children:[]}]},{value:"IMU Data",id:"imu-data",children:[]},{value:"Input Feature Flags",id:"input-feature-flags",children:[]}],d={rightToc:l};function m(e){var n=e.components,t=Object(i.default)(e,["components"]);return Object(r.mdx)("wrapper",Object(a.default)({},d,t,{components:n,mdxType:"MDXLayout"}),Object(r.mdx)("h2",{id:"different-types-of-inputs"},"Different types of inputs"),Object(r.mdx)("p",null,"An Input represents a grouping of sensor data (e.g. camera images, lidar pointclouds) that should be annotated together. Any information necessary to express the relationship between the sensors and their captured data is also present, be it camera resolution, sensor name or the frequency at which the data was recorded at."),Object(r.mdx)("p",null,"There are different input types depending on what kind of sensor(s) are used to represent the contents of the input. For example, if we want to create an input only consisting of data from camera sensors then we would use the input type ",Object(r.mdx)("inlineCode",{parentName:"p"},"Cameras"),". Similarly, if we want to create an input consisting of lidar sensors and camera sensors then we would use the input type ",Object(r.mdx)("inlineCode",{parentName:"p"},"LidarsAndCameras"),". Additionally, inputs can either be ",Object(r.mdx)("strong",{parentName:"p"},"sequential")," or ",Object(r.mdx)("strong",{parentName:"p"},"non-sequential"),"."),Object(r.mdx)("h3",{id:"sequential-vs-non-sequential"},"Sequential vs non-sequential"),Object(r.mdx)("p",null,"Sequential inputs represent a ",Object(r.mdx)("em",{parentName:"p"},"sequence")," of sensor data, whereas non-sequential inputs only contain a single snapshot of sensor data. The sequential relationship is expressed via a sequence of ",Object(r.mdx)("strong",{parentName:"p"},"Frames"),", where each ",Object(r.mdx)("strong",{parentName:"p"},"Frame")," object contains information related to what kind of sensor data constitues the frame (e.g. which image and/or point cloud is a part of the Frame) as well as a ",Object(r.mdx)("em",{parentName:"p"},"relative timestamp")," that captures where in time (relative to the other frames) the Frame is located."),Object(r.mdx)("p",null,"Non-sequential inputs only express a single snapshot of sensor data. As such, these kinds of inputs only contain a single Frame object and do not require any relative timestamp information."),Object(r.mdx)("p",null,"Sequential input types are easily identified by the suffix ",Object(r.mdx)("inlineCode",{parentName:"p"},"Seq")," present in their name."),Object(r.mdx)("p",null,"The following input types are currently supported"),Object(r.mdx)("ul",null,Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"Cameras")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"LidarsAndCameras")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"CamerasSeq")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"LidarAndCamerasSeq"))),Object(r.mdx)("h2",{id:"input-fields"},"Input Fields"),Object(r.mdx)("p",null,"All non-sequential inputs have the following structure"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class Input(BaseModel):\n    external_id: str\n    frame: Frame\n    sensor_specification: SensorSpecification\n    calibration_id: Optional[str] # Required if using lidar sensors\n    metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)\n")),Object(r.mdx)("p",null,"Sequential inputs are similarly represented, except that they instead contain a list of Frames"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class InputSeq(BaseModel):\n    external_id: str\n    frames: List[Frame]\n    sensor_specification: SensorSpecification\n    calibration_id: Optional[str] # Required if using lidar sensors\n    metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)\n")),Object(r.mdx)("p",null,"The fields contain all of the information required to create the input."),Object(r.mdx)("h3",{id:"external-id"},"External Id"),Object(r.mdx)("p",null,"Whenever an input is uploaded it automatically gets an UUID, this is used as the primary identifier by Kognic and by all of our internal systems. However, in order to make communication around specific inputs easier we also allow for clients to include any kind of identifier to the input via the external id."),Object(r.mdx)("h3",{id:"sensor-specification"},"Sensor Specification"),Object(r.mdx)("p",null,"The sensor specification contains information related to the different camera and/or lidar sensors\nused for capturing the data present on the input."),Object(r.mdx)("p",null,'The additional fields are optional and relate to specifying the order of the camera sensors and\nhuman readable variants of the sensor name (e.g. "Front Camera" instead of "FC").'),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class SensorSpecification(BaseModel):\n    sensor_to_pretty_name: Optional[Dict[str, str]] = None\n    sensor_order: Optional[List[str]] = None\n")),Object(r.mdx)("p",null,"As an example, let's say we have three different camera sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". The ",Object(r.mdx)("inlineCode",{parentName:"p"},"R")," sensor is mounted on the right side of the ego vehicle, the ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," sensor at the front and the ",Object(r.mdx)("inlineCode",{parentName:"p"},"L")," sensor to the left. Creating a sensor specification for this scenario would correspond to"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'sensor_spec = SensorSpecification(\n    sensor_to_pretty_name={\n        "R": "Right Camera",\n        "F": "Front Camera",\n        "L": "Left Camera"\n    },\n    sensor_order=["L", "F", "R"]\n)\n')),Object(r.mdx)("p",null,"The specified ",Object(r.mdx)("inlineCode",{parentName:"p"},"sensor_order")," will cause the different camera sensors to be presented in a clockwise manner in the annotation tool (Left -> Front -> Right), while the ",Object(r.mdx)("inlineCode",{parentName:"p"},"sensor_to_pretty_name")," parameter will result in the annotation tool showing the human readable version of all the sensor names when changing sensor."),Object(r.mdx)("h3",{id:"calibration-id"},"Calibration Id"),Object(r.mdx)("p",null,"Any input consisting of lidar and camera sensors requires a calibration. The calibration captures the spatial relationship (position and rotation) between the different sensors, as well as different camera specific parameters."),Object(r.mdx)("p",null,"This information is used by the annotation tool to highlight regions in the point cloud visible in the selected camera sensors as well as for projecting information from the pointcloud onto the different camera sensors (points, cuboids etc)."),Object(r.mdx)("p",null,"Detailed documentation on how to create calibrations via the API is present in the ",Object(r.mdx)("a",{parentName:"p",href:"calibration"},"Calibration section"),"."),Object(r.mdx)("p",null,"When including calibration id make sure that all of the sensors present on the input are also present in the calibration as well. If this is not the case the input will not be created and a validation error will be returned by the API."),Object(r.mdx)("p",null,"Inputs without a lidar sensor do not require a calibration."),Object(r.mdx)("h3",{id:"metadata"},"Metadata"),Object(r.mdx)("p",null,"Metadata can be added to inputs via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"metadata")," field. It consists of ",Object(r.mdx)("em",{parentName:"p"},"flat")," key-value pairs, which means that nested data structures are not allowed. Metadata can be used to include additional information about an input.\nNothing specified in the metadata can be seen by the annotators, but there are some reserved keywords which can alter the annotation tools behaviour, and can be found here:"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/kognic-io/kognic/io/model/input/metadata/metadata.py\n")),Object(r.mdx)("h3",{id:"frame-non-sequential-inputs"},"Frame (non-sequential inputs)"),Object(r.mdx)("p",null,"The Frame object specifies the binary data to be annotated (.jpg, .png, .las etc) as well as which sensor the data originated from."),Object(r.mdx)("p",null,"The Frame object is different for each input type since they all support different kinds of sensors, even though the overall structure is the same."),Object(r.mdx)("p",null,"As an example, let's say we want to create an input consiting of images from three different camera sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". The corresponding binary data is present in the files ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_R.jpg"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_F.jpg")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_F.jpg"),". This would correspond to creating a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Cameras")," input."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'cameras_input = Cameras(\n    ...,\n    frame=Frame(\n        images=[\n            Image("img_cam_R.jpg", sensor_name="R"),\n            Image("img_cam_F.jpg", sensor_name="F"),\n            Image("img_cam_L.jpg", sensor_name="L"),\n        ]\n    )\n)\n')),Object(r.mdx)("p",null,"Similarly, if we also had an associated lidar pointcloud from the sensor ",Object(r.mdx)("inlineCode",{parentName:"p"},"VDL-64")," and a corresponding binary file ",Object(r.mdx)("inlineCode",{parentName:"p"},"scan_vdl_64.las")," we would instead express this as a ",Object(r.mdx)("inlineCode",{parentName:"p"},"LidarsAndCameras")," input instead."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'lidars_and_cameras = LidarsAndCameras(\n    ...,\n    frame=Frame(\n        images=[\n            Image("img_cam_R.jpg", sensor_name="R"),\n            Image("img_cam_F.jpg", sensor_name="F"),\n            Image("img_cam_L.jpg", sensor_name="L"),\n        ],\n        point_clouds=[\n            PointCloud("scan_vdl_64.las", sensor_name="VDL-64")\n        ]\n    )\n\n)\n')),Object(r.mdx)("h3",{id:"frames-sequential-inputs"},"Frames (sequential inputs)"),Object(r.mdx)("p",null,"Sequential inputs deal with a list of Frame objects instead of a single Frame object. In addition, Frame objects associated with sequential inputs have three additional parameters not present in their non-sequential Frame counterparts: ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"relative_timestamp")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"metadata"),"."),Object(r.mdx)("p",null,"The sequential relationship is expressed via the ordering of the Frame objects in the ",Object(r.mdx)("inlineCode",{parentName:"p"},"frames")," list"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = Frame(...)\nframe_2 = Frame(...)\nframe_3 = Frame(...)\nframes = [frame_1, frame_2, frame_3]\n")),Object(r.mdx)("p",null,"This representation captures that ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_1")," comes first, then ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_2")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_3"),", but it does not express how much time has passed between the different frames. This information is encoded via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"relative_timestamp")," parameter present on each Frame object. The relative timestamp is expressed in milliseconds and describes the relative time between the Frame and the start of the input."),Object(r.mdx)("p",null,"For example, let's say that the sensor data is collected and aggregated at 2Hz. That would then be expressed as"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = Frame(..., relative_timestamp=0)\nframe_2 = Frame(..., relative_timestamp=500)\nframe_3 = Frame(..., relative_timestamp=1000)\nframes = [frame_1, frame_2, frame_3]\n")),Object(r.mdx)("p",null,"The ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," is expressed as a string and is used to produce a unique identifier for each frame in the list of frames.\nThe ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," is used as a top-level key in the produced annotations, indicating which parts of the complete annotation\nbelong to this specific frame."),Object(r.mdx)("p",null,"A common use case is to use uuids for each ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id"),", or a combination of ",Object(r.mdx)("inlineCode",{parentName:"p"},"external_id")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_index"),".\nFor example, if the ",Object(r.mdx)("inlineCode",{parentName:"p"},"external_id")," of the input is ",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101")," then the ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," could be encoded as\n",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101:0")," for the first frame, ",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101:1")," for the second frame and so on."),Object(r.mdx)("p",null,"Similarly to the metadata capability available on an input-level, it's also possible to provide metadata on a ",Object(r.mdx)("em",{parentName:"p"},"frame")," level as well.\nIt behaves the same way, i.e. consists of ",Object(r.mdx)("em",{parentName:"p"},"flat")," key-value pairs and is not exposed to annotators during the production of annotators."),Object(r.mdx)("p",null,"As an example, let's say we want to create an input of type ",Object(r.mdx)("inlineCode",{parentName:"p"},"CamerasSeq")," consisting of 2 frames, each with camera data\nfrom two different sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". If we have individual images for each frame and sensor, this would correspond\nto the following list of frames"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'frames = [\n    Frame(\n        frame_id="1",\n        relative_timestamp=0,\n        images=[\n            Image("img_L_1.jpg", sensor_name=\'L\'),\n            Image("img_R_1.jpg", sensor_name=\'R\')\n        ]),\n    Frame(\n        frame_id="2",\n        relative_timestamp=500,\n        images=[\n            Image("img_L_2.jpg", sensor_name=\'L\'),\n            Image("img_R_2.jpg", sensor_name=\'R\')\n        ])\n]\n\ncameras_sequence = CamerasSequence(\n        ...,\n        frames=frames,\n    )\n')),Object(r.mdx)("h2",{id:"image--pointcloud-resources"},"Image & Pointcloud Resources"),Object(r.mdx)("p",null,"Every single file containing binary sensor data (e.g. image or pointcloud files) is represented as a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Resource"),", with\n",Object(r.mdx)("inlineCode",{parentName:"p"},"Image")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"PointCloud")," being the concrete subclasses."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/kognic-io/kognic/io/model/input/resources/resource.py#L12-L16\n")),Object(r.mdx)("p",null,Object(r.mdx)("inlineCode",{parentName:"p"},"Resource"),"s ultimately describe how to obtain some binary or textual sensor data, which can be done in different ways:"),Object(r.mdx)("ol",null,Object(r.mdx)("li",{parentName:"ol"},"Indirectly: by refering to a local filename that contains the data"),Object(r.mdx)("li",{parentName:"ol"},"Directly: provide some ",Object(r.mdx)("inlineCode",{parentName:"li"},"bytes")," at creation time"),Object(r.mdx)("li",{parentName:"ol"},"Lazily: provide a callback function which can provide the ",Object(r.mdx)("inlineCode",{parentName:"li"},"bytes")," later in the process")),Object(r.mdx)("p",null,Object(r.mdx)("inlineCode",{parentName:"p"},"Resource"),"s must always be given a ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename"),". For alternative 1 this must point to the local file to upload. For alternatives 2 & 3 the filename is treated more as an identifier; it is used to name the uploaded file but does not have to correspond to anything in the filesystem."),Object(r.mdx)("p",null,Object(r.mdx)("inlineCode",{parentName:"p"},"Resource"),"s also always have a ",Object(r.mdx)("inlineCode",{parentName:"p"},"sensor_name")," which identifies the sensor they were captured from. In sequential inputs, each ",Object(r.mdx)("inlineCode",{parentName:"p"},"Frame")," will have a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Resource")," for each sensor."),Object(r.mdx)("p",null,"For alternatives 2 & 3 a ",Object(r.mdx)("inlineCode",{parentName:"p"},"FileData")," object is attached to the ",Object(r.mdx)("inlineCode",{parentName:"p"},"Image")," or ",Object(r.mdx)("inlineCode",{parentName:"p"},"PointCloud")," to capture the source of data. It is created with either ",Object(r.mdx)("inlineCode",{parentName:"p"},"data: bytes")," or a ",Object(r.mdx)("inlineCode",{parentName:"p"},"callback: Callable[[str], bytes]"),", as well as a ",Object(r.mdx)("inlineCode",{parentName:"p"},"format")," which identifies the type of data contained in the bytes."),Object(r.mdx)("div",{className:"admonition admonition-info alert alert--info"},Object(r.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(r.mdx)("h5",{parentName:"div"},Object(r.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(r.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(r.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),Object(r.mdx)("div",{parentName:"div",className:"admonition-content"},Object(r.mdx)("p",{parentName:"div"},"Previous API client releases advertised support for ingesting files from external URIs, such as ",Object(r.mdx)("inlineCode",{parentName:"p"},"gs://bucket/path/file"),". Please contact Kognic if you believe you require this functionality going forward."))),Object(r.mdx)("h3",{id:"local-file"},"Local File"),Object(r.mdx)("p",null,"Set ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename")," to the path of the local file and do not provide data via another means (directly or callback). The content is uploaded using a content type inferred from the filename suffix."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'Image(filename="/Users/johndoe/images/img_FC.png",\n      sensor_name="FC")\n')),Object(r.mdx)("h3",{id:"bytes-in-memory"},"Bytes in Memory"),Object(r.mdx)("p",null,"In addition to  ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename"),", provide a ",Object(r.mdx)("inlineCode",{parentName:"p"},"FileData")," object via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"data")," attribute, which in turn holds the raw bytes in its own ",Object(r.mdx)("inlineCode",{parentName:"p"},"data")," attribute, e.g."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'png_blob = FileData(data=b\'some PNG bytes\',\n                    format=FileData.Format.PNG)\nImage(filename="FC-frame15",\n      sensor_name="FC",\n      data=png_blob)\n')),Object(r.mdx)("h3",{id:"bytes-from-callback"},"Bytes from Callback"),Object(r.mdx)("p",null,"In addition to ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename"),", provide a ",Object(r.mdx)("inlineCode",{parentName:"p"},"FileData")," object via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"data")," attribute, which holds a reference to the callback function, e.g."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'png_from_callback = FileData(callback=get_png,\n                             format=FileData.Format.PNG)\nImage(filename="FC-frame15",\n      sensor_name="FC",\n      data=png_from_callback)\n')),Object(r.mdx)("p",null,"The callback function (",Object(r.mdx)("inlineCode",{parentName:"p"},"get_the_bytes"),") is a unary function with the following signature."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"def get_png(file: str) -> bytes:\n    pass\n")),Object(r.mdx)("p",null,"The callback function is invoked with the ",Object(r.mdx)("inlineCode",{parentName:"p"},"Resource.filename")," as its argument when it is time to upload that single file."),Object(r.mdx)("h2",{id:"imu-data"},"IMU Data"),Object(r.mdx)("p",null,"Intertial Measurement Unit (IMU) data may be provided for inputs containing LIDAR pointclouds. This can be used to\nperform motion compensation in multi-lidar setups, and by default if any IMU data is provided this will be done.\nMotion compensation may be disabled via an ",Object(r.mdx)("a",{parentName:"p",href:"feature_flags"},"input feature flag"),", for cases where motion compensation has\nalready been performed prior to upload."),Object(r.mdx)("p",null,"Refer to ",Object(r.mdx)("a",{parentName:"p",href:"inputs/lidars_with_imu_data"},"Motion Compensation for Multi-Lidar Setups"),"."),Object(r.mdx)("h2",{id:"input-feature-flags"},"Input Feature Flags"),Object(r.mdx)("p",null,"Control over optional parts of the input creation process is possible via ",Object(r.mdx)("inlineCode",{parentName:"p"},"FeatureFlags")," that are passed when invoking\nthe create operation on the input. Refer to ",Object(r.mdx)("a",{parentName:"p",href:"feature_flags"},"the feature flags documentation")," for details."))}m.isMDXComponent=!0}}]);