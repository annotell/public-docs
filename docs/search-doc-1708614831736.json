[{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/introduction","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#prerequisites","content":"Before you begin, make sure you have: Access to the dataset exploration toolAn account with permissions to use our APIGenerated API credentials. See API CredentialsInstalled our Python 3 SDK for authentication - kognic-auth "},{"title":"No API Client Available​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#no-api-client-available","content":"At the moment we do not provide an API client for the dataset exploration tool. Instead, we'll provide examples of how you can interact with our API. "},{"title":"Endpoints​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#endpoints","content":"You can discover the list of accessible endpoints within our swagger documentation. "},{"title":"Request Example​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#request-example","content":"Here's an example using the kognic-auth library to list all datasets available to the user: import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession() try: response = client.session.get(base_url + &quot;datasets&quot;) response.raise_for_status() data = response.json() print(data) except requests.exceptions.RequestException as e: print(f&quot;Request error: {e}&quot;)  "},{"title":"Key Concepts","type":0,"sectionRef":"#","url":"/docs/","content":"","keywords":""},{"title":"Project​","type":1,"pageTitle":"Key Concepts","url":"/docs/#project","content":"Project is the top-most concept when interfacing with the Kognic Platform. It is possible to have multiple ongoing projects, and they act as a container for other Kognic resources. Project setup is usually performed by the Kognic Professional Services team during the Guideline Agreement Process (GAP) of a new client engagement. Within the Kognic APIs, projects are identified using an external identifier. "},{"title":"Batch​","type":1,"pageTitle":"Key Concepts","url":"/docs/#batch","content":"Input batches allow grouping of input data into smaller batches within a project. By default, every project has at least one input batch. Ongoing projects can be benefited from having multiple batches in two ways: Group input data that are collected during a certain time intervalPerform guideline or task definition changes without the need for retroactive changes. "},{"title":"Batch Status​","type":1,"pageTitle":"Key Concepts","url":"/docs/#batch-status","content":"Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed. "},{"title":"Request​","type":1,"pageTitle":"Key Concepts","url":"/docs/#request","content":"During GAP, projects can have several annotation types as the end goal. For example, a project consisting of images can be assigned for both lane detection and object annotation. Within Kognic, a Request represents a specific annotation goal for a given input. We divide big and complex projects into several independent annotation types. This makes it possible to: Reduce the cognitive load on the annotatorsMore annotators can work on the same data in parallelSimplify user interfaces All of these contribute to a high level of quality while also reducing the total time needed for producing an annotation. "},{"title":"Guideline​","type":1,"pageTitle":"Key Concepts","url":"/docs/#guideline","content":"In order to produce annotations, one needs to know what to annotate and how. This type of information is found in Guideline. A guideline defines what specific object to mark (e.g. vehicles and pedestrians), as well as how (e.g. bounding box). A guideline also includes detailed information about how to interpret the data, e.g. what it means by a vehicle is &quot;heavily occluded&quot;. "},{"title":"Task Definition​","type":1,"pageTitle":"Key Concepts","url":"/docs/#task-definition","content":"Task Definition describes what should/could be annotated. How many object types? Bounding box, semantic segmentation or lines/splines for each object type? What are the properties for each object type? Task definitions are json files that the Kognic Professional Services team generates from the guideline. The task definition is then used by the Kognic App to construct the appropriate drawing tool. In other words, task definition can be understood as the machine readable quivalent of a guideline. "},{"title":"Scene​","type":1,"pageTitle":"Key Concepts","url":"/docs/#scene","content":"Before setting up any annotation task, the raw data needs to be correctly uploaded to the Kognic Platform. The scene specifies how data from different sources are combined together. Resources are images and point clouds, as well as metadata and calibrations (define sensors' properties). We support different types of setup, for example: Image(s) from a (multiple) camera(s)Image(s) from camera(s) combined with lidar point clouds Another concept related to scene is frame. A frame is a discrete moment of a scene in time. Scenes can be either single frame or sequence (multiple frames). Sequence should be used when temporal information is important for producing the annotation. "},{"title":"Scene Types​","type":1,"pageTitle":"Key Concepts","url":"/docs/#scene-types","content":"Type\tDescriptionCameras\tA single frame consisting of image(s) from 1-9 cameras LidarsAndCameras\tA single frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras CamerasSeq\tA sequence of frames, each frame consisting of image(s) from 1-9 cameras LidarsAndCamerasSeq\tA sequence of frames, each frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras AggregatedLidarsAndCamerasSeq\tA sequence of frames, each frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras. However, point clouds are aggregated over time, producing a unified point cloud. "},{"title":"Input​","type":1,"pageTitle":"Key Concepts","url":"/docs/#input","content":"Once a scene has been uploaded to the Kognic Platform, one can create annotation tasks as inputs where each input is associated to a request. Differenciate input from scene enables efficient reuse of the uploaded data. For instance, multiple inputs can be created from the same scene enabling different kinds of annotation setups. Note that one can create an input simultaneously when creating a scene by providing the project/batch that it should be associated to, see examples in Working with Scenes and Inputs. "},{"title":"Annotation​","type":1,"pageTitle":"Key Concepts","url":"/docs/#annotation","content":"An annotation is produced when inputs are successfully annotated in a request. Annotations are provided by kognic-io API as json objects in ASAM OpenLABEL format. More information on how to download these annotations along with some examples of how to work with them is available in the Downloading Annotations chapter. Apart from kognic-io API, Kognic also provides a library called kognic-openlabel, which makes it easy to parse and work with the OpenLABEL json objects. "},{"title":"annotell-input-api to kognic-io","type":0,"sectionRef":"#","url":"/docs/a2k_migration_guide","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#overview","content":"In September 2022 Annotell rebranded as Kognic. We took opportunity to make some changes to our public Python libraries during the transition. Some of these changes are incompatible with code written against the last version of annotell-input-api (1.3.1 at time of writing). annotell-input-api will now only be updated to fix critical bugs. All users are encouraged to migrate to kognic-io as soon as possible. "},{"title":"Breaking changes​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#breaking-changes","content":""},{"title":"New packages​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#new-packages","content":"Packages under annotell.input_api were moved to kognic.io. In most cases, updating imports is all that will be needed for client code to continue working. "},{"title":"Packaging and imports​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#packaging-and-imports","content":"Previously some classes could be imported via multiple packages due to how we had used * imports internally. The kognic.io.model.* subpackages are now stricter about which internal classes they export than their equivalents in annotell.input_api.model.* It is no longer possible to, for example, use from kognic.io.model.scene.lidars_and_cameras import * or from kognic.io.model.lidars_and_cameras import IMUData to import IMUData along with all other classes related to Lidar+Camera scenes. info Replace instances of from annotell.input_api.model.input.xxx import * with imports for specific classes from kognic.io.model.xxx packages, or with imports from other packages in cases where the import has broken. "},{"title":"Removal of legacy code​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#removal-of-legacy-code","content":"Legacy calibration models were removed, having been deprecated since 2021-11. info Use the typed calibrations from kognic.io.model.calibration.camera and kognic.io.model.calibration.lidar instead of the generic annotell.input_api.model.sensors.LidarCalibration and CameraCalibration. The get_annotations method was removed, having been deprecated since 2021-09. info Use get_project_annotations instead. "},{"title":"Named arguments​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#named-arguments","content":"Many scene model objects such as Image and PointCloud were converted to use pydantic for validation. These classes must now be instanced by passing all non-defaulted properties as named arguments (kwargs). e.g. an Image now requires both filename and sensor_name to be specified. info Ensure named constructor args are present for all non-defaulted fields for model classes like Image and PointCloud. For instance filename was not previously required to be a named arg, now it is. "},{"title":"Non-breaking changes​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#non-breaking-changes","content":""},{"title":"Environment variables​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#environment-variables","content":"Environment variables used by our libraries which are named with the prefix ANNOTELL_ now have a KOGNIC_ equivalent, e.g. ANNOTELL_CREDENTIALS. The old names will continue to work for the time being, with the library code preferring the newer name if both are set. Future changes are not guaranteed to be backwards compatible (e.g. introduction of new variables, or renaming, may not be applied to both prefixes). We therefore recommend that any scripts are updated at the first opportunity. "},{"title":"Validation​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#validation","content":"Many scene model objects (e.g. Image, PointCloud, LidarsAndCameras) have been converted from Python @dataclass to Pydantic models. Client-side validation errors will now be reported differently for these classes. Validation is stricter but the error messages are also more descriptive. Typically validation will be failed because of significant problems: using the wrong datatype for a field, or missing required parameters for an object. Server-side validation is still used to check that the content/values of the input are sound. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/introduction","content":"Introduction info This page has moved here","keywords":""},{"title":"Understand what your dataset contains","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/understand-dataset-content","content":"","keywords":""},{"title":"Understand which scenes you have annotations for in your dataset​","type":1,"pageTitle":"Understand what your dataset contains","url":"/docs/dataset-exploration/understand-dataset-content#understand-which-scenes-you-have-annotations-for-in-your-dataset","content":"To be able to know which scenes you can provide predictions for you need to understand which scenes you've annotations for in your dataset. This can be done by using an endpoint for listing annotations in your dataset. You're currently only able to add scenes indirectly to your dataset by adding annotations to the dataset. Please contact us if this is something you want help with or if you want to connect scenes to your dataset. The following code snippet by utilizing an endpoint for will do this and save it to a csv file. from kognic.auth.requests.auth_session import RequestsAuthSession import csv base_url = &quot;https://dataset.app.kognic.com&quot; client = RequestsAuthSession() dataset_uuid = YOUR_DATASET_UUID done = False offset = 0 annotations = [] while not done: print(f&quot;Fetched {offset} annotations&quot;) res = client.session.get( f&quot;{base_url}/v1/datasets/{dataset_uuid}/annotations?offset={offset}&quot; ) res_annotations = res.json()['data'] annotations += res_annotations offset += len(res_annotations) if len(res_annotations) == 0: done = True print(f&quot;Fetched {len(annotations)} annotations&quot;) # save to csv called dataset_{dataset_uuid}_annotations.csv with open(f'dataset_{dataset_uuid}_annotations.csv', 'w', newline='') as csvfile: writer = csv.writer(csvfile) writer.writerow(annotations[0].keys()) for annotation in annotations: writer.writerow(annotation.values())  "},{"title":"The prediction format","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/prediction-format","content":"The prediction format info This page has moved here","keywords":""},{"title":"Uploading predictions","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/uploading-predictions","content":"Uploading predictions info This page has moved here","keywords":""},{"title":"API Client Overview","type":0,"sectionRef":"#","url":"/docs/kognic-apis","content":"","keywords":""},{"title":"Authentication​","type":1,"pageTitle":"API Client Overview","url":"/docs/kognic-apis#authentication","content":"Authentication is handled by kognic-auth, a Python 3 library providing foundations for Kognic Authentication on top of the requests library. The authentication builds on the standard Oauth 2.0 Client Credentials flow. There are a few ways to provide authentication credentials to our API clients. Kognic Python clients such as in kognic-query or kognic-io accept an auth parameter that can be set explicitly. Alternatively, one can set environment variables that point to the Kognic credentials file. See examples below. "},{"title":"Generating Credentials​","type":1,"pageTitle":"API Client Overview","url":"/docs/kognic-apis#generating-credentials","content":"The credentials file that contains the Kognic Client ID and the Kognic Client secret, can be generated in the Kognic web application by clicking on &quot;Api Credentials...&quot; in the user menu, followed by clicking on the &quot;Generate Credentials&quot; button.  The credentials file should be saved in an appropriate directory, such as ~/.config/kognic/credentials.json. "},{"title":"Examples​","type":1,"pageTitle":"API Client Overview","url":"/docs/kognic-apis#examples","content":"There are a few ways to set your credentials in auth. Set the environment variable KOGNIC_CREDENTIALS to point to your Kognic Credentials file, by issuing the command export KOGNIC_CREDENTIALS=~/.config/kognic/credentials.jsonSet the environment variables KOGNIC_CLIENT_ID and KOGNIC_CLIENT_SECRETSet the auth parameter to the credentials file path, such as auth=&quot;~/.config/kognic/credentials.json&quot;Set the auth parameter to credentials tuple, such as auth=(&lt;client_id&gt;, &lt;client_secret&gt;) By default, Kognic API clients use the credentials set in environment variable(s). To create an authenticated kognic-io client, assuming the environment variable(s) are set correctly, simply do: from kognic.io.client import KognicIOClient api_client = KognicIOClient()  Otherwise, one can override the credentiails explicitly: from kognic.io.client import KognicIOClient api_client = KognicIOClient(auth=&quot;~/.config/kognic/credentials.json&quot;)  Under the hood, they all use the AuthSession class which is implements a requests session with automatic token refresh. "},{"title":"Proxy Configuration​","type":1,"pageTitle":"API Client Overview","url":"/docs/kognic-apis#proxy-configuration","content":"If your organizations' network policy requires HTTP(S) traffic to be proxied out via a specific host, then you should configure this via your OS or execution environment. kognic-io uses Python's urllib which will pick up the proxy configuration from your OS and environment variables. The correct proxy host/address to use depends on the network configuration within your organization, so reach out to your internal IT support for details. For example: export HTTPS_PROXY='http://10.9.8.7:1234'  "},{"title":"Uploading predictions","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/uploading-predictions","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#introduction","content":"In this example, we'll walk you through how to upload predictions using our API into an already existing dataset. Before you begin: See Prerequisites and learn about the prediction format. "},{"title":"Steps​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#steps","content":"Create a new python file and import the following libraries: import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession()  "},{"title":"1. Get the UUID of the dataset​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#1-get-the-uuid-of-the-dataset","content":"You can either access the tool and copy the UUID following dataset/ in the URL, or utilize the datasets endpoint to get the uuid of the dataset: client.session.get(base_url + &quot;datasets&quot;)  "},{"title":"2. Get the UUID of an existing predictions group​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#2-get-the-uuid-of-an-existing-predictions-group","content":"In order to upload predictions, a prediction group needs to exist. Predictions can be organized into groups for any purpose imaginable. The UUID of an existing prediction group can be found in the URL after predictions/ or by using the endpoint client.session.get(base_url + f&quot;/datasets/{datasetUuid}/predictions-groups&quot;)  You can also create a new prediction group using the following code snippet path = base_url + f&quot;/datasets/{datasetUuid}/predictions-groups&quot; body = {&quot;name&quot;: &quot;My predictions group&quot;, &quot;description&quot;: &quot;A description of my new predictions group&quot;} try: response = client.session.post(path, json=body) response.raise_for_status() response_json = response.json() print(f&quot;Created predictions group with uuid {response_json['data']}&quot;) except requests.exceptions.RequestException as e: msg = e.response.text print(f&quot;Request error: {e}. {msg}&quot;)  "},{"title":"3. Upload predictions​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#3-upload-predictions","content":"For a small amount of predictions, synchronous calls might work import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession() predictions_group_uuid = &quot;...&quot; openlabel_content = {&quot;openlabel&quot;: ...} data = { &quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content, } try: response = client.session.post( base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot;, json=data ) response.raise_for_status() response_json = response.json() print(f&quot;Created prediction with uuid {response_json['data']}&quot;) except requests.exceptions.RequestException as e: msg = e.response.text print(f&quot;Request error: {e}. {msg}&quot;)  For larger amounts of predictions, asynchronous calls are recommended. The following example uses the async client from the kognic-auth library to make 100 asynchronous calls: import asyncio from kognic.auth.httpx.async_client import HttpxAuthAsyncClient base_url = &quot;https://dataset.app.kognic.com/v1/&quot; predictions_group_uuid = &quot;...&quot; url = base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot; openlabel_content = {&quot;openlabel&quot;: ...} MAX_CONNECTIONS = 10 async def upload_prediction(payload, session, sem): async with sem: response = await session.post(url, json=payload) response.raise_for_status() return response.json().get(&quot;data&quot;) async def main(n_runs: int): client = HttpxAuthAsyncClient() session = await client.session sem = asyncio.Semaphore(MAX_CONNECTIONS) tasks = [] for i in range(n_runs): payload = {&quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content} task = upload_prediction(payload, session, sem) tasks.append(task) responses = await asyncio.gather(*tasks) await session.aclose() print(responses) if __name__ == '__main__': asyncio.run(main(100))  Setting MAX_CONNECTIONS to something bigger than 10 might not work and is not recommended. "},{"title":"Annotation Types","type":0,"sectionRef":"#","url":"/docs/kognic-io/annotation_types","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#examples","content":"For the following examples we will be creating cameras_sequence inputs, however the procedure would be identical for any other input type. We will also assume that the project example_project_id is configured with the Annotation Types:static_objects, and dynamic_objects, and that they are also available in the batchexample_batch_id. "},{"title":"Get Annotation Types for Project​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#get-annotation-types-for-project","content":"from kognic.io.client import KognicIOClient client = KognicIOClient() project_annotation_types = client.project.get_annotation_types(project=&quot;example_project_id&quot;)  This will return a list of all Annotation Types available in the project. "},{"title":"Get Annotation Types for a specified Project Batch​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#get-annotation-types-for-a-specified-project-batch","content":"from kognic.io.client import KognicIOClient client = KognicIOClient() batch_annotation_types = client.project.get_annotation_types(project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;)  This will return a list of all Annotation Types available in the specified batch. Note that this list does not need to contain all Annotation Types in the project. "},{"title":"Create inputs for specific Annotation Types​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#create-inputs-for-specific-annotation-types","content":"from kognic.io.client import KognicIOClient from kognic.io.model.scene.cameras_sequence import CamerasSequence client = KognicIOClient() camera_input = CamerasSequence(external_id=..., frames=...) client.cameras_sequence.create(camera_input, project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;, annotation_types=[&quot;static_objects&quot;, &quot;dynamic_objects&quot;])  The above example will create a new input which will be annotated for the annotation types specified. If one or more of the specified annotation types would not be available in the specified batch the validation in the API would fail. Specifying batch is optional In these examples we have specified which batch the inputs should be created for, but this is optional. If no batch is specified the inputs will be created in the latest batch with status open. "},{"title":"Create inputs for all Annotation Types in batch​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#create-inputs-for-all-annotation-types-in-batch","content":"from kognic.io.client import KognicIOClient from kognic.io.model.scene.cameras_sequence import CamerasSequence client = KognicIOClient() camera_input = CamerasSequence(external_id=..., frames=...) client.cameras_sequence.create(camera_input, project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;)  The above example will create a new input which will be annotated for all Annotation Types available in the batch example_batch_id. However, this way it is not explicit what Annotation Types that the inputs will be annotated with, and you would not get an error if e.g. static_objects was missing from the specified batch. Always specify Annotation Types In order to get the best possible validation it is recommended that you always specify annotation types when you create inputs. "},{"title":"Add/remove annotation types for an input (deprecated)​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#addremove-annotation-types-for-an-input-deprecated","content":"Deprecated Removing annotation types has been deprecated in favor ofDeleting Inputs and will be removed in the near future. Deprecated Adding annotation types has been deprecated in favor ofCreating Inputs from Scene and will be removed in the near future. Adding an annotation type to an input means that an annotation will be produced for that input with the specified annotation type. In the same way, removing annotation types from an input means that annotations will not be produced for that input with the specified annotation types. In the case when multiple annotation types are annotated in the same task, it is enough to specify one annotation type when adding but all annotation types must be specified when removing. Note that it is currently not possible to add an annotation type that has already been removed from an input. from kognic.io.client import KognicIOClient client = KognicIOClient() input_uuid = 'cca60a67-cb68-4645-8bae-00c6e6415555' # Add an annotation type to an input client.input.add_annotation_type(input_uuid=input_uuid, annotation_type=&quot;annotation-type&quot;) # Remove annotation types from an input annotation_types = [&quot;annotation-type-1&quot;, &quot;annotation-type-2&quot;, ...] client.input.remove_annotation_types(input_uuid=input_uuid, annotation_type=annotation_types)  "},{"title":"The prediction format","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/prediction-format","content":"","keywords":""},{"title":"Supported prediction features​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#supported-prediction-features","content":"The current API for uploading predictions supports the following geometries: Name\tOpenLABEL field\tDescriptionCuboid\tcuboid\tCuboid in 3D Bounding box\tbbox\tBounding box in 2D Bitmaps (segmentation)\timage\tSegmentation bitmap for images The rotation of cuboids should be the same as that in exports (see coordinate systems for more information). 2D geometries should be expressed in pixel coordinates. For this API, the relevant parts (keys) are frames, objects, streams, ontologies and metadata. The last one (metadata) is the easiet one, and should just read schema_version&quot;: &quot;1.0.0&quot; (see examples below for full context). Also stream is straightforward, and should specify what sensors (cameras, lidars, ...) there are and what their name, like sensor_name: {&quot;type&quot;: &quot;camera&quot;} or sensor_name: {&quot;type&quot;: &quot;lidar&quot;}. Again, see the examples below for full context. All parts of a prediction that is time-varying throughout a sequence is described in frames, such as corodinates and dynamic properties. Each frame in the sequence is represented by a key-value pair under frames. The key is the frame_id, and the value should look like frame_id: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { ... } }  The value for frame_properties.timestamp (measured in ms, recommended to set to 0 for non-sequence data) will be used for matching each predicted frame to the relevant annotated frame, and must therefore match the scene that has been annotated. We recommend that frame_id (a string) follows the frame_id used to describe the underlying scene, although frame_properties.timestamp will take precedence in case of mismatch. In case of non-sequence data, a good choice for frame_id is &quot;0&quot;. The values for frame_properties.external_id and frame_properties.stream will be resolved automatically if left empty as shown. The key objects in turn contains key-value pairs, where each such pair is basically an object in that frame. Note that there is the key objects in each frame, as well as in the root. They describe basically the same objects, but the information that is potentially time-varying (i.e. frame-specific, such as coordinates) belongs to the frame, whereas static information (such as the object class) belongs in the root. The object keys (strings) are arbitrary, but must match the keys in the different objects if they are describing the same object. Please refer to the examples below on how to describe the objects in detail. For cuboids and bounding boxes, an existence confidence can be provided by specifying the frame-specific attribute confidence. It must be a numeric value between 0.0 and 1.0, and will be set to 1.0 if left empty. If provided, it must be defined as a numeric value. The static object_data.type will show up as the class name in the tool. For segmentation bitmaps, the image itself is a grayscale 8-bit PNG image of the same resolution as the annotated images (if the actual prediction only partially cover the annotated image or is of lower resolution, it has to be padded and/or upscaled). The image itself is supplied in the openlabel by pasting its base64-encoding as a string as an object to a frame. See the example below. Moreover, also an ontology has to be supplied which describes what class corresponds to each color level. With an 8-bit grayscale image, it is possible to encode up to 256 classes. The ontologycan be left out for non-segmentation predictions. The camera_id in the examples below must match the id of the sensors in the annotated scene, whereas the corresponding id for the lidar sensor should be set to @lidar. "},{"title":"Prediction examples​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#prediction-examples","content":""},{"title":"2D bounding box in two frames with a static property color​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#2d-bounding-box-in-two-frames-with-a-static-property-color","content":"In OpenLabel, a bounding box is represented as a list of 4 values: [x, y, width, height], where x and y are the center coordinates of the bounding box. The width and height are the width and height of the bounding box. The xand y coordinates are relative to the upper left corner of the image. { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;camera_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;val&quot;: [ 1.0, 1.0, 40.0, 30.0 ] } ] } } } }, &quot;1&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 50, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.82, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;camera_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;val&quot;: [ 2.0, 3.0, 30.0, 20.0 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;camera_id&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"3D cuboid in two frames with a static property color​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#3d-cuboid-in-two-frames-with-a-static-property-color","content":"Cuboids are represented as a list of 10 values: [x, y, z, qx, qy, qz, qw, width, length, height], where x, y, and z are the center coordinates of the cuboid. x, y, z, width, length, and height are in meters.qx, qy, qz, and qw are the quaternion values for the rotation of the cuboid. Read more about coordinate systems and quaternions here. { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;@lidar&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } }, &quot;1&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 50, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.87, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;@lidar&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;val&quot;: [ 3.123312801361927, -20.285740376586913, 0.0649137773513349, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;@lidar&quot;: { &quot;type&quot;: &quot;lidar&quot; } } } }  "},{"title":"A single frame segmentation bitmap​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#a-single-frame-segmentation-bitmap","content":"Transforming, upscaling, padding and base64-encoding a small color-image to a larger grayscale image using Python PIL​ This code example gives an example of how to go from a multicolor prediction bitmap image of resolution 300 x 200 to a grayscale image of resolution 1000 x 800, by first converting to grayscale, then rescaling the prediction to 600 x 400 and then padding equally on the sides. It also includes code for base64-encoding the image as a string, that later can be used in the openlabel. This code only makes use of built-in numpy functions, but is not optimized for performance. import base64 import io import numpy as np from PIL import Image # The original mapping used to produce the images original_mapping = { (0,0,0): &quot;_background&quot;, (255,0,0): &quot;class_1&quot;, (0,0,255): &quot;class_2&quot;, } # The grayscale mapping (this will also be the ontology in the openlabel) grayscale_mapping = { &quot;_background&quot;: 0, &quot;class_1&quot;: 1, &quot;class_2&quot;: 2, } prediction = Image.open(&quot;my_original_prediction_file.png&quot;) # Let's say this has resolution 300 x 200 def lookup(pixel_color): return grayscale_mapping[original_mapping[tuple(pixel_color)]] # convert to grayscale via numpy array lookup prediciton_numpy = np.array(prediction) grayscale_prediction_numpy = np.vectorize(lookup, signature=&quot;(m)-&gt;()&quot;)(prediciton_numpy) grayscale_prediction = Image.fromarray(grayscale_prediction_numpy.astype(np.uint8)) # upscale to another resolution upscaled_grayscale_prediction = grayscale_prediction.resize((600, 400), resample=Image.Resampling.NEAREST) # padding by first constructing a new background image of target size, and then paste the prediction in the right position padded_grayscale_prediction = Image.new(&quot;L&quot;, (1000, 800), 0) padded_grayscale_prediction.paste(upscaled_grayscale_prediction, (201, 201)) image_bytes = io.BytesIO() padded_grayscale_prediction.save(image_bytes, format=&quot;PNG&quot;) prediction_str = base64.b64encode(image_bytes.getvalue()).decode(&quot;utf-8&quot;)  Openlabel for a segmentation bitmap​ The prediction_str and grayscale_mapping can thereafter be used in the openlabel like { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;: { &quot;object_data&quot;: { &quot;image&quot;: [ { &quot;name&quot;: &quot;a_human_readable_name&quot;, &quot;val&quot;: prediction_str, &quot;mime_type&quot;: &quot;image/png&quot;, &quot;encoding&quot;: &quot;base64&quot;, &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;val&quot;: &quot;camera_id&quot;, &quot;name&quot;: &quot;stream&quot; } ] } } ] } } }, &quot;frame_properties&quot;: { &quot;streams&quot;: {}, &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot; }, } }, &quot;objects&quot;: { &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;: { &quot;name&quot;: &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;, &quot;type&quot;: &quot;segmentation_bitmap&quot; } }, &quot;streams&quot;: { &quot;camera_id&quot;: { &quot;type&quot;: &quot;camera&quot; } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;ontologies&quot;: { &quot;0&quot;: { &quot;classifications&quot;: grayscale_mapping, &quot;uri&quot;: &quot;&quot; } } } }  If providing predictions for multiple cameras in the scene, the list of images could be extended. "},{"title":"Using kognic-openlabel to validate the format​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#using-kognic-openlabel-to-validate-the-format","content":"See kognic-openlabel for more information. "},{"title":"Downloading Annotations","type":0,"sectionRef":"#","url":"/docs/kognic-io/annotations","content":"","keywords":""},{"title":"v1.1.x​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#v11x","content":"This section describes how you can fetch annotations on the OpenLABEL format. These annotations are automatically available as soon as they are finished and can be downloaded either for an entire project/batch or individually via the methods listed below. All methods return either a single Annotation object or a generator yielding Annotation objects, which contains identifiers as well as a dictionary containing the OpenLABEL json: class Annotation(BaseSerializer): scene_uuid: str annotation_type: str created: datetime content: Optional[Dict]  The OpenLABEL json can be used as it is or be converted into a pythonic object using the kognic-openlabel library, describedhere. "},{"title":"Get Single Annotation​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#get-single-annotation","content":"Using scene and annotation type​ examples/get_annotation.py loading... See full example on GitHub This method returns a single Annotation object, containing the OpenLABEL json, using a scene uuid and an annotation type. "},{"title":"Get Annotations for a Project or Batch​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#get-annotations-for-a-project-or-batch","content":"examples/get_project_annotations.py loading... See full example on GitHub This example fetches annotations for an entire project or batch. The run() method returns a generator which will yield Annotation objects for all finished annotations , for the given project, batch and annotation_type, and in the end prints all of these annotations. "},{"title":"Common use cases​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#common-use-cases","content":""},{"title":"Download and convert a single annotation​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#download-and-convert-a-single-annotation","content":"This example shows a common workflow where an annotation is fetched, parsed into an OpenLabelAnnotation and then converted into a custom annotation format. from kognic.io.client import KognicIOClient from kognic.openlabel.models import OpenLabelAnnotation from pydantic import BaseModel class CustomAnnotationFormat(BaseModel): ... @staticmethod def from_openlabel(openlabel_annotation: OpenLabelAnnotation): pass client = KognicIOClient() annotation = client.annotation.get_annotation( scene_uuid='&lt;scene-uuid-identifier&gt;', annotation_type='&lt;annotation-type&gt;' ) openlabel_annotation = OpenLabelAnnotation.parse_obj(annotation.content) # Create pydantic object converted_annotation = CustomAnnotationFormat.from_openlabel(openlabel_annotation=openlabel_annotation) # Convert annotation converted_dict = converted_annotation.dict(exclude_none=True) # Serialize to dict (or json)  "},{"title":"Download and save annotations to a zip file​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#download-and-save-annotations-to-a-zip-file","content":"In this example, all annotations are fetch for a project batch and then converted saved into a zip file. Note that the save_file must have the extension .zip. import io, json, zipfile import kognic.io.client as KognicIOClient client = KognicIOClient() zip_buffer = io.BytesIO() with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file: for annotation in client.annotation.get_project_annotations( project=&quot;Project-identifier&quot;, batch=&quot;Batch-identifier&quot;, annotation_type=&quot;Annotation-Type-identifier&quot; ): encoded_annotation = io.BytesIO(json.dumps(annotation.content, indent=4).encode()) zip_file.writestr(f&quot;{annotation.scene_uuid}.json&quot;, encoded_annotation.getvalue()) with open('path/to/annotations.zip', 'wb') as f: f.write(zip_buffer.getvalue())  "},{"title":"Calibrations Overview","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/overview","content":"","keywords":""},{"title":"Types of calibrations​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#types-of-calibrations","content":"All calibration types contain information about the position and orientation of the sensor in 3D. This is relative to the reference coordinate system. More information about lidar calibrations can be found here. The calibration also contains information about mapping 3D points to the image plane of the camera. For cameras, we support different types of standard camera calibrations, where you only have to provide the intrinsic parameters of the camera. If your camera model is not supported, you can also provide a custom camera calibration where you provide the implementation in the form of a WebAssembly module. "},{"title":"Examples​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#examples","content":""},{"title":"Creating a calibration​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#creating-a-calibration","content":"The following example code shows how you can create a unity (i.e. we assume that all sensors are placed at origin and have no rotation) calibration for a LIDAR sensor and several camera sensors of different types. examples/calibration/calibration.py loading... See full example on GitHub "},{"title":"Listing existing calibrations​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#listing-existing-calibrations","content":"As a final step we can fetch the calibration via the external id. This can either be done via the client, or via the CLI kognicutil tool. client.calibration.get_calibration(external_id=&quot;Collection 2020-06-16&quot;)  $ kognicutil calibration --external-id &quot;Collection 2020-06-16&quot;  "},{"title":"Standard Camera Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/cameras-standard","content":"","keywords":""},{"title":"Common​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#common","content":"All camera calibrations have the following attributes Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z camera_matrix\tA CameraMatrix object\tfx, fy, cx, cy image_width\tInteger\tNA image_height\tInteger\tNA field_of_view\tFloat\tNA "},{"title":"Pinhole​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#pinhole","content":"The PINHOLE camera model expands the common model with: Key\tValue\tParametersdistortion_coefficients\tA DistortionCoefficients object\tk1, k2, p1, p2, k3 examples/calibration/create_pinhole_calibration.py loading... See full example on GitHub "},{"title":"Fisheye​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#fisheye","content":"The Fisheye camera model expands the PINHOLE model with the following Key\tValue\tParametersxi\tFloat\tNA examples/calibration/create_fisheye_calibration.py loading... See full example on GitHub "},{"title":"Kannala​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#kannala","content":"The KANNALA camera model changes and expands the PINHOLE with the following Key\tValue\tParametersdistortion_coefficients\tA KannalaDistortionCoefficients object. The distortion parameters k3, k4, if available, can be assigned to p1 and p2 respectively. That is p1=k3 and p2=k4.\tk1, k2, p1, p2 undistortion_coefficients\tA UndistortionCoefficients object.\tl1, l2, l3, l4 examples/calibration/create_kannala_calibration.py loading... See full example on GitHub "},{"title":"Principal point distortion​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#principal-point-distortion","content":"The principal point distortion model consists of the common attributes plus Key\tValue\tParametersprincipal_point_distortion_coefficients\tA PrincipalPointDistortionCoefficients object\tk1, k2 lens_projection_coefficients (optional. Default to values for model SF806)\tA LensProjectionCoefficients object\tc1, c2,c3, c4,c5, c6 distortion_center\tA DistortionCenter object\tx, y principal_point\tA PrincipalPoint object\tx, y examples/calibration/create_principal_point_distortion_calibration.py loading... See full example on GitHub "},{"title":"Fused cylindrical​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#fused-cylindrical","content":"The fused cylindrical model consists of the common attributes plus Key\tValue\tParameterscut_angles_degree\tA CutAngles object. Note these angles should be expressed in degrees.\tupper, lower vertical_fov_degree (optional. Default 72.5 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA horizontal_fov_degree (optional. Default 93 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA max_altitude_angle_degree (optional. Default 90 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA examples/calibration/create_fused_cylindrical_calibration.py loading... See full example on GitHub "},{"title":"Cylindrical​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#cylindrical","content":"The cylindrical model consists only of the common attributes. There are no extra attributes to set for this model. examples/calibration/create_cylindrical_calibration.py loading... See full example on GitHub "},{"title":"Principal point fisheye​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#principal-point-fisheye","content":"The principal point fisheye model consists of the common attributes plus Key\tValue\tParametersprincipal_point_fisheye_coefficients\tA PrincipalPointFisheyeCoefficients object\talpha_l, alpha_r, beta_u, beta_l examples/calibration/create_principal_point_fisheye_calibration.py loading... See full example on GitHub "},{"title":"Custom Camera Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/cameras-custom","content":"","keywords":""},{"title":"The WebAssembly module​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#the-webassembly-module","content":"The WebAssembly must follow a strict interface where the module exports a function called project_point_to_image. The function must take three arguments of type float64 and return two values of type float64. Thus, the WebAssembly text representation of this interface is func (param f64 f64 f64) (result f64 f64). The three arguments are the x, y and z coordinates of the 3D point. The two return values are the x and y coordinates of the projected point in the image plane. WebAssembly doesn't support multiple return values by default but this can be enabled with the multi-valueproposal. If the point is not within the field of view, the function should return NaN for both the x and y coordinates. "},{"title":"Validation​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#validation","content":"note This requires wasmtime to be installed which is an optional of dependency kognic-io. Run pip install kognic-io[wasm] to install it. We provide validation code both as python functions and via the kognicutil cli. We validate things such as but not limited to The module can be loadedThe function exists and has the correct signatureThat a point can be projected using the moduleThat points are projected correctly if test cases are provided In python there are three different ways to validate a calibration import kognic.io.tools.calibration.validation as wasm_validation from kognic.io.model.calibration.camera.custom_camera_calibration import CustomCameraCalibration, Point2d, Point3d, TestCase test_cases = [ TestCase( point3d=Point3d(x=1.0, y=2.0, z=3.0), point2d=Point2d(x=2.0, y=5.6) ), TestCase( point3d=Point3d(x=1.0, y=1.0, z=-1.0), point2d=Point2d(x=float(&quot;nan&quot;), y=float(&quot;nan&quot;)) # point is outside field of view ) ] wasm_file = &quot;/path/to/calibration.wasm&quot; calibration = CustomCameraCalibration.from_bytes(wasm_file, test_cases=test_cases, ...) wasm_bytes = calibration.to_bytes() # Validate the calibration object wasm_validation.validate_custom_camera_calibration(calibration, test_cases=test_cases) # Validate the wasm file wasm_validation.validate_wasm_file(wasm_file, test_cases=test_cases) # Validate the wasm binary wasm_validation.validate_wasm_bytes(wasm_bytes, test_cases=test_cases)  The kognicutil cli can be used as follows kognicutil wasm validate calibration.wasm  "},{"title":"Compilation​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#compilation","content":"note It is recommended to keep the wasm file as small as possible. Try to avoid dependencies that are not needed. For example, it may be preferred to implement some mathematical functions yourself instead of using the standard library. As stated above the WebAssembly module must follow a strict interface and compilation requires the multi-value proposal. We provide a set of utilities that will make it easier to compile the WebAssembly module from a few languages, see table below. Language\tTarget\tCompilation toolRust\t*.rs\trustc Rust (Cargo)\tCargo.toml\tcargo C++\t.cc, .cpp\temscripten C\t*.c\temscripten The utilities are available both as python functions and via the kognicutil cli. From Python, you can compile the module with from kognic.io.tools.calibration.compilation import compile_to_wasm wasm_binary = compile_to_wasm(&quot;path/to/source&quot;)  The returned binary can then be used to create a CustomCameraCalibration object. If the output_wasm parameter is passed, the binary will be saved to the specified path. The kognicutil cli can be used as follows kognicutil wasm compile path/to/source path/to/output.wasm  Note that, validation is run by default after compilation. This can be disabled with the --skip-validation flag. Calibration parameters have to be embedded in the binary so that they can be used by the WebAssembly module. Try to pre-compute as much as possible to increase the speed of the projection function at runtime. Below follows examples of a simplified version of the pinhole calibration in a few different languages. "},{"title":"Example: Rust​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-rust","content":"A Rust filecan be compiled with kognicutil wasm compile path/to/source.rs path/to/output.wasm  Note that panics are not supported and compilation will fail if the code contains it. Rust with Cargo​ A Rust module with Cargocan be compiled with kognicutil wasm compile path/to/source/Cargo.toml path/to/output.wasm  Note that it is important to specify that the library is a cdylib and it is also recommended to set strip = true to reduce the size of the WebAssembly module. This is done by adding the following to the Cargo.toml file [lib] crate-type = [&quot;cdylib&quot;] [profile.release] strip = true  "},{"title":"Example: C++​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-c","content":"A C++ filecan be compiled with kognicutil wasm compile path/to/source.cc path/to/output.wasm  or with kognicutil wasm compile path/to/source.cpp path/to/output.wasm  "},{"title":"Example: C​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-c-1","content":"A C++ filecan be compiled with kognicutil wasm compile path/to/source.c path/to/output.wasm  "},{"title":"Lidar Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/lidars","content":"Lidar Calibrations A LIDAR calibration is represented as a LidarCalibration object and consists of a position expressed with three coordinates and a rotation in the form of a Quaternion. Optionally, the sensor's field of view may be specified by providing an object that has a sweep start angle and sweep stop angle. The field of view may also optionally include the depth to which the field extends. Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z field_of_view (optional)\tA LidarFieldOfView object\tstart_angle_deg, stop_angle_deg and optionally depth See the code example below for creating a base LidarCalibration object. examples/calibration/create_lidar_calibration.py loading... See full example on GitHub","keywords":""},{"title":"Errors","type":0,"sectionRef":"#","url":"/docs/kognic-io/error_handling","content":"Errors When the client sends a http request to the API and waits until it receives a response. If the response code is 2xx(the status code for a successful call) the client converts the received message into a python object which can be viewed or used. However, if the API responds with an error code (4xx or 5xx) the python client will raise an error. It's up to the user to decide if and how they want to handle this error.","keywords":""},{"title":"Scene Feature Flags","type":0,"sectionRef":"#","url":"/docs/kognic-io/feature_flags","content":"","keywords":""},{"title":"Currently Supported Features​","type":1,"pageTitle":"Scene Feature Flags","url":"/docs/kognic-io/feature_flags#currently-supported-features","content":""},{"title":"PointCloudFeatures​","type":1,"pageTitle":"Scene Feature Flags","url":"/docs/kognic-io/feature_flags#pointcloudfeatures","content":"Flag\tDefault state\tDescriptionMOTION_COMPENSATION\tEnabled\tCauses motion compensation of point clouds using IMU data. "},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/kognic-io/FAQ","content":"","keywords":""},{"title":"Receiving requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: ... when trying to create inputs​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#receiving-requestsexceptionshttperror-403-client-error-forbidden-for-url--when-trying-to-create-inputs","content":"This implies that the authenticated user does not have access to the endpoint being called. Make sure you're authenticating correctly. If a Kognic user, make sure client_organization_id is specified on the KognicIOClient. "},{"title":"How do I know that my input was created successfully?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-do-i-know-that-my-input-was-created-successfully","content":"Whenever a .create(...) call for an input has been successfully made it's (asynchronously) submitted for pre-processing in the Kognic platform. The input is available only once the pre-processing has been successfully executed. However, pre-processing can also fail, for example if the pointcloud or image files are poorly formatted or corrupt. The easiest way to check the status of an input is the input status field present on inputs returned by the methods get_inputs(...) and get_inputs_by_uuids(...). The input is successfully created and available in the platform once the status is set to created. note Since pre-processing is an asynchronous process it might take a while before the input changes status from processing to either created or failed. # Example code of how to check if an input has been successfully created resp = client.cameras.create(...) uuid = resp.uuid [i] = client.input.get_inputs_by_uuids(input_uuids=[uuid]) # Successfully created and available once status is `created` print(f'Input {uuid} status:', i.status)  "},{"title":"How can I view my input?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-can-i-view-my-input","content":"Successfully created inputs can be viewed in the Kognic platform via their view-link. The view-link can be accessed via the view_link field present on inputs returned by the methods get_inputs(...) and get_inputs_by_uuids(...). # Example code of how to access view-links for all inputs in a project inputs = client.input.get_inputs(project=&quot;project-identifier&quot;) for i in inputs: print(f&quot;Input {i.external_id} view-link: {i.view_link}&quot;)  "},{"title":"Why are the cuboids rotated by 90 degrees?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#why-are-the-cuboids-rotated-by-90-degrees","content":"The coordinate system is defined by the uploaded data, but the rotation is defined by Kognic. This is somewhat different (90-degree rotation) compared to the ISO 8855 standard. See Rotation of Cuboids for more about this and how you can convert to ISO 8855. "},{"title":"Projects","type":0,"sectionRef":"#","url":"/docs/kognic-io/project","content":"","keywords":""},{"title":"Project​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#project","content":"A Kognic project must first be set in order to create inputs. Projects are usually configured by the Kognic Professional Services team, during the Guideline Agreement Process (GAP) of a new client engagement. "},{"title":"List Projects​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#list-projects","content":"All existing projects within your organization can be listed by using the Python API KognicIOClient. Make sure the authentication are set (see Authentication). from kognic.io.client import KognicIOClient client = KognicIOClient() projects = client.project.get_projects()  Alternatively, projects can be listed with the kognicutil command line interface (CLI) kognicutil projects  "},{"title":"Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch","content":"Input batches allow further grouping of inputs into smaller batches within a project. Specifying batch during the input creation is optional, and will otherwise be the latest open batch by default. "},{"title":"Batch Status​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch-status","content":"Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed. "},{"title":"Listing Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#listing-batches","content":"All existing batches withint a project can be listed by using the Python API KognicIOClient. from kognic.io.client import KognicIOClient client = KognicIOClient() project_batches = client.project.get_project_batches(project=&quot;&lt;project_external_id&gt;&quot;)  Alternatively, batches can be listed with the kognicutil CLI kognicutil projects &lt;project_external_id&gt; --get-batches  "},{"title":"Creating Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#creating-batches","content":"To create a new batch in the open state within a project from kognic.io.client import KognicIOClient client = KognicIOClient() project_batch = client.project.create_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, )  The newly created batch will contain the same Annotation Types as the latest batch by default. This method has an optional flag publish_previous_batches which defaults to False. By setting this flag toTrue, as shown in the example below, all previous batches in the &quot;open&quot; state would be published and you would no longer be able to upload new inputs to those batches. You should therefore be certain that you no longer need to upload more inputs to the previous batches if you use this flag. from kognic.io.client import KognicIOClient client = KognicIOClient() project_batch = client.project.create_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, publish_previous_batches=True, )  Contact Kognic before use Kognic usually helps with creating batches before a client becomes autonomous, in order to avoid any confusion please contact Kognic before you start using this feature. "},{"title":"Publish Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#publish-batch","content":"from kognic.io.client import KognicIOClient client = KognicIOClient() project_batch = client.project.publish_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, )  When the input batch is published, the status of the batch will be set to &quot;ready&quot;. Published batches are not open for new inputs any longer. A project with multiple open batches will require you to specify which open batch to target when creating new inputs, whereas a project with a single open batch will allow you omit the batch parameter when creating inputs. "},{"title":"Coordinate Systems","type":0,"sectionRef":"#","url":"/docs/kognic-io/coordinate_systems","content":"","keywords":""},{"title":"The reference coordinate system and calibrations​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-reference-coordinate-system-and-calibrations","content":"Each sensor has its own coordinate system in 3D space that depends on its location and orientation on the ego vehicle. Being able to transform measurements between these sensor coordinate systems is important. To do this, a reference coordinate system is defined which works as a middle man between the sensor coordinate systems. The reference coordinate system can be chosen arbitrarily relative to the ego vehicle. By defining a calibration function CiC_iCi​ for sensor iiiwe can map a point xi⃗\\vec{x_i}xi​​ to the reference coordinate system in the following way xR⃗=Ci(xi⃗)\\vec{x_R} = C_i(\\vec{x_i})xR​​=Ci​(xi​​) In the same way we can map points from all other sensors to the reference coordinate system. Subsequently, we can also map a point from coordinate system iii to coordinate system jjj by applying the inverse of the calibration xj⃗=Cj−1(Ci(xi⃗))\\vec{x_j} = C_j^{-1}(C_i(\\vec{x_i}))xj​​=Cj−1​(Ci​(xi​​)) "},{"title":"The world coordinate system and ego motion data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-world-coordinate-system-and-ego-motion-data","content":"With this, we can now express points in coordinate systems local to the ego vehicle. This is great, but sometimes it is also valuable to express points recorded at different times in the same coordinate system. We call this the world coordinate system since it is static in time. We can transform a point to the world coordinate system using ego motion data, which describes the location and orientation of the ego vehicle at any given time. With the ego motion data we can transform a point xt⃗\\vec{x_t}xt​​ to the world coordinate system with xw⃗=Et(xt⃗)\\vec{x_w} = E_t(\\vec{x_t})xw​​=Et​(xt​​) Subsequently, we can also transform a point recorded at time ttt to the coordinate system at time t′t't′ by applying the inverse of the ego transformation function xt′⃗=Et′−1(Et(xt⃗))\\vec{x_{t'}} = E_{t'}^{-1}(E_t(\\vec{x_t}))xt′​​=Et′−1​(Et​(xt​​)) This can be used to compensate each lidar point for the motion of the ego vehicle, a process also known asmotion compensation. It is highly recommended to motion compensate point clouds since lidar points are recorded at different instants in time. This can be done by providing high frequency ego motion data (IMU data) when creating a scene. "},{"title":"Single-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#single-lidar-case","content":"The image below displays how the different sensors relate to each other in 3D space in the single-lidar case. Note that the ego motion data should be expressed in the lidar coordinate system.  "},{"title":"Multi-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#multi-lidar-case","content":"In the multi-lidar case (see image below) there are multiple point clouds, each in their own lidar coordinate system. These are merged into one point cloud in the reference coordinate system during scene creation since it's more efficient to annotate one point cloud rather than several. If IMU data is available, we can also compensate for the ego motion so that each point is transformed to the reference coordinate system at the frame timestamp. This is done by applying xw⃗=Et(Ci(xi,t⃗))xt′⃗=Et′−1(xw⃗)\\vec{x_w} = E_t(C_i(\\vec{x_{i,t}})) \\\\ \\vec{x_{t'}} = E_{t'}^{-1}(\\vec{x_{w}})xw​​=Et​(Ci​(xi,t​​))xt′​​=Et′−1​(xw​​) where xi,t⃗\\vec{x_{i,t}}xi,t​​ is the point expressed in the lidar coordinate system of lidar iii at time ttt and xt′⃗\\vec{x_{t'}}xt′​​is the point expressed in the reference coordinate system at the frame time t′t't′. It is recommended to provide IMU data so that motion compensation can be utilized. Since the merged point cloud is expressed in the reference coordinate system we also expect any ego motion data to be expressed in the reference coordinate system.  "},{"title":"Different coordinate systems for different kinds of data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#different-coordinate-systems-for-different-kinds-of-data","content":"Different kinds of data are expressed in different coordinate systems depending on whether it's single-lidar or multi-lidar. This is summarized in the table below where we can see that ego motion data should be expressed in the lidar coordinate system in the single-lidar case but in the reference coordinate system in the multi-lidar case for example. Type of data\tSingle-lidar\tMulti-lidarEgo poses &amp; IMU data\tLidar\tReference OpenLABEL export 3D geometries\tLidar\tReference OpenLABEL export 2D geometries\tPixel\tPixel Pre-annotations 3D geometries\tLidar\tReference Pre-annotations 2D geometries\tPixel\tPixel "},{"title":"Images","type":0,"sectionRef":"#","url":"/docs/kognic-io/resources/images","content":"Images The API allows uploading of annotation project related data such as images and point clouds. For images, we currently support two formats: png, jpg and webp.","keywords":""},{"title":"Aggregated Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq","content":"Aggregated Lidars and Cameras Sequence note This feature is new in version 1.1.5 An AggregatedLidarsAndCamerasSeq scene consists of a sequence of camera images and lidar point clouds, where each frame consists on 1-9 camera images as well as 1-20 point clouds (in the case where you have pre-aggregated your point clouds, the first frame consists of 1-20 point clouds and all other frames 0 point clouds). What differentiates AggregatedLidarsAndCamerasSeqfrom LidarsAndCamerasSeq is that point clouds are aggregated over time during annotation which results in one big point cloud in the coordinate system of the first frame. Therefore, ego motion data is mandatory for this type of scene. For more documentation on what each field corresponds to in the AggregatedLidarsAndCamerasSeq object please check the section related to Scene Overview. Refer to Coordinate Systems for more information about what coordinate systems to use. examples/agg_lidars_and_cameras_seq.py loading... See full example on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it. reuse calibration Note that you can, and should, reuse the same calibration for multiple s if possible.","keywords":""},{"title":"Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/cameras","content":"Cameras A Cameras consists of a single frame of camera images, where the frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the Cameras object please check the section related to Scene Overview. examples/cameras.py loading... See full example on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":""},{"title":"Point clouds","type":0,"sectionRef":"#","url":"/docs/kognic-io/resources/pointclouds","content":"","keywords":""},{"title":"PCD​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#pcd","content":"The currently supported format includes the following header: VERSION .7 FIELDS x y z intensity timestamp SIZE 4 4 4 4 8 TYPE F F F U U COUNT 1 1 1 1 1 WIDTH &lt;w&gt; HEIGHT &lt;h&gt; VIEWPOINT 0 0 0 1 0 0 0 POINTS &lt;n&gt; DATA ascii  Apart from ascii as DATA type, we also support binary and binary_compressed. Note that we currently don't support organized point clouds in the binary_compressed case, i.e. when HEIGHT is not equal to 1. "},{"title":"CSV​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#csv","content":"We currently only support the following exact header and using , as separation character (where intensity is uint8,ts_gps is an uint64 and x, y, z are all float32): ts_gps,x,y,z,intensity  All other formats will fail. "},{"title":"LAS​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#las","content":"We currently support version 1.2 and point format id 3, as defined in the las 1.2 specification. All other formats will cause the conversion to fail. "},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/kognic-io/overview","content":"","keywords":""},{"title":"Different types of scenes​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#different-types-of-scenes","content":"A scene represents a grouping of sensor data (e.g. camera images, lidar pointclouds) that should be annotated together. Any information necessary to describe the relationship between the sensors and their captured data is also specifed in the scene, be it camera resolution, sensor name, and the frequency at which the data was recorded at, etc. There are different scene types depending on what kind of sensor(s) are used to represent the contents of the scene. For example, if one wants to create a scene only consisting of image data from camera sensors then one would use the scene type Cameras. Similarly, if one wants to create a scene consisting of both lidar and camera sensors then one would use the scene type LidarsAndCameras. Additionally, scenes can either be single frame or sequence type. "},{"title":"Sequential vs non-sequential​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sequential-vs-non-sequential","content":"Sequential scenes represent a sequence of frames in time, whereas non-sequential scenes only contain one snapshot of the sensor data. The sequential relationship is expressed via a sequence of Frames, where each Frame contains information related to what kind of sensor data constitues the frame (e.g. which image and/or pointcloud is part of the Frame) as well as a relative timestamp that captures where in time (relative to the other frames) the Frame is located. Non-sequential scenes only contains a single Frame and do not require any relative timestamp information. Sequential scene types are identified by the suffix Seq in their type name. The following scene types are currently supported CamerasLidarsAndCamerasCamerasSeqLidarsAndCamerasSeqAggregatedLidarsAndCamerasSeq "},{"title":"Scene Fields​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#scene-fields","content":"Non-sequential scenes have the following structure class Scene(BaseModel): external_id: str frame: Frame sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)  Sequential scenes are similarly represented, except that they instead contain a list of Frames class SceneSeq(BaseModel): external_id: str frames: List[Frame] sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)  "},{"title":"External Id​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#external-id","content":"A scene automatically gets a UUID when it is created. This UUID is used as the primary identifier by Kognic and all of our internal systems. Additionally, an external id is required as an identifier when creating the scene in order to make communication around specific scenes easier. "},{"title":"Sensor Specification​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sensor-specification","content":"The sensor specification contains information about the camera and/or lidar sensors used in the scene. The additional fields are optional, and can be used to specify the order of the camera images and a human readable sensor name (e.g. &quot;Front Camera&quot; instead of &quot;FC&quot;) when viewed in the Kognic annotation App. As an example, let's say we have three camera sensors R, F and L positioned on the ego vehicle. Creating a sensor specification would be from kognic.io.model.scene.sensor_specification import SensorSpecification sensor_spec = SensorSpecification( sensor_to_pretty_name={ &quot;R&quot;: &quot;Right Camera&quot;, &quot;F&quot;: &quot;Front Camera&quot;, &quot;L&quot;: &quot;Left Camera&quot; }, sensor_order=[&quot;L&quot;, &quot;F&quot;, &quot;R&quot;] )  sensor_order configures the order of camera images, and sensor_to_pretty_name affects the labels when viewed in the Kognic annotation App. "},{"title":"Calibration​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#calibration","content":"Any scene consisting of lidar and camera sensors requires a calibration. The calibration specifies the spatial relationship (position and rotation) between the sensors, and the camera intrinsic parameters. However, scenes without a lidar sensor do not require a calibration. Calibration is used by the Kognic annotation App to project regions in the pointcloud when a camera image is selected, and, similarly, to project the selected object (e.g. point, cuboid) in the pointcloud onto the images . When creating a calibration, all sensors must match those present on the scene. If this is not the case the scene will not be created and a validation error will be returned by the Kognic API. Detailed documentation on how to create calibrations via the API is present in the Calibration section. "},{"title":"Metadata​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#metadata","content":"Metadata can be added to scenes via the metadata field. It consists of flat key-value pairs, which means that nested data structures are not allowed. Metadata can be used to include additional information about a scene. Metadata cannot be seen by the annotators, but there are some reserved keywords that can alter the behaviour of the Kognic annotation tool. Reserved keywords can be found in the MetaData object in the python client. "},{"title":"Frame​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#frame","content":"The Frame object specifies the binary data to be annotated (.jpg, .png, .las etc) as well as which sensor the data originated from. Note that the Frame object is different for each scene type, even though the overall structure is similar (see details below). Non sequential frame​ As an example, let's say we want to create a scene consiting of images from three camera sensors R, F and L. The corresponding binary data are in the files img_cam_R.jpg, img_cam_F.jpg and img_cam_F.jpg. This would correspond the scene type Cameras. from kognic.io.model.scene.resources import Image from kognic.io.model.scene.cameras import Cameras, Frame cameras_scene = Cameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ] ) )  Similarly, if we also had an associated lidar pointcloud from the sensor VDL-64 and a corresponding binary file scan_vdl_64.las, we would instead use the scene type LidarsAndCameras. Note that the Frame class shall be imported under the corresponding scene type. from kognic.io.model.scene.resources import Image, PointCloud from kognic.io.model.scene.lidars_and_cameras import LidarsAndCameras, Frame lidars_and_cameras = LidarsAndCameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ], point_clouds=[ PointCloud(&quot;scan_vdl_64.las&quot;, sensor_name=&quot;VDL-64&quot;) ] ) )  Sequential frames​ Sequential scene takes a list of Frame objects instead of a single Frame. In addition, the Frame object associated with sequential scenes have three additional parameters: frame_id, relative_timestamp and metadata. The sequential relationship is expressed via the order of the list of Frame. To express how much time has passed between the different frames, one can use the relative_timestamp parameter for each Frame. The relative timestamp is expressed in milliseconds and describes the relative time between the Frame and the start of the scene. For example, let's say that the sensor data is collected and aggregated at 2Hz. frame_1 = Frame(..., relative_timestamp=0) frame_2 = Frame(..., relative_timestamp=500) frame_3 = Frame(..., relative_timestamp=1000) frames = [frame_1, frame_2, frame_3]  The frame_id is a string that uniquely identifies each frame in the list of frames. A common use case is to use uuids for each frame_id, or a combination of external_id and frame_index. For example, if the external_id of the scene is shanghai_20200101 then the frame_id could be encoded asshanghai_20200101:0 for the first frame, shanghai_20200101:1 for the second frame and so on. It's also possible to provide metadata on a frame level for sequential frames. It consists of flat key-value pairs and is not exposed to annotators during the production of annotators. As an example, let's say we want to create a scene of type CamerasSequence consisting of 2 frames, each with camera images from two sensors R and L. from kognic.io.model.scene.resources import Image from kognic.io.model.scene.cameras_sequence import CamerasSequence, Frame frames = [ Frame( frame_id=&quot;1&quot;, relative_timestamp=0, images=[ Image(&quot;img_L_1.jpg&quot;, sensor_name='L'), Image(&quot;img_R_1.jpg&quot;, sensor_name='R') ]), Frame( frame_id=&quot;2&quot;, relative_timestamp=500, images=[ Image(&quot;img_L_2.jpg&quot;, sensor_name='L'), Image(&quot;img_R_2.jpg&quot;, sensor_name='R') ]) ] cameras_sequence = CamerasSequence( ..., frames=frames, )  "},{"title":"Image & Pointcloud Resources​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#image--pointcloud-resources","content":"Every file containing sensor data is represented as a Resource, withImage and PointCloud being the concrete subclasses. class Resource(ABC, BaseSerializer): filename: str resource_id: Optional[str] = None sensor_name: str file_data: Optional[FileData] = Field(default=None, exclude=True)  Resources ultimately describe how to obtain some binary or textual sensor data, which can be done in different ways: Indirectly: by refering to a local filename that contains the dataDirectly: provide some bytes-like object at creation timeLazily: provide a callback function which can provide the bytes later in the process Resources must always be given a filename. For alternative 1 this must point to the local file to upload. For alternatives 2 &amp; 3 the value of filename parameter is treated as an identifier; it is used to name the uploaded file but does not have to correspond to anything in the filesystem. Resources also always have a sensor_name which identifies the sensor they were captured from. In sequential scenes, each Frame will have a Resource for each sensor. Resources take their actual data (bytes) from bytes, a BinaryIO or an IOBase-compatible object. These are referred to with the type alias UploadableData = Union[bytes, BinaryIO, IOBase]. For alternatives 2 &amp; 3 listed above, a FileData object is attached to the Image or PointCloud to capture the source of data. It is created with either data: UploadableData or a callback: Callable[[str], UploadableData], as well as a format which identifies the type of data contained in the bytes. info Previous API client releases advertised support for ingesting files from external URIs, such as gs://bucket/path/file. Please contact Kognic if you believe you require this functionality going forward. "},{"title":"Local File​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#local-file","content":"Set filename to the path of the local file and do not provide data via other means (directly or callback). The content is uploaded using a content type inferred from the filename suffix. Image(filename=&quot;/Users/johndoe/images/img_FC.png&quot;, sensor_name=&quot;FC&quot;)  "},{"title":"Data in Memory​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-in-memory","content":"In addition to filename, provide a FileData object via the file_data attribute, which in turn has an UploadableData as its own data attribute. This example uses raw bytes: png_blob = FileData(data=b'some PNG bytes', format=FileData.Format.PNG) Image(filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=png_blob)  "},{"title":"Data from Callback​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-from-callback","content":"In addition to filename, provide a FileData object via the file_data attribute, with a callback function that produces an UploadableData, e.g. png_from_callback = FileData(callback=get_png, format=FileData.Format.PNG) Image( filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=png_from_callback )  The callback function (get_png) is a unary function with the following signature. def get_png(file: str) -&gt; UploadableData: pass  The callback function is invoked with the Resource.filename as its argument when it is time to upload that single file. If the callback requires extra arguments then we recommend creating a closure over the additional arguments like this: def get_callback(arg1, arg2, **kwargs): def callback(filename) -&gt; bytes: # ... use arg1, arg2, filename and kwargs return callback FileData( callback=get_callback(&quot;foo&quot;, &quot;bar&quot;, extra1=&quot;baz&quot;, extra2=&quot;qux&quot;), format=FileData.Format.JPG )  "},{"title":"Data from Asynchronous Callback (new in version 1.5.0)​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-from-asynchronous-callback-new-in-version-150","content":"Using asynchronous callbacks can be useful to speed up data uploads, especially when the data is not available locally. In the same way as for synchronous callbacks, the callback function is invoked with the Resource.filename as its argument when it is time to upload that single file. Asynchronous callbacks can be used in the following way: async def get_png(filename: str) -&gt; UploadableData: pass png_from_async_callback = FileData(callback=get_png, format=FileData.Format.PNG) Image( filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=png_from_async_callback )  "},{"title":"IMU Data​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#imu-data","content":"Inertial Measurement Unit (IMU) data may be provided for scenes containing LIDAR pointclouds. This can be used to perform motion compensation in multi-lidar setups, and by default if any IMU data is provided this will be done. Motion compensation may be disabled via a scene feature flag, for cases where motion compensation has already been performed prior to upload. Refer to Motion Compensation for Multi-Lidar Setups. "},{"title":"Scene Feature Flags​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#scene-feature-flags","content":"Control over optional parts of the scene creation process is possible via FeatureFlags that are passed when invoking the create operation on the scene. Refer to the feature flags documentation for details. "},{"title":"Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/cameras_seq","content":"Cameras Sequence A CamerasSeq consists of a sequence of camera images, where each frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the CamerasSeq object please check the section related to Scene Overview. examples/cameras_seq_images.py loading... See full example on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":""},{"title":"Lidars and Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_and_cameras","content":"Lidars and Cameras A LidarsAndCameras consists of a single frame which contains 1-9 cameras images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the LidarsAndCameras object please check the section related to Scene Overview. examples/lidars_and_cameras.py loading... See full example on GitHub Use dryrun to validate Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":""},{"title":"Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq","content":"","keywords":""},{"title":"Providing Ego Vehicle Motion Information​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq#providing-ego-vehicle-motion-information","content":"Ego vehicle motion (i.e. the position and rotation of the ego vehicle) is optional information that can be provided when creating LidarsAndCamerasSeqs. This information can enable a massive reduction in the time it takes to annotate static objects. Ego vehicle motion information is provided by passing a EgoVehicleMotion object to each Framein the scene. examples/lidars_and_cameras_seq_full.py loading... See full example on GitHub Coordinate Systems Note that both position and rotation for ego vehicle pose are with respect to the local coordinate system. "},{"title":"Shutter timings​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq#shutter-timings","content":"Shutter timings are optional metadata that may be provided when creating an Image within a Frame. Timings are two values: shutter start and end timestamp in nanoseconds since unix epoch and are specified for each image in each frame. examples/lidars_and_cameras_seq_with_imu_and_shutter_times.py loading... See full example on GitHub "},{"title":"Motion Compensation","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_with_imu_data","content":"","keywords":""},{"title":"Enable/disable motion compensation​","type":1,"pageTitle":"Motion Compensation","url":"/docs/kognic-io/scenes/lidars_with_imu_data#enabledisable-motion-compensation","content":"By default motion compensation is performed for scenes with LIDAR pointclouds when IMU data is provided. Whether motion compensation is enabled or not is controlled by a feature flag. By default it is enabled but it can be disabled by providing an empty feature flag. from kognic.io.model.scene.feature_flags import FeatureFlags client.lidars_and_cameras_sequence.create( ..., feature_flags=FeatureFlags() )  It may be desirable to disable motion compensation in cases where pointclouds are already motion compensated outside of the Kognic platform. "},{"title":"The Python client","type":0,"sectionRef":"#","url":"/docs/openlabel/python-client","content":"The Python client Using this schema we have developed a python client kognic-openlabel which makes it easier to work with annotations. The python client models the OpenLABEL format as pydantic models. It is publicly availablehere and can be installed with pip install kognic-openlabel Since pydantic is used, the model contains validation as well as methods for serialization and deserialition. Below are examples of how you can easily change between different formats openlabel_dict = { &quot;openlabel&quot;: { &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; } } } from kognic.openlabel.models import OpenLabelAnnotation # Deserialize dict openlabel_annotation = OpenLabelAnnotation.parse_obj(openlabel_dict) # Serialize to json openlabel_json = openlabel_annotation.json(exclude_none=True) # Deserialize json openlabel_annotation = OpenLabelAnnotation.parse_raw(openlabel_json) # Serialize to dict openlabel_dict = openlabel_annotation.dict(exclude_none=True) ","keywords":""},{"title":"Working with Scenes & Inputs","type":0,"sectionRef":"#","url":"/docs/kognic-io/working_with_scenes_and_inputs","content":"","keywords":""},{"title":"Creating Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-scenes","content":"Each scene resource has a create method that can be used to create a scene of the corresponding type. It takes the corresponding scene model as input, so for example to create a Cameras scene you would do the following: from kognic.io.model.scene.cameras import Cameras scene = Cameras(...) scene = client.cameras.create(cameras_scene) scene_uuid = created_scene.uuid  As you can see, the create method returns the associated scene_uuid, which can later be used to work with the scene. At this point all files have been uploaded to the Kognic Platform and the scene starts to be pre-processed. When pre-processing is finished, we say that the scene has been created. Refer to the Scene Status section for more information about the different scene statuses. Note that it is often useful to use the dryrun parameter when experimenting. This will validate the scene format but not create it. "},{"title":"Scene Status​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#scene-status","content":"Once a scene has been created, it might be preprocessed before being made available for annotation. Also, postprocessing such as conversion to the client-specific format might take place after annotation has been performed. During this process, the status property of a scene can be used to keep track of progress. Status\tDescriptionpending\tScene has been validated but the server is waiting for the associated data to be uploaded processing\tAssociated data has been uploaded and is currently being processed by the Kognic Platform, potentially performing conversion of file formats created\tScene is created and available for annotation failed\tConversion of scene failed invalidated:broken-input\tScene was invalidated since it did not load invalidated:duplicate\tScene was invalidated due to being uploaded several times invalidated:incorrectly-created\tScene was invalidated because it was incorrectly created "},{"title":"Creating Inputs from Scene​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-inputs-from-scene","content":"Once a scene has been created, it can be used to create inputs which is done by associating it with a projectand an input batch. Consider the following project setup: organization # root for projects and scenes └── projects ├── project-a ├── batch-1 - completed ├── batch-2 - open ├── request-1 ├── input 9c08f7a3-3216-4bd6-a41a-1dda6f66f53e – using scene 0edb ├── input ddf548e3-9806-433c-afb5-fb951a721462 - using scene 37d9 └── ... └── request-2 └── batch-3 - pending └── project-b ├── batch-1 └── ... └── scenes ├── scene 0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3 ├── scene 37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36 └── ...  The create_from_scene method is used to create inputs from a scene. The method takes the scene uuid as input along with annotation information such as project, batch and annotation types. For example, to create inputs in project-aand batch-2, you would do the following: client.cameras.create_from_scene( scene_uuid=&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, project=&quot;project-a&quot;, batch=&quot;batch-2&quot; )  The above code will create inputs for the scene in all requests in batch batch-2 for project project-a. If the batchparameter is omitted, the latest open batch for the project will be used. You can later reuse the same scene to create inputs for other projects and batches. "},{"title":"Creating Inputs Directly​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-inputs-directly","content":"It is often useful to create inputs directly instead of the 2-step process described above. To do this, you can simply pass the annotation information directly into the create method of the corresponding scene type. For example, to create an input in project-a and batch-2, you would do the following: client.cameras_sequence.create( ..., project=&quot;project-a&quot;, batch=&quot;batch-2&quot; )  This would trigger the scene creation process, and once the scene is created, inputs are created in all requests in the given batch. If the batch parameter is omitted, the latest open batch for the project will be used. We also provide a wrapper function create_inputs to help with this process, see Creating Multiple Inputs With One Call. "},{"title":"List Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#list-scenes","content":"note This feature is new in version 1.6.0 It can be useful to list scenes that have been uploaded to the Kognic Platform. One example is to check the status during scene creation. Scenes can be retrieved in the following way: scene_uuids = [&quot;cca60a67-cb68-4645-8bae-00c6e6415555&quot;, &quot;cc8776d0-f537-4094-8b11-8c2111741e2f&quot;] client.scene.get_scenes_by_uuids(scene_uuids=scene_uuids)  "},{"title":"Response​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#response","content":"The response is a list of Scene objects containing the following properties Property\tDescriptionuuid\tUUID used to identify the scene within the Kognic Platform external_id\tExternal ID supplied during scene creation scene_type\tType of scene (see Scene Types) status\tScene status (see Scene Statuses) created\tWhen the scene was created calibration_id\tCalibration used for the scene (if any) view_link\tA url to view the scene in the Kognic Platform error_message\tIf there is an error during scene creation the error message will be included, otherwise it's None "},{"title":"List Inputs​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#list-inputs","content":"note This feature is new in version 1.7.0 Inputs can be queried from the platform using the query_inputs method, which can be used in the following way examples/query_inputs.py loading... See full example on GitHub Additional filter parameters for querying inputs are listed below. Parameter\tDescriptionproject\tProject identifier to filter by batch\tWhich batch in the project to return inputs for scene_uuids\tReturn inputs using scenes matching the supplied uuids external_ids\tReturn inputs using scenes matching the supplied external_ids "},{"title":"Response​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#response-1","content":"The response is a list of Input objects containing the following properties Property\tDescriptionuuid\tID used to identify the input within the Kognic Platform scene_uuid\tID used to identify the scene that the input is using request_uid\tID used to identify the request that the input belongs to view_link\tA url to view the input in the Kognic Platform "},{"title":"Invalidate Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#invalidate-scenes","content":"note This feature is new in version 1.6.0 If issues are detected upstream related to scenes created, it is possible to invalidate them. This could be useful during development or if issues are detected with the data. Invalidating a scene means that it will be removed from requests, meaning that all inputs using the scene will be deleted. In turn invalidated scenes will not produce annotations and any completed annotations of the scene will be removed. There is no way to undo this operation so use with caution. from kognic.io.model.scene.invalidated_reason import SceneInvalidatedReason scene_uuids = [&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, &quot;37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36&quot;] reason = SceneInvalidatedReason.BAD_CONTENT client.scene.invalidate_scenes(scene_uuids, reason)  The following reasons are available when invalidating scenes: Reason\tDescriptionbad-content\tScene does not load, or has erroneous metadata such as invalid calibration duplicate\tIf the same scene has been created several times incorrectly-created\tIf the scene was unintentionally created. "},{"title":"Deleting Inputs​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#deleting-inputs","content":"note This feature is new in version 1.6.0 If issues are detected upstream related to inputs created, it is possible to delete them. This could be useful when the issues are related to the input itself and not the scene. One example would be if there are two inputs for a lidars and cameras scene, one where we want to annotate in 2D/3D and one where we only want to annotate in 2D. If the issue is an erroneous calibration the 2D input can still be used while the 2D/3D input should be deleted. Deleting an input means that no annotations will be produced for it and any completed annotations of the input will be removed. There is no way to undo this operation so use with caution. input_uuid = &quot;9c08f7a3-3216-4bd6-a41a-1dda6f66f53e&quot; client.input.delete_input(input_uuid)  "},{"title":"Invalidate Inputs (deprecated)​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#invalidate-inputs-deprecated","content":"Deprecated This has been deprecated in favor of Invalidate Scenes or Deleting Inputsand will be removed in the near future. Invalidation of an input means that it will be removed for all annotation types. See Annotation Typeson how to remove a specific annotation types for an input. from kognic.io.model.input.invalidated_reason_input import InvalidatedReasonInput invalid_uuids = [&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, &quot;37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36&quot;] reason = InvalidatedReasonInput.BAD_CONTENT client.input.invalidate_inputs(invalid_uuids, reason)  If issues are detected upstream related to inputs created, it is possible to invalidate inputs. Invalidated inputs will not produce annotations and any completed annotations of the input will be invalidated. See Invalidate Scenes for more information about invalidation reasons. "},{"title":"Creating Multiple Inputs With One Call​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-multiple-inputs-with-one-call","content":"note This feature is new in version 1.1.9 Since the input creation process is asynchronous, it is sometimes useful to wait for the inputs to be created before continuing. In order to do this, we provide a wrapper function create_inputs which can create multiple scenes and inputs, wait for them to be created (or failed) and yield the results. The function will block until it has a result to yield or all of the inputs have completed in one way or another. The function takes a list of SceneWithPreannotation(a new wrapper object containing a scene and optionally a pre-annotation) along with the normal input creation parameters. from kognic.io.tools.input_creation import create_inputs, SceneWithPreAnnotation, InputCreationStatus from kognic.io.model.scene import LidarsAndCamerasSequence from kognic.openlabel.models import OpenLabelAnnotation scenes_with_pre_annotations: List[SceneWithPreAnnotation] = [ SceneWithPreAnnotation( scene=LidarsAndCamerasSequence(...), preannotation=OpenLabelAnnotation(...) # Optional ), ... ] for input_result in create_inputs(client, scenes_with_pre_annotations, &quot;project-identifier&quot;, batch=&quot;batch-identifier&quot;): # Do something with the result if input_result.status == InputCreationStatus.CREATED: print(f&quot;Input {input_result.external_id} was created, got uuid {input_result.input_uuid}&quot;) elif input_result.status == InputCreationStatus.FAILED: print(f&quot;Input {input_result.external_id} failed to be created at stage {input_result.error.stage} with error {input_result.error.message}&quot;) else: print(f&quot;Input {input_result.external_id} is in status {input_result.status}&quot;)  Note that the functions also accepts the parameters wait_timeout and sleep_time which can be used to control the wait-behavior. The wait_timeout parameter specifies the maximum time to wait for the inputs to be created/failed, whilesleep_time specifies the time to sleep between each check. Units are in seconds. The time it takes for inputs to be created depends on their size and the number of inputs to be created so the wait_timeout should be set accordingly. The default value is 30 minutes, starting from the time when all scene jobs have been committed. "},{"title":"Waiting for Scene Creation​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#waiting-for-scene-creation","content":"It can sometimes be useful to wait for a scene to be created before continuing. This can be done by using below example in utils.py examples/utils.py loading... See full example on GitHub "},{"title":"Pre-annotations","type":0,"sectionRef":"#","url":"/docs/kognic-io/pre_annotations","content":"","keywords":""},{"title":"Creating pre-annotations using the kognic-io client​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#creating-pre-annotations-using-the-kognic-io-client","content":"There are 3 steps that are needed in order to create pre-annotations in the Kognic platform. Create a scene by uploading all the needed dataUpload an OpenLabel annotation as a pre-annotationCreate an input from the scene Note that these steps can be performed in one call with the create_inputs function, see Creating Multiple Inputs With One Call "},{"title":"1. Creating a scene​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#1-creating-a-scene","content":"Start by creating a scene from kognic.io.client import KognicIOClient client = KognicIOClient() # Create Scene but not input since we don't provide project or batch scene_response = client.lidars_and_cameras_sequence.create( lidars_and_cameras_seq, annotation_types=annotation_types, dryrun=dryrun )  Note that you now have to wait for the scene to be created before you can proceed to the next step. More information this can be found Waiting for Scene Creation. "},{"title":"2. Uploading an OpenLabel annotation​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#2-uploading-an-openlabel-annotation","content":"The pre-annotation can be uploaded to the Kognic platform once the scene has been created successfully. Load your OpenLabel annotation according to the documentation in kognic-openlabel and upload it to the Kognic platform as such: client.pre_annotation.create( scene_uuid=scene_response.scene_uuid, # from step 1 pre_annotation=OpenLabelAnnotation(...), dryrun=dryrun )  "},{"title":"3. Create the input​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3-create-the-input","content":"When the scene and pre-annotation have been successfully created, the input can be created. This will add it to the latest open batch in a project, or the specific batch that's specified, and be ready for annotation with the pre-annotation present. client.lidars_and_cameras_sequence.create_from_scene( scene_uuid=scene_response.scene_uuid, # from step 1 annotation_types=annotation_types, project=project, dryrun=dryrun )  "},{"title":"OpenLabel support​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#openlabel-support","content":"Pre-annotations use the OpenLabel format/schema but not all OpenLabel features are supported in pre-annotations. "},{"title":"Unsupported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#unsupported-pre-annotation-features","content":"These features or combinations of features are not currently supported, or only have partial support. Static geometries: not supported These are bounding boxes, cuboids, etc. declared in the OpenLabel under objects.*.objectData Geometry-specific attributes: not supported on 3D geometry These are attributes declared in the OpenLabel on a single geometric shape, in other words an attribute that only applies to the object as seen by one sensor; a common example is occlusion which is recorded separately for each camera.May also be referred to as source-, stream- or sensor-specific attributes.3D geometry is anything that can be drawn when annotating a pointcloud, e.g. cuboids.Geometry-specific attributes are permitted on 2D geometry e.g. bounding boxesNote that the task definition, must designate a property as source specific before it may be used in this way.The stream attribute is a special case and is excepted from this rule "},{"title":"Supported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#supported-pre-annotation-features","content":""},{"title":"Geometries​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#geometries","content":"note Objects cannot have multiple 3D geometries in the same frame Name\tOpenLABEL field\tDescription\tAttributesCuboid\tcuboid\tCuboid in 3D\t- Bounding box\tbbox\tBounding box in 2D\t- 3D line\tpoly3d\tLine in 3D. Append the first point at the end if you want it to be closed.\t- Polygon\tpoly2d\tPolygon in 2D\tis_hole Multi-polygon\tpoly2d\tMulti-polygon in 2D\tis_hole &amp; polygon_id Curve\tpoly2d\tCurve or line in 2D\tinterpolation_method 2D point\tpoint2d\tPoint\t- Group of 2D points\tpoint2d\tGroup of points\tpoint_class Note that all geometries should be specified under frames rather than in the root of the pre-annotation. 3D geometries should be expressed in the lidar coordinate system in the single-lidar case, but in the reference coordinate system in the multi-lidar case. The rotation of cuboids should be the same as that in exports. 2D geometries should be expressed in pixel coordinates. See coordinate systems for more information. "},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes","content":"TextNumBoolean For 2D geometry, attributes may be specified as geometry specific (aka source/sensor specific), or object specific. Attributes can be static (specified in the objects key) or dynamic (specified in the object_data for the object in the frame) and must be allowed by the task definition, if one exists. Geometry specific attributes (those which appear on a single shape within frames) must also be declared as such in the task definition; arbitrary properties cannot be used in a source-specific way. "},{"title":"Contexts​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#contexts","content":"Currently not supported. Contact Kognic if you need support for this or use regular attributes instead. "},{"title":"Frames​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#frames","content":"Every pre-annotation must contain frames with unique timestamps that are among the ones specified in the scene. The reason for this is that the timestamps are used to map the frame in the pre-annotation to the correct frame in the scene. In the static case, one frame should be used with timestamp 0. "},{"title":"Relations​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#relations","content":"Currently not supported. Contact Kognic if you need support for this or use regular attributes instead. "},{"title":"Streams​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#streams","content":"Every geometry must have the stream property specified. This property determines which stream (or sensor) that the geometry appears in. It is important that the stream is among the ones specified in the scene and of the same type, for example camera or lidar. "},{"title":"Sparseness​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness","content":"Pre-annotations can be sparse, meaning that its objects or geometries do not need to be present in every frame. Instead, they can be present in a subset of frames and then interpolated in the frames in between. Utilizing this feature can speed up the annotation process significantly for sequences. Sparseness can be accomplished in two different ways, either by using object data pointers or the boolean property interpolated. The former is the recommended way of doing it in most cases since it will lead to a more compact pre-annotation. The latter is useful when the pre-annotation is created from exported annotations from the Kognic platform. Interpolation is done by linearly interpolating the geometry values between key frames. This is done in pixel coordinates for 2D geometries. For 3D geometries, the interpolation can be done in either the frame local coordinate system or the world coordinate system (see Coordinate Systems). This is configured in the annotation instruction so reach out to the Kognic team about this if you are unsure. Note that interpolation in the world coordinate system is recommended but requires that the scene contains ego poses. "},{"title":"Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#object-data-pointers","content":"In OpenLABEL, object data pointers are used to create a specification for objects. For example, you can specify what attributes and geometries that are used for specific objects. In addition, you can specify for which frames that these are present. If a geometry is specified in the object data pointer, it will be present in all frames that the object data pointer is pointing to. If the geometry is not provided in some of these frames, it will be interpolated. Note that geometries mustbe provided for the first and last frame in the object data pointer. Otherwise, the pre-annotation will be rejected. One limitation is that a geometry must be in the same stream for all frames when using object data pointers. This is because interpolation is done in the stream coordinate system. If you need to use geometries of the same type in different streams, you can simply use different names for the geometries in the different streams. Sparseness with Object Data Pointers shows an example of how to use object data pointers. "},{"title":"Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#interpolated-property","content":"The boolean property interpolated can be used to specify that a geometry should be interpolated. Geometries are still required to be present in interpolated frames but their geometry values will be ignored. Note that interpolated geometries must have corresponding geometries (interpolated or not) in the first and last frame of the pre-annotation. Otherwise, the pre-annotation will be rejected. Using the interpolated property is the recommended way of doing it when the pre-annotation is created from exported annotations from the Kognic platform. Sparseness with Interpolated Property shows an example of how to use the interpolated property. "},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes-1","content":"Attributes are handled differently compared to geometries. If an attribute is not present in a frame, its last value will simply be used if the object (or geometry if the property is source-specific) is present in the frame. If the object is not present in the frame, the attribute will be ignored. Dense attributes will be sparsified automatically when the pre-annotation is uploaded to the Kognic platform. "},{"title":"Examples​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#examples","content":"Below follows examples of supported pre-annotations. "},{"title":"3D cuboid and 2D bounding box with a static property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-cuboid-and-2d-bounding-box-with-a-static-property","content":"{ &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot;, &quot;streams&quot;: { &quot;LIDAR1&quot;: {}, &quot;ZFC&quot;: {} } }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;ZFC&quot; }] }, &quot;name&quot;: &quot;Bounding-box-1&quot;, &quot;val&quot;: [1.0, 1.0, 40.0, 30.0] } ], &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;LIDAR1&quot; }] }, &quot;name&quot;: &quot;cuboid-89ac8a2b&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;, &quot;object_data&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; }] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;LIDAR1&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"3D line with a dynamic property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-line-with-a-dynamic-property","content":"{ &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [{ &quot;frame_end&quot;: 0, &quot;frame_start&quot;: 0 }], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;streams&quot;: { &quot;lidar&quot;: {} }, &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; }] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;line-3d-1&quot;, &quot;val&quot;: [ -5.0, 0.0, 0.0, -5.0, 10.0, 0.0, 5.0, 10.0, 0.0, 5.0, 0.0, 0.0, -5.0, 0.0, 0.0 ] } ], &quot;text&quot;: [{ &quot;name&quot;: &quot;occluded&quot;, &quot;val&quot;: &quot;No&quot; }] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;name&quot;: &quot;cc06aced&quot;, &quot;type&quot;: &quot;Region&quot; } }, &quot;streams&quot;: { &quot;lidar&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"Sparseness with Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-object-data-pointers","content":"In the example below the object 1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box called the-bbox-name that is provided in frames 0 and 3. In frames 1 and 2, the bounding box will be interpolated. { &quot;openlabel&quot;: { &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;car-name&quot;, &quot;type&quot;: &quot;car&quot;, &quot;object_data_pointers&quot;: { &quot;the-bbox-name&quot;: { &quot;type&quot;:&quot;bbox&quot;, &quot;frame_intervals&quot;: [{&quot;frame_start&quot;: 0, &quot;frame_end&quot;: 3}] } } } }, &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } }, &quot;1&quot;: {}, &quot;2&quot;: {}, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } } } } }  "},{"title":"Sparseness with Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-interpolated-property","content":"In the example below sparseness is determined using the interpolated property. The object1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box for which the interpolated property is set to true in frames 1 and 2 but not in frames 0 and 3. The geometry values in frames 1 and 2 are ignored and instead interpolated from the geometry values in frames 0 and 3. { &quot;openlabel&quot;: { ..., &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } }, &quot;1&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;2&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } } } } }  "},{"title":"OpenLABEL format","type":0,"sectionRef":"#","url":"/docs/openlabel/openlabel-format","content":"","keywords":""},{"title":"Rotation of Cuboids​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#rotation-of-cuboids","content":"The rotation is such that the y-axis is facing forwards, with a rotation order of XYZ. This means that a cuboid with a heading (yaw) equal to 0 is aligned with the y-axis in the positive direction along the axis. This is somewhat different compared to theISO 8855standard, where the forward direction is along the x-axis. Conversion to ISO 8855 can then be done by applying a rotation around the z-axis and changing sx and sy in the following way import math from typing import List from scipy.spatial.transform import Rotation def convert_to_iso8855(val: List[float]) -&gt; List[float]: &quot;&quot;&quot; Converts cuboid values to ISO 8855 &quot;&quot;&quot; [x, y, z, qx, qy, qz, qw, sx, sy, sz] = val rotation_1 = Rotation.from_quat([qx, qy, qz, qw]) rotation_2 = Rotation.from_rotvec([0, 0, math.pi / 2]) rot_object = rotation_1 * rotation_2 [qx, qy, qz, qw] = rot_object.as_quat() return [x, y, z, qx, qy, qz, qw, sy, sx, sz]  "},{"title":"Non-sequences are sequences with one frame​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#non-sequences-are-sequences-with-one-frame","content":"Due to reasons of simplicity we have made the choice to treat non-sequences in the same way as sequences. This means that non-sequences are represented as a sequence with only one frame. Only data such as name and type are defined in the top level element keys. All other information is stored under frames, see example below { &quot;objects&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot; } }, &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;0&quot;: {&quot;object_data&quot;: {...}} } } } }  "},{"title":"Stream is just another text property​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#stream-is-just-another-text-property","content":"The stream property is used to indicate which stream/sensor/source that the geometry och property was annotated in. For example here is an object with a point that has been annotated in a stream with the name Camera. Note that all corresponding attributes for the geometry have also been annotated in the same stream. { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;name&quot;: &quot;point-4d2d325f&quot;, &quot;val&quot;: [300.5300, 286.4396], &quot;attributes&quot;: { &quot;text&quot;: [ {&quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;Camera&quot;}, {&quot;name&quot;: &quot;Color&quot;, &quot;val&quot;: &quot;Black&quot;} ] } } ] } }  "},{"title":"Relations​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations","content":"Regarding changes on 2022-04-08 Some changes were made regarding how to represent certain types of relations on 2022-04-08. Contact Kognic in case your annotations were produced before this date, but you wish to include these changes anyways. We consider two types of relations; unidirectional relations between two objects and group relations. In addition to these, there is a need to represent false relations, i.e. relation properties that are not actually pointers to other objects but rather take values such as Inconclusive, Nothing or Unclear. "},{"title":"Relations are unidirectional​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations-are-unidirectional","content":"Relations are unidirectional, meaning that if an object, object1, has a relation to another object, object2, it does not mean that object2 has a relation to object1. Below follows an example where car-0 is following car-1 and it is unclear whether car-2 is following another car or not. Deprecated since 2022-04-08 Representing false relations using the relation uid is deprecated and has moved to the use of actions (see the next section) { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;car-1&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;car-2&quot;, &quot;type&quot;: &quot;Car&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;2&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;Unclear&quot;}] } } }  "},{"title":"Actions are used to represent false relations (new since 2022-04-08)​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#actions-are-used-to-represent-false-relations-new-since-2022-04-08","content":"In the Kognic Platform, there is support for assigning values to relations that are not actually references to other objects. Examples are Inconclusive and Nothing. Actions are used to represent these in the following way, where the name of the action determines the value and the type determines the property name. { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;Nothing&quot;, &quot;type&quot;: &quot;is_pulling_or_pushing&quot;} } }  "},{"title":"Groups are represented as actions​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#groups-are-represented-as-actions","content":"Deprecated since 2022-04-08 The group concept has been deprecated in favor of single relations between objects. This means that annotations produced after 2022-04-08 will no longer contain the group concept Group relations are relations where objects can be seen as belonging to a group. There is then a need for an abstract concept that describes the group. OpenLABEL suggests the use of actions for this in such a way that each object in the group has a relation of type isSubjectOfAction to this action. Below follows an example where two lane-0 and lane-1belong to the same road, while it is unclear whether lane-2 belongs to a road. { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;lane-1&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;lane-2&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;2&quot;: { &quot;name&quot;: &quot;2&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;1&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;Road&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;Unclear&quot;, &quot;type&quot;: &quot;Road&quot;} } }  "},{"title":"Stream specific relations​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#stream-specific-relations","content":"If a relation is stream specific, there will be a property stream_relations denoting which stream the list of relations belong to. { // frames.0 // ... &quot;frame_properties&quot;: { &quot;streams&quot;: { &quot;CAMERA_FRONT&quot;: { &quot;description&quot;: null, &quot;stream_properties&quot;: { &quot;stream_relations&quot;: { &quot;1&quot;: {} } } } } }, &quot;relations&quot;: { &quot;0&quot;: {} } }  "},{"title":"Representing polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-polygons","content":"Polygons are described by a list of Poly2d objects in OpenLABEL. One of these represents the exterior while the others represent potential holes and this is determined by the boolean property is_hole. Below follows an example of a polygon with one hole. { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). "},{"title":"Representing multi-polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-multi-polygons","content":"Multi-polygons are simply lists of polygons, so we describe these in a similar way with lists of Poly2d objects with the property is_hole. However, we also add one additional property polygon_id that determines which polygon a Poly2d object belongs to in the multi-polygon. Below follows an example of a multi-polygon with two polygons with one hole each. { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } }, { &quot;name&quot;: &quot;poly3&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly4&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). "},{"title":"Representing curves​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-curves","content":"caution The name of the interpolation method has changed from interpolation-method to interpolation_method. However, old annotations might still contain the old name Curves are represented using the poly2d geometry and the interpolation method is specified as a text property in the following way. { &quot;poly2d&quot;: [ { &quot;closed&quot;: false, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;name&quot;: &quot;curve-d633ca89&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;interpolation_method&quot;, &quot;val&quot;: &quot;natural-cubic-spline&quot; } ] } } ] }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). The property interpolation_method is mandatory and determines how the nodes should be associated to each other. The following values are supported: natural-cubic-splinecatmull-rom-0.5polyline "},{"title":"Representing 3D lanes​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-3d-lanes","content":"A 3D lane is represented as two lines in 3D (poly3d), one to the right and the other to the left. The text propertylane_edge determines whether the line is to the right or to the left. The lines will always have closed set to false. { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;left&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.2647494200238287, -51.51747573498745, -2.315540290283199, 1.0807419132566136, -48.91298533071834, -2.313640304199211, -0.0892715141237751, -34.705936676401016, -2.235569814758307, -0.4442893388935316, -29.60917111552865, -2.1894531147766174, -1.0952988968721313, -17.193981050037397, -2.1397902661132875 ] }, { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;right&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.5845765823868767, -51.49487958011918, -2.315540290283199, 1.4004322100638888, -48.888528958803036, -2.313640304199211, 0.23043085215069048, -34.68163859008775, -2.235569814758307, -0.12426061849402326, -29.589636067040036, -2.1894531147766174 ] } ] } }  "},{"title":"Representing 2D points​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-2d-points","content":"A 2D point is represented as a single point2d. Each point2d has an optional point_class attribute. For single points this may be ommited, but if set it must be equal to the type of the object. This attribute is reserved for future use on other point-based geometries. { &quot;openlabel&quot;: { ..., &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;a940239d-ff27-4480-8294-c482977a1b32&quot;: { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;point_class&quot;, &quot;val&quot;: &quot;APoint&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;stream1&quot; } ] }, ... } ] } }, &quot;e027e626-eb7a-4a8e-a9ae-083464e137d1&quot;: { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;stream1&quot; } ] }, .... } ] } } } } }, &quot;metadata&quot;: {...}, &quot;objects&quot;: { &quot;a940239d-ff27-4480-8294-c482977a1b32&quot;: { ... &quot;type&quot;: &quot;APoint&quot; }, &quot;e027e626-eb7a-4a8e-a9ae-083464e137d1&quot;: { ... &quot;type&quot;: &quot;AnotherPoint&quot; } }, ... } }  "},{"title":"Representing groups of 2d points​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-groups-of-2d-points","content":"A group of points is used when multiple points refere to the same object. The attribute point_class is required for each of the points in the point group, and the point_class has to be different from the object type. The point_class value &quot;line_reference_point&quot; is reserved for future use cases. "},{"title":"Representing Geometry Collections​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-geometry-collections","content":"note Introduced in kognic_format_version 2.2 Related documenation for the task viewhttps://docs.kognic.com/class-groups A collection of geometries as described in the link above will be represented as having a reserved relation as type geometry_collection. { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;name&quot;: &quot;bbox-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;: { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;curve-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;: { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;curve-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;: { &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane&quot;, &quot;val&quot;: &quot;right&quot; } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot;, &quot;kognic_format_version&quot;: &quot;2.2&quot; }, &quot;objects&quot;: { &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;: { &quot;name&quot;: &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_bbox&quot; }, &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;: { &quot;name&quot;: &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_line&quot; }, &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;: { &quot;name&quot;: &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_line&quot; }, &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;: { &quot;name&quot;: &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_collection&quot; } }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;rdf_objects&quot;: [ { &quot;uid&quot;: &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;, &quot;type&quot;: &quot;object&quot; }, { &quot;uid&quot;: &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;, &quot;type&quot;: &quot;object&quot; }, { &quot;uid&quot;: &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;, &quot;type&quot;: &quot;object&quot; } ], &quot;rdf_subjects&quot;: [ { &quot;uid&quot;: &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;, &quot;type&quot;: &quot;object&quot; } ], &quot;type&quot;: &quot;geometry_collection&quot; } } } }  "}]