{"searchDocs":[{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/introduction","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#prerequisites","content":" Before you begin, make sure you have:  Access to the dataset exploration toolAn account with permissions to use our APIGenerated API credentials. See API CredentialsInstalled our Python 3 SDK for authentication - kognic-auth  ","version":"Next","tagName":"h2"},{"title":"No API Client Available​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#no-api-client-available","content":" At the moment we do not provide an API client for the dataset exploration tool. Instead, we'll provide examples of how you can interact with our API.  ","version":"Next","tagName":"h2"},{"title":"Endpoints​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#endpoints","content":" You can discover the list of accessible endpoints within our swagger documentation.  ","version":"Next","tagName":"h2"},{"title":"Request Example​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-exploration/introduction#request-example","content":" Here's an example using the kognic-auth library to list all datasets available to the user:  import requests from kognic.auth.requests.auth_session import RequestsAuthSession client = RequestsAuthSession() try: response = client.session.get(&quot;https://dataset.app.kognic.com/v2/datasets&quot;) response.raise_for_status() data = response.json() print(data) except requests.exceptions.RequestException as e: print(f&quot;Request error: {e}&quot;)   (The v1 version of this endpoint, previously mentioned in this document, is now deprecated) ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/annotation-integration/introduction","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Introduction","url":"/docs/annotation-integration/introduction#prerequisites","content":" Before you begin, make sure you have:  An account with permissions to use our APIGenerated API credentials. See API Credentials.Installed our Python 3 SDK for authentication - kognic-auth  This page will also assume that:  the user has access to at least one project where at least one input batch has been uploaded. (See the client docs regarding these concepts here.)  ","version":"Next","tagName":"h2"},{"title":"API Client​","type":1,"pageTitle":"Introduction","url":"/docs/annotation-integration/introduction#api-client","content":" The kognic-io Python client covers a majority of the endpoints. For the case of missing coverage, we'll provide examples of how you can interact with our API with just kognic-auth and the requests library.  ","version":"Next","tagName":"h2"},{"title":"Endpoints​","type":1,"pageTitle":"Introduction","url":"/docs/annotation-integration/introduction#endpoints","content":" You can discover the list of accessible endpoints within ourswagger documentation.  ","version":"Next","tagName":"h2"},{"title":"Request Example​","type":1,"pageTitle":"Introduction","url":"/docs/annotation-integration/introduction#request-example","content":" Here's an example using the kognic-auth library to fetch the feedback error types available to the user:  import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://annotation-integration.app.kognic.com/v1/&quot; client = RequestsAuthSession() try: response = client.session.get(base_url + &quot;reviews/error-types&quot;) response.raise_for_status() data = response.json() print(data) except requests.exceptions.RequestException as e: print(f&quot;Request error: {e}&quot;)  ","version":"Next","tagName":"h2"},{"title":"Review","type":0,"sectionRef":"#","url":"/docs/annotation-integration/review","content":"","keywords":"","version":"Next"},{"title":"Posting a review​","type":1,"pageTitle":"Review","url":"/docs/annotation-integration/review#posting-a-review","content":" The current integration capabilities only allow posting reviews for delivered annotations. It is only available in requests where your organization is the producer. The annotations are identified using their open label UUID that can be found in the metadata (and in the file name) ofdownloaded annotations.  When posting a review, the API expects feedback data as well as a boolean accepted that indicates whether the quality is perceived as sufficient or not. The API also expects an enum workflow that details how the improvement should happen. For a rejected review and the workflow 'correct', a single correction task is triggered, with no follow-up Review task. The completed Correction task becomes the new delivered Annotation.  caution This integration API is currently limited to rejecting delivered annotations, the possible values of these input parameters are thus limited to accepted=false and workflow=&quot;correct&quot;. They are included in the interface at this stage in order to allow future expansions on the available use cases, workflows and actions.  A successful review post will return a UUID that identifies the review, and that can be used to fetch the posted feedback data. This will also create a task according to the chosen workflow. Depending on project set up, there might be a need for this to be coordinated with the project's managers.  Here's a example from kognic-io:  examples/add_review.py loading... View on GitHub  ","version":"Next","tagName":"h2"},{"title":"Posting a partial review​","type":1,"pageTitle":"Review","url":"/docs/annotation-integration/review#posting-a-partial-review","content":" A partial review is an incomplete review with optional feedback data, this capability may be used in order to achieve one or more of the following:  Prepare a review task with manual or machine generated feedbackSelect what should be reviewed  This feature is available in requests where your organization is theproducer or the owner. Like the capability to post a complete review, this feature is currently only available for delivered annotations.  When a partial review is posted, a review task will be created for the annotation. This review task will contain the feedback that may have been supplied when posting the partial review. The reviewer can then delete or invalidate any feedback they disagree with, or add their own feedback. It is currently not possible to override the workflow of a partial review, the default workflow is a review loop - where the correction of a rejected review task will be followed by a new review task. If the review is accepted by the reviewer, the annotation will be delivered again, updated to account for any potential changes the reviewer might have done.  ","version":"Next","tagName":"h2"},{"title":"Feedback​","type":1,"pageTitle":"Review","url":"/docs/annotation-integration/review#feedback","content":" A feedback item details something noteworthy in an annotation. In order to do this the following information is available to add, and will be available for annotators when improving the annotation:  Error type ID - a UUID selected from the list of available error types (fetch these using KognicIOClient().review.get_error_types())Description - a string describing what should be improvedSensor ID - the identifier of the sensor where the error appearsFrame ID - the identifier of the frame the error appears. In our OpenLabel file this is frame.frame_properties.external_idObject ID - the identifier of a particular object that is subject to this particular feedbackPin - a pointer to a specific area where something is of interest, this can be used to indicate missing objects (a pin contains mandatory x and y coordinates with an optional z coordinate, the coordinate unit in images are pixels and is sensor specific in 3d data)Suggested properties - when inappropriate property values are discovered, this field can be used to indicate a more appropriate valueMetadata - this field may contain any data, it could be used to identify the version of the system that generated the feedback ","version":"Next","tagName":"h2"},{"title":"Understand what your dataset contains","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/understand-dataset-content","content":"","keywords":"","version":"Next"},{"title":"Understand which scenes you have annotations for in your dataset​","type":1,"pageTitle":"Understand what your dataset contains","url":"/docs/dataset-exploration/understand-dataset-content#understand-which-scenes-you-have-annotations-for-in-your-dataset","content":" To be able to know which scenes you can provide predictions for you need to understand which scenes you've annotations for in your dataset. This can be done by using an endpoint for listing annotations in your dataset. You're currently only able to add scenes indirectly to your dataset by adding annotations to the dataset. Please contact us if this is something you want help with or if you want to connect scenes to your dataset. The following code snippet by utilizing an endpoint for will do this and save it to a csv file.  from kognic.auth.requests.auth_session import RequestsAuthSession import csv base_url = &quot;https://dataset.app.kognic.com&quot; client = RequestsAuthSession() dataset_uuid = YOUR_DATASET_UUID done = False offset = 0 annotations = [] while not done: print(f&quot;Fetched {offset} annotations&quot;) res = client.session.get( f&quot;{base_url}/v1/datasets/{dataset_uuid}/annotations?offset={offset}&quot; ) res_annotations = res.json()['data'] annotations += res_annotations offset += len(res_annotations) if len(res_annotations) == 0: done = True print(f&quot;Fetched {len(annotations)} annotations&quot;) # save to csv called dataset_{dataset_uuid}_annotations.csv with open(f'dataset_{dataset_uuid}_annotations.csv', 'w', newline='') as csvfile: writer = csv.writer(csvfile) writer.writerow(annotations[0].keys()) for annotation in annotations: writer.writerow(annotation.values())  ","version":"Next","tagName":"h2"},{"title":"The prediction format","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/prediction-format","content":"The prediction format info This page has moved here","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/introduction","content":"Introduction info This page has moved here","keywords":"","version":"Next"},{"title":"Uploading predictions","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/uploading-predictions","content":"Uploading predictions info This page has moved here","keywords":"","version":"Next"},{"title":"Download Annotations","type":0,"sectionRef":"#","url":"/docs/download-annotations/","content":"Download Annotations After annotations are ordered for a scene they will be produced, reviewed and quality-controlled within the Kognic Platform. Once the annotations are made delivery-ready and are exported, they can be downloaded in the OpenLABEL format. Annotations for a Scene from kognic.io.client import KognicIOClient client = KognicIOClient() for annotation in client.annotation.get_annotations_for_scene(scene_uuid=scene_uuid): openlabel_dict = annotation.content # ... This method returns all delivered annotations in the form of Annotation objects, containing the OpenLABEL json, using a Scene UUID. Annotations for a Project or Batch from kognic.io.client import KognicIOClient client = KognicIOClient() # Get generator with annotations annotations = client.annotation.get_project_annotations(project=&quot;project-identifier&quot;, batch=&quot;batch-identifier&quot;, annotation_type=&quot;annotation-type&quot;) for annotation in annotations: openlabel_dict = annotation.content # ... This example fetches annotations for an entire project or batch. This retrieves Annotation objects for all delivered annotations for the given project, batch and annotation_type.","keywords":"","version":"Next"},{"title":"Data requirements","type":0,"sectionRef":"#","url":"/docs/getting-started/data-reqs","content":"","keywords":"","version":"Next"},{"title":"Images​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#images","content":" We currently support the following image formats: png, jpg, jpeg, webp and avif.  ","version":"Next","tagName":"h2"},{"title":"Point clouds​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#point-clouds","content":" Kognic uses a potree format internally to represent and present point clouds, this means that uploaded point cloud data needs to be converted into this format before it can be used as scene in the system. We currently support automatic conversion of the following formats: pcd, csv and las. The converter does not however exhaustively support all possible versions of these formats, see below for details of each format.  A timestamp field must always be present in point clouds, both in single-frame and sequence scenes, but the values are irrelevant if motion compensation is not enabled. An intensity field may be provided in point clouds and will be preserved during conversion. If omitted, the intensity for all points will be zero. Color and other auxiliary data that is not used in the platform is currently discarded in the conversion to potree.  A more detailed description of the point cloud formats is found here  ","version":"Next","tagName":"h2"},{"title":"Calibrations​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#calibrations","content":" Scenes with 2D and 3D data across various coordinate systems need calibrations to align sensors by location and orientation. Both an extrinsic calibration that maps the position and rotation in 3D relative to the reference system and an intrinsic camera calibration that projects the 3D points to camera's image plane. All extrinsic calibrations shall represent the transformation from the sensor to the reference system.  Types of calibrations All calibrations detail a sensor’s 3D position and orientation relative to the reference system, the calibrations shall map the transformation from the sensor to the reference system. They also map 3D points to the camera’s image plane. For LiDAR/RADAR, there is only one type of calibration available, read more hereFor cameras, we support different types of standard camera calibrations, where you have to provide the intrinsic parameters of the camera. All camera calibration are implemented using the OpenCV coordinate system. Unsupported camera model If your camera model is not supported, you can also provide a custom camera calibration where you provide the implementation in the form of a WebAssembly module.  ","version":"Next","tagName":"h2"},{"title":"Ego vehicle poses​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#ego-vehicle-poses","content":" An ego vehicle pose can optionally be added to each frame which describes the relative pose. It is highly recommended for 3D sequence annotations as it enables more efficient workflows and functions in the Kognic platform, especially for static objects.  The pose is represented using a 3D position and a quaternion in the local coordinate system. For a single lidar input the poses shall be on the lidar coordinate system. For multi-lidar inputs the poses shall be on the reference coordinate system  Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z  In addition to the frame poses, there is also the option to upload higher frequency IMU data to enable motion compensation. More details on motion compensation can be found here  ","version":"Next","tagName":"h2"},{"title":"FAQ​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#faq","content":" ","version":"Next","tagName":"h2"},{"title":"How do I check the if the calibration is correct​","type":1,"pageTitle":"Data requirements","url":"/docs/getting-started/data-reqs#how-do-i-check-the-if-the-calibration-is-correct","content":" Follow the instructions in the link to check your calibration. You can also quickly check the orientation of your camera quickly by opening a scene and go to the 3D viewer. At the ego vehicle there is a small yellow circle representing the position and a red arrow representing the orientation of the currently selected camera. Check that the arrow is pointing in the direction that you expect, for example in front of the ego vehicle. ","version":"Next","tagName":"h3"},{"title":"Uploading predictions","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/uploading-predictions","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#introduction","content":" In this example, we'll walk you through how to upload predictions using our API into an already existing dataset.  Before you begin: See Prerequisites and learn about the prediction format.  ","version":"Next","tagName":"h2"},{"title":"Steps​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#steps","content":" Create a new python file and import the following libraries:  import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession()   ","version":"Next","tagName":"h2"},{"title":"1. Get the UUID of the dataset​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#1-get-the-uuid-of-the-dataset","content":" You can either access the tool and copy the UUID following dataset/ in the URL, or utilize the datasets endpoint to get the uuid of the dataset:  client.session.get(&quot;https://dataset.app.kognic.com/v2/datasets&quot;)   ","version":"Next","tagName":"h3"},{"title":"2. Get the UUID of an existing predictions group or create a new one​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#2-get-the-uuid-of-an-existing-predictions-group-or-create-a-new-one","content":" 2.a Get the UUID of an existing predictions group​  In order to upload predictions, a prediction group needs to exist. Predictions can be organized into groups for any purpose imaginable. The UUID of an existing prediction group can be found in the URL after predictions/ or by using the endpoint  client.session.get(base_url + f&quot;/datasets/{datasetUuid}/predictions-groups&quot;)   2.b Creating a predictions group (optional)​  For datasets not containing segmentation tasks, a new prediction group can be creaited either by clicking in the app (Manage Predictions in the upper right corner and then + Create predictions group), or by using the following code snippet  path = base_url + f&quot;/datasets/{datasetUuid}/predictions-groups&quot; body = {&quot;name&quot;: &quot;My predictions group&quot;, &quot;description&quot;: &quot;A description of my new predictions group&quot;} try: response = client.session.post(path, json=body) response.raise_for_status() response_json = response.json() print(f&quot;Created predictions group with uuid {response_json['data']}&quot;) except requests.exceptions.RequestException as e: msg = e.response.text print(f&quot;Request error: {e}. {msg}&quot;)   Special case: Segmentation datasets  Predictions groups connected to segmentation datasets mus be created using the code snippet, and requires also one extra parameter called classMapping. The mapping is used when calculating disagreement between predictions and annotations and will impact the sorting as well as how disagreements appear in the gallery. The classMapping parameter is a list of dictionaries, where each dictionary contains the keys annotated and predicted. The annotatedkey is the class name in the annotations, and the predicted key is the class name in the predictions.{&quot;annotated&quot;: &quot;oak&quot;, &quot;predicted&quot;: &quot;tree&quot;} if you have annotated different species of trees, but only predict wether it is a tree or not.  All class names in the predictions and the annotations must be present in the class mappings, even if they don't need to be mapped. In the annotations, non-segmented areas are labeled with the class name _background.  example_body = { &quot;name&quot;: &quot;My predictions group&quot;, &quot;description&quot;: &quot;A description of my new predictions group&quot;, &quot;classMapping&quot;: [ {&quot;annotated&quot;: &quot;oak&quot;, &quot;predicted&quot;: &quot;tree&quot;}, {&quot;annotated&quot;: &quot;_background&quot;, &quot;predicted&quot;: &quot;not_tree&quot;}, {&quot;annotated&quot;: &quot;only_in_annotations&quot;} ] }   ","version":"Next","tagName":"h3"},{"title":"3. Upload predictions​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-exploration/uploading-predictions#3-upload-predictions","content":" For a small amount of predictions, synchronous calls might work  import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession() predictions_group_uuid = &quot;...&quot; openlabel_content = {&quot;openlabel&quot;: ...} data = { &quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content, } try: response = client.session.post( base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot;, json=data ) response.raise_for_status() response_json = response.json() print(f&quot;Created prediction with uuid {response_json['data']}&quot;) except requests.exceptions.RequestException as e: msg = e.response.text print(f&quot;Request error: {e}. {msg}&quot;)   For larger amounts of predictions, asynchronous calls are recommended. The following example uses the async client from the kognic-auth library to make 100 asynchronous calls:  import asyncio from kognic.auth.httpx.async_client import HttpxAuthAsyncClient base_url = &quot;https://dataset.app.kognic.com/v1/&quot; predictions_group_uuid = &quot;...&quot; url = base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot; openlabel_content = {&quot;openlabel&quot;: ...} MAX_CONNECTIONS = 10 async def upload_prediction(payload, session, sem): async with sem: response = await session.post(url, json=payload) response.raise_for_status() return response.json().get(&quot;data&quot;) async def main(n_runs: int): client = HttpxAuthAsyncClient() session = await client.session sem = asyncio.Semaphore(MAX_CONNECTIONS) tasks = [] for i in range(n_runs): payload = {&quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content} task = upload_prediction(payload, session, sem) tasks.append(task) responses = await asyncio.gather(*tasks) await session.aclose() print(responses) if __name__ == '__main__': asyncio.run(main(100))   Setting MAX_CONNECTIONS to something bigger than 10 might not work and is not recommended. ","version":"Next","tagName":"h3"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/getting-started/quickstart","content":"","keywords":"","version":"Next"},{"title":"Installation​","type":1,"pageTitle":"Quickstart","url":"/docs/getting-started/quickstart#installation","content":" Install via pip:  kognic-iokognic-auth Install kognic-io for the most common use cases like file uploads and project management. bash pip install --upgrade kognic-io   ","version":"Next","tagName":"h2"},{"title":"Generating credentials​","type":1,"pageTitle":"Quickstart","url":"/docs/getting-started/quickstart#generating-credentials","content":" In the main platform, credentials can be generated by clicking here in the user menu.    ","version":"Next","tagName":"h2"},{"title":"Setting credentials​","type":1,"pageTitle":"Quickstart","url":"/docs/getting-started/quickstart#setting-credentials","content":" Set the environment variable KOGNIC_CREDENTIALS to point to it.  export KOGNIC_CREDENTIALS=~/path/to/credentials.json   ","version":"Next","tagName":"h2"},{"title":"Verify credentials​","type":1,"pageTitle":"Quickstart","url":"/docs/getting-started/quickstart#verify-credentials","content":" To verify that they are correct you can simply list projects in Python:  note Since kognic-io v2.4.0 you can provide a workspace id that will be used for writes. For users with multiple workspaces this will be mandatory. See this page for instructions.  kognic-iokognic-auth from kognic.io.client import KognicIOClient client = KognicIOClient() client.project.get_projects() print(&quot;success&quot;)   You should now be set up to interact with the Kognic Platform.  ","version":"Next","tagName":"h2"},{"title":"Next step​","type":1,"pageTitle":"Quickstart","url":"/docs/getting-started/quickstart#next-step","content":" Upload your first scene: Quick guide on how to upload data to the Kognic Platform ","version":"Next","tagName":"h2"},{"title":"Key Concepts","type":0,"sectionRef":"#","url":"/docs/key-concepts","content":"","keywords":"","version":"Next"},{"title":"Project​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#project","content":" Project is the top-most concept when interfacing with the Kognic Platform. It is possible to have multiple ongoing projects, and they act as a container for other Kognic resources. Project setup is usually performed by the Kognic Professional Services team during the Guideline Agreement Process (GAP) of a new client engagement.  Within the Kognic APIs, projects are identified using an external identifier.  ","version":"Next","tagName":"h2"},{"title":"Batch​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#batch","content":" Input batches allow grouping of input data into smaller batches within a project. By default, every project has at least one input batch.  Ongoing projects can be benefited from having multiple batches in two ways:  Group input data that are collected during a certain time intervalPerform guideline or task definition changes without the need for retroactive changes.  ","version":"Next","tagName":"h2"},{"title":"Batch Status​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#batch-status","content":" Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed.  ","version":"Next","tagName":"h3"},{"title":"Request​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#request","content":" During GAP, projects can have several annotation types as the end goal. For example, a project consisting of images can be assigned for both lane detection and object annotation. Within Kognic, a Request represents a specific annotation goal for a given input. We divide big and complex projects into several independent annotation types. This makes it possible to:  Reduce the cognitive load on the annotatorsMore annotators can work on the same data in parallelSimplify user interfaces  All of these contribute to a high level of quality while also reducing the total time needed for producing an annotation.  ","version":"Next","tagName":"h2"},{"title":"Guideline​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#guideline","content":" In order to produce annotations, one needs to know what to annotate and how. This type of information is found in Guideline. A guideline defines what specific object to mark (e.g. vehicles and pedestrians), as well as how (e.g. bounding box). A guideline also includes detailed information about how to interpret the data, e.g. what it means by a vehicle is &quot;heavily occluded&quot;.  ","version":"Next","tagName":"h3"},{"title":"Task Definition​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#task-definition","content":" Task Definition describes what should/could be annotated. How many object types? Bounding box, semantic segmentation or lines/splines for each object type? What are the properties for each object type? Task definitions are json files that the Kognic Professional Services team generates from the guideline. The task definition is then used by the Kognic App to construct the appropriate drawing tool. In other words, task definition can be understood as the machine readable quivalent of a guideline.  ","version":"Next","tagName":"h3"},{"title":"Scene​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#scene","content":" Before setting up any annotation task, the raw data needs to be correctly uploaded to the Kognic Platform. The scene specifies how data from different sources are combined together. Resources are images and point clouds, as well as metadata and calibrations (define sensors' properties). We support different types of setup, for example:  Image(s) from a (multiple) camera(s)Image(s) from camera(s) combined with lidar point clouds  Another concept related to scene is frame. A frame is a discrete moment of a scene in time. Scenes can be either single frame or sequence (multiple frames). Sequence should be used when temporal information is important for producing the annotation.  ","version":"Next","tagName":"h2"},{"title":"Scene Types​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#scene-types","content":" Type\tDescriptionCameras\tA single frame consisting of image(s) from 1-9 cameras LidarsAndCameras\tA single frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras CamerasSeq\tA sequence of frames, each frame consisting of image(s) from 1-9 cameras LidarsAndCamerasSeq\tA sequence of frames, each frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras AggregatedLidarsAndCamerasSeq\tA sequence of frames, each frame consisting of 1-20 lidar point clouds and image(s) from 1-9 cameras. However, point clouds are aggregated over time, producing a unified point cloud.  ","version":"Next","tagName":"h3"},{"title":"Input​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#input","content":" Once a scene has been uploaded to the Kognic Platform, one can create annotation tasks as inputs where each input is associated to a request. Differenciate input from scene enables efficient reuse of the uploaded data. For instance, multiple inputs can be created from the same scene enabling different kinds of annotation setups.  Note that one can create an input simultaneously when creating a scene by providing the project/batch that it should be associated to, see examples in Working with Scenes and Inputs.  ","version":"Next","tagName":"h2"},{"title":"Annotation​","type":1,"pageTitle":"Key Concepts","url":"/docs/key-concepts#annotation","content":" An annotation is produced when inputs are successfully annotated in a request. Annotations are provided by kognic-io API as json objects in ASAM OpenLABEL format. More information on how to download these annotations along with some examples of how to work with them is available in the Downloading Annotations chapter.  Apart from kognic-io API, Kognic also provides a library called kognic-openlabel, which makes it easy to parse and work with the OpenLABEL json objects. ","version":"Next","tagName":"h2"},{"title":"Standard Camera Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/cameras-standard","content":"","keywords":"","version":"Next"},{"title":"Common​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#common","content":" All camera calibrations have the following attributes  Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z camera_matrix\tA CameraMatrix object\tfx, fy, cx, cy image_width\tInteger\tNA image_height\tInteger\tNA field_of_view\tFloat\tNA  ","version":"Next","tagName":"h3"},{"title":"Pinhole​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#pinhole","content":" The PINHOLE camera model expands the common model with:  Key\tValue\tParametersdistortion_coefficients\tA DistortionCoefficients object\tk1, k2, p1, p2, k3  examples/calibration/create_pinhole_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Fisheye​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#fisheye","content":" The Fisheye camera model expands the PINHOLE model with the following  Key\tValue\tParametersxi\tFloat\tNA  examples/calibration/create_fisheye_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Kannala​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#kannala","content":" The KANNALA camera model changes and expands the PINHOLE with the following  Key\tValue\tParametersdistortion_coefficients\tA KannalaDistortionCoefficients object. The distortion parameters k3, k4, if available, can be assigned to p1 and p2 respectively. That is p1=k3 and p2=k4.\tk1, k2, p1, p2 undistortion_coefficients\tA UndistortionCoefficients object.\tl1, l2, l3, l4  examples/calibration/create_kannala_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Principal point distortion​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#principal-point-distortion","content":" The principal point distortion model consists of the common attributes plus  Key\tValue\tParametersprincipal_point_distortion_coefficients\tA PrincipalPointDistortionCoefficients object\tk1, k2 lens_projection_coefficients (optional. Default to values for model SF806)\tA LensProjectionCoefficients object\tc1, c2,c3, c4,c5, c6 distortion_center\tA DistortionCenter object\tx, y principal_point\tA PrincipalPoint object\tx, y  examples/calibration/create_principal_point_distortion_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Fused cylindrical​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#fused-cylindrical","content":" The fused cylindrical model consists of the common attributes plus  Key\tValue\tParameterscut_angles_degree\tA CutAngles object. Note these angles should be expressed in degrees.\tupper, lower vertical_fov_degree (optional. Default 72.5 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA horizontal_fov_degree (optional. Default 93 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA max_altitude_angle_degree (optional. Default 90 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA  examples/calibration/create_fused_cylindrical_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Cylindrical​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#cylindrical","content":" The cylindrical model consists only of the common attributes. There are no extra attributes to set for this model.  examples/calibration/create_cylindrical_calibration.py loading... View on GitHub  ","version":"Next","tagName":"h3"},{"title":"Principal point fisheye​","type":1,"pageTitle":"Standard Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-standard#principal-point-fisheye","content":" The principal point fisheye model consists of the common attributes plus  Key\tValue\tParametersprincipal_point_fisheye_coefficients\tA PrincipalPointFisheyeCoefficients object\talpha_l, alpha_r, beta_u, beta_l  examples/calibration/create_principal_point_fisheye_calibration.py loading... View on GitHub ","version":"Next","tagName":"h3"},{"title":"Advanced setup","type":0,"sectionRef":"#","url":"/docs/kognic-apis","content":"","keywords":"","version":"Next"},{"title":"Authentication​","type":1,"pageTitle":"Advanced setup","url":"/docs/kognic-apis#authentication","content":" Authentication is handled by kognic-auth, a Python 3 library providing foundations for Kognic Authentication on top of the requests library.  The authentication builds on the standard Oauth 2.0 Client Credentials flow. There are a few ways to provide authentication credentials to our API clients. Kognic Python clients such as in kognic-query or kognic-io accept an auth parameter that can be set explicitly. Alternatively, one can set environment variables that point to the Kognic credentials file. See examples below.  ","version":"Next","tagName":"h2"},{"title":"Generating Credentials​","type":1,"pageTitle":"Advanced setup","url":"/docs/kognic-apis#generating-credentials","content":" The credentials file that contains the Kognic Client ID and the Kognic Client secret, can be generated in the Kognic web application by clicking on &quot;Api Credentials...&quot; in the user menu, followed by clicking on the &quot;Generate Credentials&quot; button.    The credentials file should be saved in an appropriate directory, such as ~/.config/kognic/credentials.json.  ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Advanced setup","url":"/docs/kognic-apis#examples","content":" There are a few different ways to set your credentials in auth.  Set the environment variable KOGNIC_CREDENTIALS to point to your Kognic Credentials file, by issuing the command export KOGNIC_CREDENTIALS=~/.config/kognic/credentials.jsonSet the environment variables KOGNIC_CLIENT_ID and KOGNIC_CLIENT_SECRETSet the auth parameter to the credentials file path, such as auth=&quot;~/.config/kognic/credentials.json&quot;Set the auth parameter to credentials tuple, such as auth=(&lt;client_id&gt;, &lt;client_secret&gt;)  By default, Kognic API clients use the credentials set in environment variable(s). To create an authenticated kognic-io client, assuming the environment variable(s) are set correctly, simply do:  from kognic.io.client import KognicIOClient api_client = KognicIOClient()   Otherwise, one can override the credentiails explicitly:  from kognic.io.client import KognicIOClient api_client = KognicIOClient(auth=&quot;~/.config/kognic/credentials.json&quot;)   Under the hood, they all use the AuthSession class which is implements a requests session with automatic token refresh.  ","version":"Next","tagName":"h2"},{"title":"Specifying a workspace​","type":1,"pageTitle":"Advanced setup","url":"/docs/kognic-apis#specifying-a-workspace","content":" From kognic-io v2.4.0 you can specify a workspace to use when uploading data, for users with access to multiple workspaces this is mandatory.  To find the workspace you want to upload data to, select it from the drop down:    click on workspace managment:    and find the copy option in the top right hand corner:    When you create your kognic-io client you can now specify this id and it will be used when uploading data:  from kognic.io.client import KognicIOClient api_client = KognicIOClient(write_workspace_id=&lt;Some workspace uuid&gt;)   ","version":"Next","tagName":"h2"},{"title":"Proxy Configuration​","type":1,"pageTitle":"Advanced setup","url":"/docs/kognic-apis#proxy-configuration","content":" If your organizations' network policy requires HTTP(S) traffic to be proxied out via a specific host, then you should configure this via your OS or execution environment. kognic-io uses Python's urllib which will pick up the proxy configuration from your OS and environment variables.  The correct proxy host/address to use depends on the network configuration within your organization, so reach out to your internal IT support for details.  For example:  export HTTPS_PROXY='http://10.9.8.7:1234'  ","version":"Next","tagName":"h2"},{"title":"Lidar Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/lidars","content":"Lidar Calibrations A LIDAR calibration is represented as a LidarCalibration object and consists of a position expressed with three coordinates and a rotation in the form of a Quaternion. Optionally, the sensor's field of view may be specified by providing an object that has a sweep start angle and sweep stop angle. The field of view may also optionally include the depth to which the field extends. Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z field_of_view (optional)\tA LidarFieldOfView object\tstart_angle_deg, stop_angle_deg and optionally depth See the code example below for creating a base LidarCalibration object. examples/calibration/create_lidar_calibration.py loading... View on GitHub","keywords":"","version":"Next"},{"title":"Custom Camera Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/cameras-custom","content":"","keywords":"","version":"Next"},{"title":"The WebAssembly module​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#the-webassembly-module","content":" The WebAssembly must follow a strict interface where the module exports a function called project_point_to_image. The function must take three arguments of type float64 and return two values of type float64. Thus, the WebAssembly text representation of this interface is func (param f64 f64 f64) (result f64 f64). The three arguments are the x, y and z coordinates of the 3D point. The two return values are the x and y coordinates of the projected point in the image plane. WebAssembly doesn't support multiple return values by default but this can be enabled with the multi-valueproposal.  If the point is not within the field of view, the function should return NaN for both the x and y coordinates.  ","version":"Next","tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#validation","content":" note This requires wasmtime to be installed which is an optional of dependency kognic-io. Run pip install kognic-io[wasm] to install it.  We provide validation code both as python functions and via the kognicutil cli. We validate things such as but not limited to  The module can be loadedThe function exists and has the correct signatureThat a point can be projected using the moduleThat points are projected correctly if test cases are provided  In python there are three different ways to validate a calibration  import kognic.io.tools.calibration.validation as wasm_validation from kognic.io.model.calibration.camera.custom_camera_calibration import CustomCameraCalibration, Point2d, Point3d, TestCase test_cases = [ TestCase( point3d=Point3d(x=1.0, y=2.0, z=3.0), point2d=Point2d(x=2.0, y=5.6) ), TestCase( point3d=Point3d(x=1.0, y=1.0, z=-1.0), point2d=Point2d(x=float(&quot;nan&quot;), y=float(&quot;nan&quot;)) # point is outside field of view ) ] wasm_file = &quot;/path/to/calibration.wasm&quot; calibration = CustomCameraCalibration.from_bytes(wasm_file, test_cases=test_cases, ...) wasm_bytes = calibration.to_bytes() # Validate the calibration object wasm_validation.validate_custom_camera_calibration(calibration, test_cases=test_cases) # Validate the wasm file wasm_validation.validate_wasm_file(wasm_file, test_cases=test_cases) # Validate the wasm binary wasm_validation.validate_wasm_bytes(wasm_bytes, test_cases=test_cases)   The kognicutil cli can be used as follows  kognicutil wasm validate calibration.wasm   ","version":"Next","tagName":"h2"},{"title":"Compilation​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#compilation","content":" caution Rust 1.82.0 removes support for the multivalue feature target (returning multiple values from a function). Since this feature is currently needed for custom camera calibrations to work, the Rust and/or Cargo version needs to be pinned to &lt; 1.82.0.  note It is recommended to keep the wasm file as small as possible. Try to avoid dependencies that are not needed. For example, it may be preferred to implement some mathematical functions yourself instead of using the standard library.  As stated above the WebAssembly module must follow a strict interface and compilation requires the multi-value proposal. We provide a set of utilities that will make it easier to compile the WebAssembly module from a few languages, see table below.  Language\tTarget\tCompilation tool\tRequired versionRust\t*.rs\trustc\t&lt; 1.82.0 Rust (Cargo)\tCargo.toml\tcargo\t&lt; 1.82.0 C++\t*.cc, *.cpp\temscripten\tN/A C\t*.c\temscripten\tN/A  The utilities are available both as python functions and via the kognicutil cli. From Python, you can compile the module with  from kognic.io.tools.calibration.compilation import compile_to_wasm wasm_binary = compile_to_wasm(&quot;path/to/source&quot;)   The returned binary can then be used to create a CustomCameraCalibration object. If the output_wasm parameter is passed, the binary will be saved to the specified path. The kognicutil cli can be used as follows  kognicutil wasm compile path/to/source path/to/output.wasm   Note that, validation is run by default after compilation. This can be disabled with the --skip-validation flag.  Calibration parameters have to be embedded in the binary so that they can be used by the WebAssembly module. Try to pre-compute as much as possible to increase the speed of the projection function at runtime.  Below follows examples of a simplified version of the pinhole calibration in a few different languages.  ","version":"Next","tagName":"h2"},{"title":"Example: Rust​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-rust","content":" note Rust needs to be installed with the wasm32-wasi target for this. Install Rust according to the instructionshere and then add the target with rustup target add wasm32-wasi.  A Rust filecan be compiled with  kognicutil wasm compile path/to/source.rs path/to/output.wasm   Note that panics are not supported and compilation will fail if the code contains it.  Rust with Cargo​  note Rust and Cargo need to be installed with the wasm32-wasi target for this. Install Rust and Cargo according to the instructions here and then add the target with rustup target add wasm32-wasi.  A Rust module with Cargocan be compiled with  kognicutil wasm compile path/to/source/Cargo.toml path/to/output.wasm   Note that it is important to specify that the library is a cdylib and it is also recommended to set strip = true to reduce the size of the WebAssembly module. This is done by adding the following to the Cargo.toml file  [lib] crate-type = [&quot;cdylib&quot;] [profile.release] strip = true   ","version":"Next","tagName":"h3"},{"title":"Example: C++​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-c","content":" note Emscripten needs to be installed for this, which can be done according to the instructionshere.  A C++ filecan be compiled with  kognicutil wasm compile path/to/source.cc path/to/output.wasm   or with  kognicutil wasm compile path/to/source.cpp path/to/output.wasm   ","version":"Next","tagName":"h3"},{"title":"Example: C​","type":1,"pageTitle":"Custom Camera Calibrations","url":"/docs/kognic-io/calibrations/cameras-custom#example-c-1","content":" note Emscripten needs to be installed for this, which can be done according to the instructionshere.  A C filecan be compiled with  kognicutil wasm compile path/to/source.c path/to/output.wasm  ","version":"Next","tagName":"h3"},{"title":"Calibrations Overview","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations/overview","content":"","keywords":"","version":"Next"},{"title":"How to create a calibration​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#how-to-create-a-calibration","content":" See this example of how to create a calibration for a LIDAR sensor and two camera sensors of type Pinhole. For other camera types as Kannala, Fisheye etc, see kognic-io-examples.  from kognic.io.client import KognicIOClient from kognic.io.model import SensorCalibration from kognic.io.model.calibration.camera.common import CameraMatrix, DistortionCoefficients from kognic.io.model.calibration.camera.pinhole_calibration import PinholeCalibration from kognic.io.model.calibration.common import Position, RotationQuaternion from kognic.io.model.calibration.common import Position, RotationQuaternion from kognic.io.model.calibration.lidar.lidar_calibration import LidarCalibration, LidarFieldOfView client = KognicIOClient() # Create a sample calibration for lidar. lidar_position = Position(x=0.0, y=0.0, z=0.0) lidar_rotation = RotationQuaternion(w=1.0, x=0.0, y=0.0, z=0.0) lidar_fov = LidarFieldOfView(start_angle_deg=315, stop_angle_deg=45, depth=200) lidar_calibration = LidarCalibration(position=lidar_position, rotation_quaternion=lidar_rotation, field_of_view=lidar_fov) # Create a sample calibration for a Pinhole camera. For other camera types visit: # https://github.com/annotell/kognic-io-examples-python/tree/master/examples/calibration camera_position = Position(x=0.0, y=0.0, z=0.0) camera_rotation = RotationQuaternion(w=0.5, x=-0.5, y=0.5, z=-0.5) camera_camera_matrix = CameraMatrix(fx=3450, fy=3250, cx=622, cy=400) camera_distortion_coefficients = DistortionCoefficients(k1=0.0, k2=0.0, p1=0.0, p2=0.0, k3=1.0) pinhole_calibration = PinholeCalibration( position=camera_position, rotation_quaternion=camera_rotation, camera_matrix=camera_camera_matrix, distortion_coefficients=camera_distortion_coefficients, image_height=1080, image_width=1920, field_of_view=190.0, ) sensor_calibration = SensorCalibration( external_id=&quot;Create your own id here&quot;, # to make it easier for you to find the calibration later calibration={ # The keys: &quot;lidar&quot;, &quot;LEFT_FRONT_CAMERA&quot; etc, must match the # names of the sensors in the scenes that use this calibration &quot;lidar&quot;: lidar_calibration, &quot;LEFT_FRONT_CAMERA&quot;: pinhole_calibration, &quot;RIGHT_FRONT_CAMERA&quot;: pinhole_calibration } ) created_calibration = client.calibration.create_calibration(sensor_calibration) print(f&quot;Created calibration with id {created_calibration.id}&quot;)   reuse calibration Note that after a calibration has been created you can, and should, reuse the same calibration for multiple scenes if possible, see below.  Get calibrations Existing calibrations can be fetched with the retrieved id or with the provided external id. This can either be done via the client in Python or via kognicutil. Pythonkognicutil # Fetch calibration by id client.calibration.get_calibration(&quot;e95980c4-fee2-4b91-9316-1bffbcb5a896&quot;) # Fetch calibration by external id client.calibration.get_calibrations(external_id=&quot;Collection 2020-06-16&quot;)   ","version":"Next","tagName":"h2"},{"title":"Next steps​","type":1,"pageTitle":"Calibrations Overview","url":"/docs/kognic-io/calibrations/overview#next-steps","content":" Lidar calibrations: Learn about lidar calibrationsStandard camera calibrations: Learn about the different types standard camera calibrations that are supportedCustom camera calibrations: Learn how to create a custom camera calibrationUpload your first scene: Quick guide on how to upload scenes to the Kognic Platform ","version":"Next","tagName":"h2"},{"title":"Errors & Troubleshooting","type":0,"sectionRef":"#","url":"/docs/kognic-io/error_handling","content":"","keywords":"","version":"Next"},{"title":"HTTP Error Handling​","type":1,"pageTitle":"Errors & Troubleshooting","url":"/docs/kognic-io/error_handling#http-error-handling","content":" When the client sends a http request to the API and waits until it receives a response. If the response code is 2xx(the status code for a successful call) the client converts the received message into a python object which can be viewed or used. However, if the API responds with an error code (4xx or 5xx) the python client will raise an error. It's up to the user to decide if and how they want to handle this error.  ","version":"Next","tagName":"h2"},{"title":"Request Timeouts​","type":1,"pageTitle":"Errors & Troubleshooting","url":"/docs/kognic-io/error_handling#request-timeouts","content":" When uploading scenes with Kognic IO Client it is sometimes that case that network conditions lead to timeout errors. Kognic IO uses asynchronous IO to multiplex uploads and make best use of the available bandwidth, and will retry failed connections many times, but sometimes some steps need to be taken on the client side to control the behaviour and rule out certain issues:  Lower the total number of connections that Kognic IO may open at one time via the constructor param KognicIOClient(max_connections = N). This affects the maximum size of the internal connection pool. With the max set to 1, the uploads are effectively serial.Increase the timeout for connections that Kognic IO opens via the constructor param KognicIOClient(timeout = N), in seconds. This affects the connect and read timeout of connections. This will allow more time for connections to be established across the internet and for the server to send an initial response. ","version":"Next","tagName":"h2"},{"title":"The prediction format","type":0,"sectionRef":"#","url":"/docs/dataset-exploration/prediction-format","content":"","keywords":"","version":"Next"},{"title":"Supported prediction features​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#supported-prediction-features","content":" The current API for uploading predictions supports the following geometries:  Name\tOpenLABEL field\tDescriptionCuboid\tcuboid\tCuboid in 3D Bounding box\tbbox\tBounding box in 2D Bitmaps (segmentation)\timage\tSegmentation bitmap for images  The rotation of cuboids should be the same as that in exports (see coordinate systems for more information). 2D geometries should be expressed in pixel coordinates.  For this API, the relevant parts (keys) are frames, objects, streams, ontologies and metadata. The last one (metadata) is the easiet one, and should just read schema_version&quot;: &quot;1.0.0&quot; (see examples below for full context). Also stream is straightforward, and should specify what sensors (cameras, lidars, ...) there are and what their name, like sensor_name: {&quot;type&quot;: &quot;camera&quot;} or sensor_name: {&quot;type&quot;: &quot;lidar&quot;}. Again, see the examples below for full context.  All parts of a prediction that is time-varying throughout a sequence is described in frames, such as corodinates and dynamic properties. Each frame in the sequence is represented by a key-value pair under frames. The key is the frame_id, and the value should look like  frame_id: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { ... } }   The value for frame_properties.timestamp (measured in ms, recommended to set to 0 for non-sequence data) will be used for matching each predicted frame to the relevant annotated frame, and must therefore match the scene that has been annotated. We recommend that frame_id (a string) follows the frame_id used to describe the underlying scene, although frame_properties.timestamp will take precedence in case of mismatch. In case of non-sequence data, a good choice for frame_id is &quot;0&quot;. The values for frame_properties.external_id and frame_properties.stream will be resolved automatically if left empty as shown.  The key objects in turn contains key-value pairs, where each such pair is basically an object in that frame. Note that there is the key objects in each frame, as well as in the root. They describe basically the same objects, but the information that is potentially time-varying (i.e. frame-specific, such as coordinates) belongs to the frame, whereas static information (such as the object class) belongs in the root. The object keys (strings) are arbitrary, but must match the keys in the different objects if they are describing the same object.  Please refer to the examples below on how to describe the objects in detail. For cuboids and bounding boxes, an existence confidence can be provided by specifying the frame-specific attribute confidence. It must be a numeric value between 0.0 and 1.0, and will be set to 1.0 if left empty. If provided, it must be defined as a numeric value. The static object_data.type will show up as the class name in the tool.  For segmentation bitmaps, the image itself is a grayscale 8-bit PNG image of the same resolution as the annotated images (if the actual prediction only partially cover the annotated image or is of lower resolution, it has to be padded and/or upscaled). The image itself is supplied in the openlabel by pasting its base64-encoding as a string as an object to a frame. See the example below. Moreover, also an ontology has to be supplied which describes what class corresponds to each color level. With an 8-bit grayscale image, it is possible to encode up to 256 classes. The ontologycan be left out for non-segmentation predictions.  The camera_id in the examples below must match the id of the sensors in the annotated scene, whereas the corresponding id for the lidar sensor should be set to @lidar.  ","version":"Next","tagName":"h2"},{"title":"Prediction examples​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#prediction-examples","content":" ","version":"Next","tagName":"h2"},{"title":"2D bounding box in two frames with a static property color​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#2d-bounding-box-in-two-frames-with-a-static-property-color","content":" In OpenLabel, a bounding box is represented as a list of 4 values: [x, y, width, height], where x and y are the center coordinates of the bounding box. The width and height are the width and height of the bounding box. The xand y coordinates are relative to the upper left corner of the image.  { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;camera_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;val&quot;: [ 1.0, 1.0, 40.0, 30.0 ] } ] } } } }, &quot;1&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 50, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.82, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;camera_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;val&quot;: [ 2.0, 3.0, 30.0, 20.0 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;camera_id&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }   ","version":"Next","tagName":"h3"},{"title":"3D cuboid in two frames with a static property color​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#3d-cuboid-in-two-frames-with-a-static-property-color","content":" Cuboids are represented as a list of 10 values: [x, y, z, qx, qy, qz, qw, width, length, height], where x, y, and z are the center coordinates of the cuboid. x, y, z, width, length, and height are in meters.qx, qy, qz, and qw are the quaternion values for the rotation of the cuboid.  Read more about coordinate systems and quaternions here.  { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;@lidar&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } }, &quot;1&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 50, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.87, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;@lidar&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;val&quot;: [ 3.123312801361927, -20.285740376586913, 0.0649137773513349, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;@lidar&quot;: { &quot;type&quot;: &quot;lidar&quot; } } } }   ","version":"Next","tagName":"h3"},{"title":"A single frame segmentation bitmap​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#a-single-frame-segmentation-bitmap","content":" Transforming, upscaling, padding and base64-encoding a small color-image to a larger grayscale image using Python PIL​  This code example gives an example of how to go from a multicolor prediction bitmap image of resolution 300 x 200 to a grayscale image of resolution 1000 x 800, by first converting to grayscale, then rescaling the prediction to 600 x 400 and then padding equally on the sides. It also includes code for base64-encoding the image as a string, that later can be used in the openlabel. This code only makes use of built-in numpy functions, but is not optimized for performance.  import base64 import io import numpy as np from PIL import Image # The original mapping used to produce the images original_mapping = { (0,0,0): &quot;_background&quot;, (255,0,0): &quot;class_1&quot;, (0,0,255): &quot;class_2&quot;, } # The grayscale mapping (this will also be the ontology in the openlabel) grayscale_mapping = { &quot;_background&quot;: 0, &quot;class_1&quot;: 1, &quot;class_2&quot;: 2, } prediction = Image.open(&quot;my_original_prediction_file.png&quot;) # Let's say this has resolution 300 x 200 def lookup(pixel_color): return grayscale_mapping[original_mapping[tuple(pixel_color)]] # convert to grayscale via numpy array lookup prediciton_numpy = np.array(prediction) grayscale_prediction_numpy = np.vectorize(lookup, signature=&quot;(m)-&gt;()&quot;)(prediciton_numpy) grayscale_prediction = Image.fromarray(grayscale_prediction_numpy.astype(np.uint8)) # upscale to another resolution upscaled_grayscale_prediction = grayscale_prediction.resize((600, 400), resample=Image.Resampling.NEAREST) # padding by first constructing a new background image of target size, and then paste the prediction in the right position padded_grayscale_prediction = Image.new(&quot;L&quot;, (1000, 800), 0) padded_grayscale_prediction.paste(upscaled_grayscale_prediction, (201, 201)) image_bytes = io.BytesIO() padded_grayscale_prediction.save(image_bytes, format=&quot;PNG&quot;) prediction_str = base64.b64encode(image_bytes.getvalue()).decode(&quot;utf-8&quot;)   Openlabel for a segmentation bitmap​  The prediction_str and grayscale_mapping can thereafter be used in the openlabel like  { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;: { &quot;object_data&quot;: { &quot;image&quot;: [ { &quot;name&quot;: &quot;a_human_readable_name&quot;, &quot;val&quot;: prediction_str, &quot;mime_type&quot;: &quot;image/png&quot;, &quot;encoding&quot;: &quot;base64&quot;, &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;val&quot;: &quot;camera_id&quot;, &quot;name&quot;: &quot;stream&quot; } ] } } ] } } }, &quot;frame_properties&quot;: { &quot;streams&quot;: {}, &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot; }, } }, &quot;objects&quot;: { &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;: { &quot;name&quot;: &quot;07d469f9-c9ab-44ec-8d09-0c72bdb44dc2&quot;, &quot;type&quot;: &quot;segmentation_bitmap&quot; } }, &quot;streams&quot;: { &quot;camera_id&quot;: { &quot;type&quot;: &quot;camera&quot; } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;ontologies&quot;: { &quot;0&quot;: { &quot;classifications&quot;: {str(v): k for k, v in grayscale_mapping.items()}, &quot;uri&quot;: &quot;&quot; } } } }   If providing predictions for multiple cameras in the scene, the list of images could be extended.  ","version":"Next","tagName":"h3"},{"title":"Using kognic-openlabel to validate the format​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-exploration/prediction-format#using-kognic-openlabel-to-validate-the-format","content":" See kognic-openlabel for more information. ","version":"Next","tagName":"h3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/kognic-io/FAQ","content":"","keywords":"","version":"Next"},{"title":"Receiving requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: ... when trying to create scenes​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#receiving-requestsexceptionshttperror-403-client-error-forbidden-for-url--when-trying-to-create-scenes","content":" This implies that the authenticated user does not have access to the endpoint being called. Make sure you're authenticating correctly. If a Kognic user, make sure client_organization_id is specified on the KognicIOClient.  ","version":"Next","tagName":"h3"},{"title":"How do I know that my scene was created successfully?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-do-i-know-that-my-scene-was-created-successfully","content":" Whenever a .create(...) call for a scene has been successfully made it's (asynchronously) submitted for pre-processing in the Kognic platform. The scene is available only once the pre-processing has been successfully executed. However, pre-processing can also fail, for example if the pointcloud or image files are poorly formatted or corrupt.  The easiest way to check the status of a scene is the scene status field present on scenes returned by the method get_scenes_by_uuids(...). The scene is successfully created and available in the platform once the status is set to created.  note Since pre-processing is an asynchronous process it might take a while before the scene changes status from processing to either created or failed.  # Example code of how to check if a scene has been successfully created resp = client.cameras.create(...) [scene] = client.scene.get_scenes_by_uuids([resp.scene_uuid]) # Successfully created and available once status is `created` print(f'Scene {scene.uuid} status:', scene.status)   ","version":"Next","tagName":"h3"},{"title":"How can I view my scene?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-can-i-view-my-scene","content":" Successfully created scenes can be viewed in the Kognic platform via their view-link. The view-link can be accessed via the view_link field present on scenes returned by the method get_scenes_by_uuids(...).  # Example code of how to access view-links for a scene [scene] = client.scene.get_scenes_by_uuids([resp.scene_uuid]) (f&quot;Scene {scene.external_id} view-link: {scene.view_link}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Why are the cuboids rotated by 90 degrees?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#why-are-the-cuboids-rotated-by-90-degrees","content":" The coordinate system is defined by the uploaded data, but the rotation is defined by Kognic. This is somewhat different (90-degree rotation) compared to the ISO 8855 standard. See Rotation of Cuboids for more about this and how you can convert to ISO 8855. ","version":"Next","tagName":"h3"},{"title":"Scene Feature Flags","type":0,"sectionRef":"#","url":"/docs/kognic-io/feature_flags","content":"","keywords":"","version":"Next"},{"title":"Currently Supported Features​","type":1,"pageTitle":"Scene Feature Flags","url":"/docs/kognic-io/feature_flags#currently-supported-features","content":" ","version":"Next","tagName":"h2"},{"title":"PointCloudFeatures​","type":1,"pageTitle":"Scene Feature Flags","url":"/docs/kognic-io/feature_flags#pointcloudfeatures","content":" Flag\tDefault state\tDescriptionMOTION_COMPENSATION\tEnabled\tCauses motion compensation of point clouds using IMU data. ","version":"Next","tagName":"h3"},{"title":"Coordinate Systems","type":0,"sectionRef":"#","url":"/docs/kognic-io/coordinate_systems","content":"","keywords":"","version":"Next"},{"title":"The reference coordinate system and calibrations​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-reference-coordinate-system-and-calibrations","content":" Each sensor has its own coordinate system in 3D space that depends on its location and orientation on the ego vehicle. Being able to transform measurements between these sensor coordinate systems is important. To do this, a reference coordinate system is defined which works as a middle man between the sensor coordinate systems. The reference coordinate system can be chosen arbitrarily relative to the ego vehicle. By defining a calibration function CiC_iCi​ for sensor iiiwe can map a point xi⃗\\vec{x_i}xi​​ to the reference coordinate system in the following way  xR⃗=Ci(xi⃗)\\vec{x_R} = C_i(\\vec{x_i})xR​​=Ci​(xi​​)  In the same way we can map points from all other sensors to the reference coordinate system. Subsequently, we can also map a point from coordinate system iii to coordinate system jjj by applying the inverse of the calibration  xj⃗=Cj−1(Ci(xi⃗))\\vec{x_j} = C_j^{-1}(C_i(\\vec{x_i}))xj​​=Cj−1​(Ci​(xi​​))  ","version":"Next","tagName":"h2"},{"title":"The world coordinate system and ego motion data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-world-coordinate-system-and-ego-motion-data","content":" With this, we can now express points in coordinate systems local to the ego vehicle. This is great, but sometimes it is also valuable to express points recorded at different times in the same coordinate system. We call this the world coordinate system since it is static in time. We can transform a point to the world coordinate system using ego motion data, which describes the location and orientation of the ego vehicle at any given time. With the ego motion data we can transform a point xt⃗\\vec{x_t}xt​​ to the world coordinate system with  xw⃗=Et(xt⃗)\\vec{x_w} = E_t(\\vec{x_t})xw​​=Et​(xt​​)  Subsequently, we can also transform a point recorded at time ttt to the coordinate system at time t′t't′ by applying the inverse of the ego transformation function  xt′⃗=Et′−1(Et(xt⃗))\\vec{x_{t'}} = E_{t'}^{-1}(E_t(\\vec{x_t}))xt′​​=Et′−1​(Et​(xt​​))  This can be used to compensate each lidar point for the motion of the ego vehicle, a process also known asmotion compensation. It is highly recommended to motion compensate point clouds since lidar points are recorded at different instants in time. This can be done by providing high frequency ego motion data (IMU data) when creating a scene.  ","version":"Next","tagName":"h2"},{"title":"Single-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#single-lidar-case","content":" The image below displays how the different sensors relate to each other in 3D space in the single-lidar case. Note that the ego motion data should be expressed in the lidar coordinate system.    ","version":"Next","tagName":"h2"},{"title":"Multi-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#multi-lidar-case","content":" In the multi-lidar case (see image below) there are multiple point clouds, each in their own lidar coordinate system. These are merged into one point cloud in the reference coordinate system during scene creation since it's more efficient to annotate one point cloud rather than several. If IMU data is available, we can also compensate for the ego motion so that each point is transformed to the reference coordinate system at the frame timestamp. This is done by applying  xw⃗=Et(Ci(xi,t⃗))xt′⃗=Et′−1(xw⃗)\\vec{x_w} = E_t(C_i(\\vec{x_{i,t}})) \\\\ \\vec{x_{t'}} = E_{t'}^{-1}(\\vec{x_{w}})xw​​=Et​(Ci​(xi,t​​))xt′​​=Et′−1​(xw​​)  where xi,t⃗\\vec{x_{i,t}}xi,t​​ is the point expressed in the lidar coordinate system of lidar iii at time ttt and xt′⃗\\vec{x_{t'}}xt′​​is the point expressed in the reference coordinate system at the frame time t′t't′. It is recommended to provide IMU data so that motion compensation can be utilized. Since the merged point cloud is expressed in the reference coordinate system we also expect any ego motion data to be expressed in the reference coordinate system.    ","version":"Next","tagName":"h2"},{"title":"Different coordinate systems for different kinds of data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#different-coordinate-systems-for-different-kinds-of-data","content":" Different kinds of data are expressed in different coordinate systems depending on whether it's single-lidar or multi-lidar. This is summarized in the table below where we can see that ego motion data should be expressed in the lidar coordinate system in the single-lidar case but in the reference coordinate system in the multi-lidar case for example.  Type of data\tSingle-lidar\tMulti-lidarLidar point clouds\tLidar\tLidar Ego poses &amp; IMU data\tLidar\tReference OpenLABEL export 3D geometries\tLidar\tReference OpenLABEL export 2D geometries\tPixel\tPixel Pre-annotations 3D geometries\tLidar\tReference Pre-annotations 2D geometries\tPixel\tPixel ","version":"Next","tagName":"h2"},{"title":"Supported file formats","type":0,"sectionRef":"#","url":"/docs/kognic-io/file_formats","content":"","keywords":"","version":"Next"},{"title":"Images​","type":1,"pageTitle":"Supported file formats","url":"/docs/kognic-io/file_formats#images","content":" We currently support the following image formats: png, jpg, jpeg, webp and avif.  ","version":"Next","tagName":"h2"},{"title":"Point clouds​","type":1,"pageTitle":"Supported file formats","url":"/docs/kognic-io/file_formats#point-clouds","content":" Kognic uses a potree format internally to represent and present point clouds, this means that uploaded point cloud data needs to be converted into this format before it can be used as scene in the system. We currently support automatic conversion of the following formats: pcd, csv and las. The converter does not however exhaustively support all possible versions of these formats, see below for details of each format.  A timestamp field must always be present in point clouds, both in single-frame and sequence scenes, but the values are irrelevant if motion compensation is not enabled.  An intensity field may be provided in point clouds and will be preserved during conversion. If omitted, the intensity for all points will be zero.  Color and other auxiliary data that is not used in the platform is currently discarded in the conversion to potree.  Column names must be exact The column names must be provided as documented below. If they are not, the conversion will fail. In the case of timestamps, default values of 0 will be used instead.  ","version":"Next","tagName":"h2"},{"title":"PCD​","type":1,"pageTitle":"Supported file formats","url":"/docs/kognic-io/file_formats#pcd","content":" The currently supported format includes the following header:  VERSION .7 FIELDS x y z intensity timestamp SIZE 4 4 4 4 8 TYPE F F F U U COUNT 1 1 1 1 1 WIDTH &lt;w&gt; HEIGHT &lt;h&gt; VIEWPOINT 0 0 0 1 0 0 0 POINTS &lt;n&gt; DATA ascii   Apart from ascii as DATA type, we also support binary and binary_compressed. Note that we currently don't support organized point clouds in the binary_compressed case, i.e. when HEIGHT is not equal to 1.  ","version":"Next","tagName":"h3"},{"title":"CSV​","type":1,"pageTitle":"Supported file formats","url":"/docs/kognic-io/file_formats#csv","content":" We currently only support the following exact header and using , as separation character (where intensity is uint8,ts_gps is an uint64 and x, y, z are all float32):  ts_gps,x,y,z,intensity   All other formats will fail.  ","version":"Next","tagName":"h3"},{"title":"LAS​","type":1,"pageTitle":"Supported file formats","url":"/docs/kognic-io/file_formats#las","content":" We currently support version 1.2 and point format id 3, as defined in the las 1.2 specification. All other formats will cause the conversion to fail. ","version":"Next","tagName":"h3"},{"title":"Projects","type":0,"sectionRef":"#","url":"/docs/kognic-io/project","content":"","keywords":"","version":"Next"},{"title":"Project​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#project","content":" A Kognic project must first be set in order to create inputs. Projects are usually configured by the Kognic Professional Services team, during the Guideline Agreement Process (GAP) of a new client engagement.  ","version":"Next","tagName":"h2"},{"title":"List Projects​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#list-projects","content":" Projects can be listed either via the client in Python or via the kognicutil CLI.  Pythonkognicutil projects = client.project.get_projects()   ","version":"Next","tagName":"h3"},{"title":"Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch","content":" Input batches allow further grouping of inputs into smaller batches within a project. Specifying batch during the input creation is optional, and will otherwise be the latest open batch by default.  ","version":"Next","tagName":"h2"},{"title":"Batch Status​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch-status","content":" Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed.  ","version":"Next","tagName":"h3"},{"title":"Listing Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#listing-batches","content":" Batches within a project can be listed either via the client in Python or via the kognicutil CLI.  Pythonkognicutil batches = client.project.get_project_batches(project=&quot;&lt;project-external-id&gt;&quot;)   ","version":"Next","tagName":"h3"},{"title":"Creating Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#creating-batches","content":" To create a new batch in the open state within a project  project_batch = client.project.create_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, )   The newly created batch will contain the same Annotation Types as the latest batch by default.  This method has an optional flag publish_previous_batches which defaults to False. By setting this flag toTrue, as shown in the example below, all previous batches in the &quot;open&quot; state would be published and you would no longer be able to upload new inputs to those batches. You should therefore be certain that you no longer need to upload more inputs to the previous batches if you use this flag.  project_batch = client.project.create_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, publish_previous_batches=True, )   Contact Kognic before use Kognic usually helps with creating batches before a client becomes autonomous, in order to avoid any confusion please contact Kognic before you start using this feature.  ","version":"Next","tagName":"h3"},{"title":"Publish Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#publish-batch","content":" project_batch = client.project.publish_batch( project=&quot;&lt;project_external_id&gt;&quot;, batch=&quot;&lt;batch_external_id&gt;&quot;, )   When the input batch is published, the status of the batch will be set to &quot;ready&quot;. Published batches are not open for new inputs any longer. A project with multiple open batches will require you to specify which open batch to target when creating new inputs, whereas a project with a single open batch will allow you omit the batch parameter when creating inputs. ","version":"Next","tagName":"h3"},{"title":"Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/cameras","content":"Cameras A Cameras consists of a single frame of camera images, where the frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the Cameras object please check the section related to Scene Overview. examples/cameras.py loading... View on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":"","version":"Next"},{"title":"Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/cameras_seq","content":"Cameras Sequence A CamerasSeq consists of a sequence of camera images, where each frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the CamerasSeq object please check the section related to Scene Overview. examples/cameras_seq_images.py loading... View on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":"","version":"Next"},{"title":"Lidars and Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_and_cameras","content":"Lidars and Cameras A LidarsAndCameras consists of a single frame which contains 1-9 cameras images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the LidarsAndCameras object please check the section related to Scene Overview. examples/lidars_and_cameras.py loading... View on GitHub Use dryrun to validate Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it.","keywords":"","version":"Next"},{"title":"Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq","content":"","keywords":"","version":"Next"},{"title":"Providing Ego Vehicle Motion Information​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq#providing-ego-vehicle-motion-information","content":" Ego vehicle motion (i.e. the position and rotation of the ego vehicle) is optional information that can be provided when creating LidarsAndCamerasSeqs. This information can enable a massive reduction in the time it takes to annotate static objects. Ego vehicle motion information is provided by passing a EgoVehicleMotion object to each Framein the scene.  examples/lidars_and_cameras_seq_full.py loading... View on GitHub  Coordinate Systems Note that both position and rotation for ego vehicle pose are with respect to the local coordinate system.  ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/kognic-io/overview","content":"","keywords":"","version":"Next"},{"title":"Different types of scenes​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#different-types-of-scenes","content":" A scene represents a grouping of sensor data (e.g. camera images, lidar pointclouds) that should be annotated together. Any information necessary to describe the relationship between the sensors and their captured data is also specifed in the scene, be it camera resolution, sensor name, and the frequency at which the data was recorded at, etc.  There are different scene types depending on what kind of sensor(s) are used to represent the contents of the scene. For example, if one wants to create a scene only consisting of image data from camera sensors then one would use the scene type Cameras. Similarly, if one wants to create a scene consisting of both lidar and camera sensors then one would use the scene type LidarsAndCameras. Additionally, scenes can either be single frame or sequence type.  ","version":"Next","tagName":"h2"},{"title":"Sequential vs non-sequential​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sequential-vs-non-sequential","content":" Sequential scenes represent a sequence of frames in time, whereas non-sequential scenes only contain one snapshot of the sensor data. The sequential relationship is expressed via a sequence of Frames, where each Frame contains information related to what kind of sensor data constitues the frame (e.g. which image and/or pointcloud is part of the Frame) as well as a relative timestamp that captures where in time (relative to the other frames) the Frame is located.  Non-sequential scenes only contains a single Frame and do not require any relative timestamp information.  Sequential scene types are identified by the suffix Seq in their type name.  The following scene types are currently supported  CamerasLidarsAndCamerasCamerasSeqLidarsAndCamerasSeqAggregatedLidarsAndCamerasSeq  ","version":"Next","tagName":"h3"},{"title":"Scene Fields​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#scene-fields","content":" Non-sequential scenes have the following structure  class Scene(BaseModel): external_id: str frame: Frame sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)   Sequential scenes are similarly represented, except that they instead contain a list of Frames  class SceneSeq(BaseModel): external_id: str frames: List[Frame] sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)   ","version":"Next","tagName":"h2"},{"title":"External Id​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#external-id","content":" A scene automatically gets a UUID when it is created. This UUID is used as the primary identifier by Kognic and all of our internal systems. Additionally, an external id is required as an identifier when creating the scene in order to make communication around specific scenes easier.  ","version":"Next","tagName":"h3"},{"title":"Sensor Specification​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sensor-specification","content":" The sensor specification contains information about the camera and/or lidar sensors used in the scene.  The additional fields are optional, and can be used to specify the order of the camera images and a human readable sensor name (e.g. &quot;Front Camera&quot; instead of &quot;FC&quot;) when viewed in the Kognic annotation App.  As an example, let's say we have three camera sensors R, F and L positioned on the ego vehicle. Creating a sensor specification would be  from kognic.io.model import SensorSpecification sensor_spec = SensorSpecification( sensor_to_pretty_name={ &quot;R&quot;: &quot;Right Camera&quot;, &quot;F&quot;: &quot;Front Camera&quot;, &quot;L&quot;: &quot;Left Camera&quot; }, sensor_order=[&quot;L&quot;, &quot;F&quot;, &quot;R&quot;] )   sensor_order configures the order of camera images, and sensor_to_pretty_name affects the labels when viewed in the Kognic annotation App.  ","version":"Next","tagName":"h3"},{"title":"Calibration​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#calibration","content":" Any scene consisting of lidar and camera sensors requires a calibration. The calibration specifies the spatial relationship (position and rotation) between the sensors, and the camera intrinsic parameters.  However, scenes without a lidar sensor do not require a calibration.  Calibration is used by the Kognic annotation App to project regions in the pointcloud when a camera image is selected, and, similarly, to project the selected object (e.g. point, cuboid) in the pointcloud onto the images .  When creating a calibration, all sensors must match those present on the scene. If this is not the case the scene will not be created and a validation error will be returned by the Kognic API.  Detailed documentation on how to create calibrations via the API is present in the Calibration section.  ","version":"Next","tagName":"h3"},{"title":"Metadata​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#metadata","content":" Metadata can be added to scenes via the metadata field. It consists of flat key-value pairs, which means that nested data structures are not allowed. Metadata can be used to include additional information about a scene. Metadata cannot be seen by the annotators, but there are some reserved keywords that can alter the behaviour of the Kognic annotation tool. Reserved keywords can be found in the MetaData object in the python client.  ","version":"Next","tagName":"h3"},{"title":"Frame​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#frame","content":" The Frame object specifies the binary data to be annotated (.jpg, .png, .las etc) as well as which sensor the data originated from. Note that the Frame object is different for each scene type, even though the overall structure is similar (see details below).  Non sequential frame​  As an example, let's say we want to create a scene consiting of images from three camera sensors R, F and L. The corresponding binary data are in the files img_cam_R.jpg, img_cam_F.jpg and img_cam_F.jpg. This would correspond the scene type Cameras.  from kognic.io.model.scene.resources import Image from kognic.io.model.scene.cameras import Cameras, Frame cameras_scene = Cameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ] ) )   Similarly, if we also had an associated lidar pointcloud from the sensor VDL-64 and a corresponding binary file scan_vdl_64.las, we would instead use the scene type LidarsAndCameras. Note that the Frame class shall be imported under the corresponding scene type.  from kognic.io.model.scene.resources import Image, PointCloud from kognic.io.model.scene.lidars_and_cameras import LidarsAndCameras, Frame lidars_and_cameras = LidarsAndCameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ], point_clouds=[ PointCloud(&quot;scan_vdl_64.las&quot;, sensor_name=&quot;VDL-64&quot;) ] ) )   Sequential frames​  Sequential scene takes a list of Frame objects instead of a single Frame. In addition, the Frame object associated with sequential scenes have three additional parameters: frame_id, relative_timestamp and metadata.  The sequential relationship is expressed via the order of the list of Frame.  To express how much time has passed between the different frames, one can use the relative_timestamp parameter for each Frame. The relative timestamp is expressed in milliseconds and describes the relative time between the Frame and the start of the scene.  For example, let's say that the sensor data is collected and aggregated at 2Hz.  frame_1 = Frame(..., relative_timestamp=0) frame_2 = Frame(..., relative_timestamp=500) frame_3 = Frame(..., relative_timestamp=1000) frames = [frame_1, frame_2, frame_3]   The frame_id is a string that uniquely identifies each frame in the list of frames.  A common use case is to use uuids for each frame_id, or a combination of external_id and frame_index. For example, if the external_id of the scene is shanghai_20200101 then the frame_id could be encoded asshanghai_20200101:0 for the first frame, shanghai_20200101:1 for the second frame and so on.  It's also possible to provide metadata on a frame level for sequential frames. It consists of flat key-value pairs and is not exposed to annotators during the production of annotators.  As an example, let's say we want to create a scene of type CamerasSequence consisting of 2 frames, each with camera images from two sensors R and L.  from kognic.io.model.scene.resources import Image from kognic.io.model.scene.cameras_sequence import CamerasSequence, Frame frames = [ Frame( frame_id=&quot;1&quot;, relative_timestamp=0, images=[ Image(&quot;img_L_1.jpg&quot;, sensor_name='L'), Image(&quot;img_R_1.jpg&quot;, sensor_name='R') ]), Frame( frame_id=&quot;2&quot;, relative_timestamp=500, images=[ Image(&quot;img_L_2.jpg&quot;, sensor_name='L'), Image(&quot;img_R_2.jpg&quot;, sensor_name='R') ]) ] cameras_sequence = CamerasSequence(frames=frames, ...)   ","version":"Next","tagName":"h3"},{"title":"Image & Pointcloud Resources​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#image--pointcloud-resources","content":" Every file containing sensor data is represented as a Resource, withImage and PointCloud being the concrete subclasses.  class Resource(ABC, BaseSerializer): filename: str resource_id: Optional[str] = None sensor_name: str file_data: Optional[FileData] = Field(default=None, exclude=True)   Resources ultimately describe how to obtain some binary or textual sensor data, which can be done in different ways:  Indirectly: by refering to a local filename that contains the dataDirectly: provide some bytes-like object at creation timeLazily: provide a callback function which can provide the bytes later in the process  Resources must always be given a filename. For alternative 1 this must point to the local file to upload. For alternatives 2 &amp; 3 the value of filename parameter is treated as an identifier; it is used to name the uploaded file but does not have to correspond to anything in the filesystem.  Resources also always have a sensor_name which identifies the sensor they were captured from. In sequential scenes, each Frame will have a Resource for each sensor.  Resources take their actual data (bytes) from bytes, a BinaryIO or an IOBase-compatible object. These are referred to with the type alias UploadableData = Union[bytes, BinaryIO, IOBase].  For alternatives 2 &amp; 3 listed above, a FileData object is attached to the Image or PointCloud to capture the source of data. It is created with either data: UploadableData or a callback: Callable[[str], UploadableData], as well as a format which identifies the type of data contained in the bytes.  info Previous API client releases advertised support for ingesting files from external URIs, such as gs://bucket/path/file. Please contact Kognic if you believe you require this functionality going forward.  ","version":"Next","tagName":"h2"},{"title":"Local File​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#local-file","content":" Set filename to the path of the local file and do not provide data via other means (directly or callback). The content is uploaded using a content type inferred from the filename suffix.  Image(filename=&quot;/path/to/images/img_FC.png&quot;, sensor_name=&quot;FC&quot;)   ","version":"Next","tagName":"h3"},{"title":"Data in Memory​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-in-memory","content":" In addition to filename, provide a FileData object via the file_data attribute, which in turn has an UploadableData as its own data attribute. This example uses raw bytes:  Image( filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=FileData(data=b'some PNG bytes', format=FileData.Format.PNG) )   ","version":"Next","tagName":"h3"},{"title":"Data from Callback​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-from-callback","content":" In addition to filename, provide a FileData object via the file_data attribute, with a callback function that produces an UploadableData, e.g.  Image( filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=FileData(callback=get_png, format=FileData.Format.PNG) )   The callback function (get_png) is a unary function with the following signature.  def get_png(file: str) -&gt; UploadableData: pass   The callback function is invoked with the Resource.filename as its argument when it is time to upload that single file.  If the callback requires extra arguments then we recommend creating a closure over the additional arguments like this:  def get_callback(arg1, arg2, **kwargs): def callback(filename) -&gt; bytes: # ... use arg1, arg2, filename and kwargs return callback FileData( callback=get_callback(&quot;foo&quot;, &quot;bar&quot;, extra1=&quot;baz&quot;, extra2=&quot;qux&quot;), format=FileData.Format.JPG )   ","version":"Next","tagName":"h3"},{"title":"Data from Asynchronous Callback (new in version 1.5.0)​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-from-asynchronous-callback-new-in-version-150","content":" Using asynchronous callbacks can be useful to speed up data uploads, especially when the data is not available locally. In the same way as for synchronous callbacks, the callback function is invoked with the Resource.filename as its argument when it is time to upload that single file. Asynchronous callbacks can be used in the following way:  async def get_png(filename: str) -&gt; UploadableData: pass Image( filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=FileData(callback=get_png, format=FileData.Format.PNG) )   ","version":"Next","tagName":"h3"},{"title":"IMU Data​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#imu-data","content":" Inertial Measurement Unit (IMU) data may be provided for scenes containing LIDAR pointclouds. This can be used to perform motion compensation in multi-lidar setups, and by default if any IMU data is provided this will be done. Motion compensation may be disabled via a scene feature flag, for cases where motion compensation has already been performed prior to upload.  Refer to Motion Compensation for Multi-Lidar Setups.  ","version":"Next","tagName":"h2"},{"title":"Scene Feature Flags​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#scene-feature-flags","content":" Control over optional parts of the scene creation process is possible via FeatureFlags that are passed when invoking the create operation on the scene. Refer to the feature flags documentation for details. ","version":"Next","tagName":"h2"},{"title":"Shutter timings​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/scenes/lidars_and_cameras_seq#shutter-timings","content":" Shutter timings are optional metadata that may be provided when creating an Image within a Frame. Timings are two values: shutter start and end timestamp in nanoseconds since unix epoch and are specified for each image in each frame.  examples/lidars_and_cameras_seq_with_imu_and_shutter_times.py loading... View on GitHub ","version":"Next","tagName":"h2"},{"title":"Aggregated Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq","content":"Aggregated Lidars and Cameras Sequence note This feature is new in version 1.1.5 An AggregatedLidarsAndCamerasSeq scene consists of a sequence of camera images and lidar point clouds, where each frame consists on 1-9 camera images as well as 1-20 point clouds (in the case where you have pre-aggregated your point clouds, the first frame consists of 1-20 point clouds and all other frames 0 point clouds). What differentiates AggregatedLidarsAndCamerasSeqfrom LidarsAndCamerasSeq is that point clouds are aggregated over time during annotation which results in one big point cloud in the coordinate system of the first frame. Therefore, ego motion data is mandatory for this type of scene. For more documentation on what each field corresponds to in the AggregatedLidarsAndCamerasSeq object please check the section related to Scene Overview. Refer to Coordinate Systems for more information about what coordinate systems to use. examples/agg_lidars_and_cameras_seq.py loading... View on GitHub Use dryrun to validate scene Setting dryrun parameter to true in the method call, will validate the scene using the API but not create it. reuse calibration Note that you can, and should, reuse the same calibration for multiple s if possible.","keywords":"","version":"Next"},{"title":"Motion Compensation","type":0,"sectionRef":"#","url":"/docs/kognic-io/scenes/lidars_with_imu_data","content":"","keywords":"","version":"Next"},{"title":"Enable/disable motion compensation​","type":1,"pageTitle":"Motion Compensation","url":"/docs/kognic-io/scenes/lidars_with_imu_data#enabledisable-motion-compensation","content":" By default motion compensation is performed for scenes with LIDAR pointclouds when IMU data is provided.  Whether motion compensation is enabled or not is controlled by a feature flag. By default it is enabled but it can be disabled by providing an empty feature flag.  from kognic.io.model.scene.feature_flags import FeatureFlags client.lidars_and_cameras_sequence.create( ..., feature_flags=FeatureFlags() )   It may be desirable to disable motion compensation in cases where pointclouds are already motion compensated outside of the Kognic platform. ","version":"Next","tagName":"h2"},{"title":"Working with Scenes & Inputs","type":0,"sectionRef":"#","url":"/docs/kognic-io/working_with_scenes_and_inputs","content":"","keywords":"","version":"Next"},{"title":"Creating Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-scenes","content":" Each scene resource has a create method that can be used to create a scene of the corresponding type. It takes the corresponding scene model as input, so for example to create a Cameras scene you would do the following:  from kognic.io.model.scene.cameras import Cameras scene = Cameras(...) created_scene = client.cameras.create(cameras_scene) scene_uuid = created_scene.uuid   As you can see, the create method returns the associated scene_uuid, which can later be used to work with the scene. At this point all files have been uploaded to the Kognic Platform and the scene starts to be pre-processed. When pre-processing is finished, we say that the scene has been created. Refer to the Scene Status section for more information about the different scene statuses.  Note that it is often useful to use the dryrun parameter when experimenting. This will validate the scene format but not create it.  ","version":"Next","tagName":"h2"},{"title":"Scene Status​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#scene-status","content":" Once a scene has been uploaded, it might be preprocessed before being made available in the platform. During this process, the status property of a scene can be used to keep track of progress.  Status\tDescriptionpending\tScene has been validated but the server is waiting for the associated resources to be uploaded processing\tAssociated data has been uploaded and is currently being processed by the Kognic Platform, potentially performing conversion of file formats created\tScene is created and available in the platform failed\tConversion of scene failed. More information can be found in the associated error message invalidated:broken-input\tScene was invalidated since it did not load invalidated:duplicate\tScene was invalidated due to being uploaded several times invalidated:incorrectly-created\tScene was invalidated because it was incorrectly created  ","version":"Next","tagName":"h3"},{"title":"Creating Inputs from Scene​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-inputs-from-scene","content":" Once a scene has been created, it can be used to create inputs which is done by associating it with a projectand an input batch. Consider the following project setup:  organization # root for projects and scenes └── projects ├── project-a ├── batch-1 - completed ├── batch-2 - open ├── request-1 ├── input 9c08f7a3-3216-4bd6-a41a-1dda6f66f53e – using scene 0edb ├── input ddf548e3-9806-433c-afb5-fb951a721462 - using scene 37d9 └── ... └── request-2 └── batch-3 - pending └── project-b ├── batch-1 └── ... └── scenes ├── scene 0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3 ├── scene 37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36 └── ...   The create_from_scene method is used to create inputs from a scene. The method takes the scene uuid as input along with annotation information such as project, batch and annotation types. For example, to create inputs in project-aand batch-2, you would do the following:  client.cameras.create_from_scene( scene_uuid=&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, project=&quot;project-a&quot;, # Important: this is the external id and not the title batch=&quot;batch-2&quot; # Important: this is the external id and not the title )   The above code will create inputs for the scene in all requests in batch batch-2 for project project-a. If the batchparameter is omitted, the latest open batch for the project will be used. You can later reuse the same scene to create inputs for other projects and batches.  ","version":"Next","tagName":"h2"},{"title":"Creating Inputs Directly​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-inputs-directly","content":" It is often useful to create inputs directly instead of the 2-step process described above. To do this, you can simply pass the annotation information directly into the create method of the corresponding scene type. For example, to create an input in project-a and batch-2, you would do the following:  client.cameras_sequence.create( ..., project=&quot;project-a&quot;, # Important: this is the external id and not the title batch=&quot;batch-2&quot; # Important: this is the external id and not the title )   This would trigger the scene creation process, and once the scene is created, inputs are created in all requests in the given batch. If the batch parameter is omitted, the latest open batch for the project will be used. We also provide a wrapper function create_inputs to help with this process, see Creating Multiple Inputs With One Call.  ","version":"Next","tagName":"h2"},{"title":"List Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#list-scenes","content":" note This feature is new in version 1.6.0  It can be useful to list scenes that have been uploaded to the Kognic Platform. One example is to check the status during scene creation. Scenes can be retrieved in the following way:  scene_uuids = [&quot;cca60a67-cb68-4645-8bae-00c6e6415555&quot;, &quot;cc8776d0-f537-4094-8b11-8c2111741e2f&quot;] client.scene.get_scenes_by_uuids(scene_uuids=scene_uuids)   ","version":"Next","tagName":"h2"},{"title":"Response​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#response","content":" The response is a list of Scene objects containing the following properties  Property\tDescriptionuuid\tUUID used to identify the scene within the Kognic Platform external_id\tExternal ID supplied during scene creation scene_type\tType of scene (see Scene Types) status\tScene status (see Scene Status) created\tWhen the scene was created calibration_id\tCalibration used for the scene (if any) view_link\tA url to view the scene in the Kognic Platform error_message\tIf there is an error during scene creation the error message will be included, otherwise it's None  ","version":"Next","tagName":"h3"},{"title":"List Inputs​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#list-inputs","content":" note This feature is new in version 1.7.0  Inputs can be queried from the platform using the query_inputs method, which can be used in the following way  examples/query_inputs.py loading... View on GitHub  Additional filter parameters for querying inputs are listed below.  Parameter\tDescriptionproject\tProject identifier to filter by batch\tWhich batch in the project to return inputs for scene_uuids\tReturn inputs using scenes matching the supplied uuids external_ids\tReturn inputs using scenes matching the supplied external_ids  ","version":"Next","tagName":"h2"},{"title":"Response​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#response-1","content":" The response is a list of Input objects containing the following properties  Property\tDescriptionuuid\tID used to identify the input within the Kognic Platform scene_uuid\tID used to identify the scene that the input is using request_uid\tID used to identify the request that the input belongs to view_link\tA url to view the input in the Kognic Platform  ","version":"Next","tagName":"h3"},{"title":"Invalidate Scenes​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#invalidate-scenes","content":" note This feature is new in version 1.6.0  If issues are detected upstream related to scenes created, it is possible to invalidate them. This could be useful during development or if issues are detected with the data. Invalidating a scene means that it will be removed from requests, meaning that all inputs using the scene will be deleted. In turn invalidated scenes will not produce annotations and any completed annotations of the scene will be removed. There is no way to undo this operation so use with caution.  from kognic.io.model.scene.invalidated_reason import SceneInvalidatedReason scene_uuids = [&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, &quot;37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36&quot;] reason = SceneInvalidatedReason.BAD_CONTENT client.scene.invalidate_scenes(scene_uuids, reason)   The following reasons are available when invalidating scenes:  Reason\tDescriptionbad-content\tScene does not load, or has erroneous metadata such as invalid calibration duplicate\tIf the same scene has been created several times incorrectly-created\tIf the scene was unintentionally created.  ","version":"Next","tagName":"h2"},{"title":"Deleting Inputs​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#deleting-inputs","content":" note This feature is new in version 1.6.0  If issues are detected upstream related to inputs created, it is possible to delete them. This could be useful when the issues are related to the input itself and not the scene. One example would be if there are two inputs for a lidars and cameras scene, one where we want to annotate in 2D/3D and one where we only want to annotate in 2D. If the issue is an erroneous calibration the 2D input can still be used while the 2D/3D input should be deleted.  Deleting an input means that no annotations will be produced for it and any completed annotations of the input will be removed. There is no way to undo this operation so use with caution.  input_uuid = &quot;9c08f7a3-3216-4bd6-a41a-1dda6f66f53e&quot; client.input.delete_input(input_uuid)   ","version":"Next","tagName":"h2"},{"title":"Creating Multiple Inputs With One Call​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#creating-multiple-inputs-with-one-call","content":" note This feature is new in version 1.1.9  Since the input creation process is asynchronous, it is sometimes useful to wait for the inputs to be created before continuing. In order to do this, we provide a wrapper function create_inputs which can create multiple scenes and inputs, wait for them to be created (or failed) and yield the results. The function will block until it has a result to yield or all of the inputs have completed in one way or another. The function takes a list of SceneWithPreannotation(a new wrapper object containing a scene and optionally a pre-annotation) along with the normal input creation parameters.  from kognic.io.tools.input_creation import create_inputs, SceneWithPreAnnotation, InputCreationStatus from kognic.io.model.scene import LidarsAndCamerasSequence from kognic.openlabel.models import OpenLabelAnnotation scenes_with_pre_annotations: List[SceneWithPreAnnotation] = [ SceneWithPreAnnotation( scene=LidarsAndCamerasSequence(...), preannotation=OpenLabelAnnotation(...) # Optional ), ... ] for input_result in create_inputs(client, scenes_with_pre_annotations, &quot;project-identifier&quot;, batch=&quot;batch-identifier&quot;): # Do something with the result if input_result.status == InputCreationStatus.CREATED: print(f&quot;Input {input_result.external_id} was created, got uuid {input_result.input_uuid}&quot;) elif input_result.status == InputCreationStatus.FAILED: print(f&quot;Input {input_result.external_id} failed to be created at stage {input_result.error.stage} with error {input_result.error.message}&quot;) else: print(f&quot;Input {input_result.external_id} is in status {input_result.status}&quot;)   Note that the functions also accepts the parameters wait_timeout and sleep_time which can be used to control the wait-behavior. The wait_timeout parameter specifies the maximum time to wait for the inputs to be created/failed, whilesleep_time specifies the time to sleep between each check. Units are in seconds. The time it takes for inputs to be created depends on their size and the number of inputs to be created so the wait_timeout should be set accordingly. The default value is 30 minutes, starting from the time when all scene jobs have been committed.  ","version":"Next","tagName":"h2"},{"title":"Waiting for Scene Creation​","type":1,"pageTitle":"Working with Scenes & Inputs","url":"/docs/kognic-io/working_with_scenes_and_inputs#waiting-for-scene-creation","content":" It can sometimes be useful to wait for a scene to be created before continuing. This can be done by using below example in utils.py  examples/utils.py loading... View on GitHub ","version":"Next","tagName":"h2"},{"title":"kognic-openlabel","type":0,"sectionRef":"#","url":"/docs/openlabel/python-client","content":"kognic-openlabel Using this OpenLABEL json schema we have created a python package kognic-openlabelwhich makes it easier to work with annotations. The python models the OpenLABEL format as pydantic models and can be installed with pip install --upgrade kognic-openlabel Since pydantic is used, the model contains validation as well as methods for serialization and deserialization. Below are examples of how you can easily change between different formats openlabel_dict = { &quot;openlabel&quot;: { &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; } } } from kognic.openlabel.models import OpenLabelAnnotation # Deserialize dict openlabel_annotation = OpenLabelAnnotation.model_validate(openlabel_dict) # Serialize to json openlabel_json = openlabel_annotation.model_dump_json(exclude_none=True) # Deserialize json openlabel_annotation = OpenLabelAnnotation.model_validate_json(openlabel_json) # Serialize to dict openlabel_dict = openlabel_annotation.model_dump(exclude_none=True) ","keywords":"","version":"Next"},{"title":"View an uploaded scene","type":0,"sectionRef":"#","url":"/docs/upload-data/view-uploaded-scene","content":"","keywords":"","version":"Next"},{"title":"Find the view link​","type":1,"pageTitle":"View an uploaded scene","url":"/docs/upload-data/view-uploaded-scene#find-the-view-link","content":" Every scene has a view link that allows it to be viewed in the annotation tool, without any annotation capabilities. You can find the scene again either in the Kognic Platform or via the Python client:  PythonKognic Platform Use the get_scenes_by_uuids method to get the scene object, and then access its view_link attribute, which is populated once the scene is ready. scene = client.scene.get_scenes_by_uuids([scene_uuid])[0] print(f&quot;View the scene at {scene.view_link}&quot;)   ","version":"Next","tagName":"h2"},{"title":"Check calibration of 3D scenes​","type":1,"pageTitle":"View an uploaded scene","url":"/docs/upload-data/view-uploaded-scene#check-calibration-of-3d-scenes","content":" 3D scenes (those with LiDAR/RADAR) require a calibration, which can be checked for accuracy when viewing the scene. Start by opening the scene view link as described above.  2D (Camera) View3D (Pointcloud) View In the 2D view you can overlay the projected pointcloud onto the camera images. Click on the &quot;2D&quot; button at the top left of the scene viewOpen the &quot;View&quot; menu, then &quot;Pointcloud overlay&quot; and then &quot;Depth&quot;Compare the overlaid points with the features of the image.  Calibrations are immutable If you detect a calibration error this way, it is necessary to recreate the scene with a corrected calibration. It is therefore a good idea to check a few scenes before uploading a large dataset that all use the same calibration. ","version":"Next","tagName":"h2"},{"title":"Upload ZOD data","type":0,"sectionRef":"#","url":"/docs/upload-data/upload-zod-data","content":"Upload ZOD data This tutorial will guide you through uploading different scene types using the Zenseact Open Dataset (ZOD). The purpose of this page is to show you some of the steps that might be needed to convert recordings into Kognic scenes. Prerequisites &amp; Dependencies To follow along in this guide you need to download the data from Zenseact Open Dataset. The data should be structured like this: zod ├── sequences │ ├── 000000 │ ├── 000002 │ ├── ... └── trainval-sequences-mini.json You will also need to install the ZOD Python package from PyPI, which provides some abstractions for reading the data: pip install zod You need to have a Kognic account and the Kognic Python client installed. If you have not done this yet, read the quickstart guide. This guide follows the process of uploading scenes using ZOD data, using the example code fromthe Kognic IO ZOD examples repository which contains the complete source files for all of the snippets in this page. The examples are runnable, if you have the data available and have Kognic authentication set up. Cameras SequenceLidars and Cameras SequenceAggregated Lidars and Cameras Sequence Our example code initialises a Kognic IO Client at the top level, then creates the scene from ZOD data for (potentially) multiple scenes at once using a function. examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub The first step in creating the scene is to load and iterate ZOD sequences, picking as many as we are interested in. examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub Then we must convert the scene. Given we have the ZOD frames converted, it's very easy to create a single camera sequence. examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub But to convert the frames is more complex. We need to add all the sensors that we are interested in: in this case only the FRONT camera. We must also convert timestamps to different precision as we go. ZOD frame start timestamps are in fractional secondsKognic frame relative timestamps are in millisecondsWe use integer nanoseconds as an intermediate. examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub Converting the camera frame to an image is a simple mapping in this case, which we have abstracted out. Note that we do not know the shutter timingof the ZOD frames, but we set it to 1 ns in this example. This is not a problem in this case where there is no 3D data. examples/zod/conversion.py loading... View on GitHub Going back to the main create function, we move on to creating the scene: examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub Where we simply hand the scene (CamerasSequence) to Kognic IO to create for us. If it is not a dry run, we get back the UUID of the created scene (if it's a dry run, expect None). examples/zod/upload_cameras_sequence_scene.py loading... View on GitHub","keywords":"","version":"Next"},{"title":"Create and setup your project","type":0,"sectionRef":"#","url":"/docs/workflow/","content":"","keywords":"","version":"Next"},{"title":"Create project​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#create-project","content":" Projects are used to group and organize annotation work in the Kognic platform. Inside the project, the annotation work is further organized into annotation requests detailing what data to annotate and how. Below is a guide on how to create a new project.  Login to the app and navigate to the Project management page In the top right corner, press “Create new project” Create your project by giving it a name and external id, then press Create project. The external id is used to access the project through Kognics APIs. If you leave it blank, it will be set to a random UUID. Congratulations, you have now created your first project. The project will appear in the project list and you can press it to continue the setup  ","version":"Next","tagName":"h2"},{"title":"Create an annotation instruction​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#create-an-annotation-instruction","content":" An Annotation Instruction is a specification of what you want to be annotated, but also an explanation or guidance of how you want it to be annotated.  For example, you may specify that you would like to annotate cars and pedestrians in 3D with bounding boxes, which we call Classes. In addition, you may also want to clarify class-related annotation rules in a separate guideline document, for example, that the 3D box annotations around each car should not include the side mirrors.  To create a Annotation Instruction, first navigate to the Annotation Instruction tab inside the project. Click on your project in the project list and then click on Annotation Instruction in the top bar, then press Create New Instruction to start. In the popup window, give your instruction a name and create it. After you press Create Instruction you will be redirected back to the list of annotation instructions and your new instruction will be listed. Click on new instruction to open it and see all different revisions. Currently there are none. Create one by clicking Create Revision in the top right corner. In the new popup window, create a new revision by naming it. For Type of revision select New revision draft. Finish the process by clicking Create Revision, upon doing that you will be redirected into the revision.  Now its time to start building out the annotation instruction. There are 5 different steps in the process. Creating the classes ( What classes are needed and with which shape types they shall be annotated with ), creating Class properties and adding them to the relevant classes, adding optional Scene Properties to the inputs, configuring various settings and attaching your guideline.  As an example here we will create the class Car that shall be annotated as a 3D cuboid with 2 different properties.  Create a class by clicking Create new class in the bottom right corner Change the Geometry type to 3D Bounding box and name the class Car Add “Auto-adjust this Frame” and “Machine Assisted 3D tool” as Automation settings To create a class property, go the the class properties tab and press Create new class property Select the property type you need and name the property, in this example a True / False property is selected. Then configure the property, for this example. The property value is unique per sensor its visible in and it’s a required property without any pre-selected value. Now it’s time to connect the property to the class. Go back to the Classes tab and select the Car class Press Select properties to this class Select the property you created and press Save changes Go to configuration, here you can configure your task according to your specific need, to learn about each setting. Press the question mark to read about it. In our case, go to 3D settings and enable “Default to cuboid estimation” Go to the Guideline tab and upload your guideline as a PDF. Now a draft is finished, to test your revision and make sure it behaves as you want, press Preview in the Top-right corner to test it. Select “Multi-lidar” and press Preview, a new tab will open with a demo task with your annotation instruction To the right you will see your configured classes. Draw an object to see that each class has the properties you want. In our case we have the “Car” class with 1 True/False property named Occluded If you are happy with the behaviour, go back to the Annotation Instruction and press “Set as ready for production in the top right bar In the pop-up, you get the option to get a review from a Kognic expert. Disable that checkbox and click Confirm in this case. Well done! The annotation instruction is now ready to be used when creating a request which is the next step  More details about Annotation Instructions and some tips &amp; tricks are available here  ","version":"Next","tagName":"h2"},{"title":"Create the first request​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#create-the-first-request","content":" Annotation request is the core concept in the organization of annotation work at Kognic. The request's configuration specifies what data to annotate (Input batch), how to annotate the data (Annotation Instruction), which process the data should be produced by (Workflow), and who should annotate and quality-assure the data (Team).  To get started, open the project you created before and select the Annotation Requests tab. Then locate the Create request button at the top right corner of the project pag Go ahead and create the request. Here there is a variety of settings to go through and select. More details about the different sections can be found by clicking the question mark on the right hand side of each section. Add Request and Input batch info Name your request and external id. The external id is again used when working with the request via the API. Annotation types are used to categorize the annotations produced in different requests. Select the one that best matches the type of annotations you plan to produce in your request. Request Producer The organization with the request role Producer is responsible for producing annotated data in the request. Users from this organization are allowed more detailed monitoring and management options and can configure the request team in detail. All to ensure they can successfully monitor and manage the production process. In this Request, select your organization as the Request Producer Input type The input type decides what data type you can upload to your newly created input batch and later annotate in the request. For this request, select Cameras The available input types are: Input type\tDescriptionLidar and Cameras\tA single frame, containing both camera data (from one or multiple cameras) and lidar data. Lidar and Cameras Sequence\tA sequence of frames, containing both camera data (from one or multiple cameras) and lidar data. Cameras\tA single frame of camera data. The data can be from one or multiple cameras. Cameras Sequence\tA sequence of camera frames. The data can be from one or multiple cameras. Aggregated Lidar and Cameras Sequence\tA sequence of frames, containing both camera data (from one or multiple cameras) and lidar data. By using this model the lidar data will be aggregated using your IMU data. This can be useful for example when annotating 3D lines Annotation Instruction Select the Annotation Instruction and Revision that you created in the previous step as the Annotation Instruction Request Workflow The request workflow determines the steps used to produce annotations and in what order. They define what type of steps (tasks) to complete before we have produced a deliverable annotation. For this request, select Annotate + Full Review + Sampled Review. A comprehensive documentation on Workflows are available on this page Annotation Workflows - Kognic: Annotation + Dataset Manageme…. Review error type The error types are available in the Feedback Tools during Review tasks, and during follow-up Correction tasks. If your organization has many error types configured, you might want to select a subset of error types that are relevant for this request. You can learn more about error types on the page Error Types - Kognic: Annotation + Dataset Management for Se… Finish by clicking Create Request  ","version":"Next","tagName":"h2"},{"title":"Upload data​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#upload-data","content":" The request configuration is complete, well done! Now it’s time to upload data to the request.  Uploading data is primarily done via the Kognic APIs. To access our APIs, Kognic provides a Python client. For simpler inputs, you can also upload data through the user interface. We'll cover this process in the Data Orchestration block below.  Create your Kognic API credentials and install the client package Follow the quickstart guide. Make sure to verify that is works. Upload and create an Input When uploading data to Kognic, you have two options: you can either send the data directly to a specific request (like the one we created earlier), or just upload it as a standalone scene. Note that a scene will be created in both cases, but in the first case the scene will also be automatically sent to the request for annotation. Note that if you only create a standalone scene you can always add it to a request later on. A scene can be reused in many requests in case you want to annotate the same data in different ways. The request above was setup as a Cameras input type request. That means that you can upload a single camera image from one or more cameras in one frame. Follow the code example Uploading a 2D scene here to upload your first scene. All you need is any image you would like to annotate. In this case we both want to create the scene and send it to the request that we created. This can be done like the code snippet below, where you specify external IDs for both the project and request (also known as batch). If you don't remember them, don't worry—you can copy them from the app as shown below. You can copy the external ids from the app. The project id under the Project main page and press the three dots in the top right corner. To copy the Batch external id, go into the request and copy the id in the same fashion If you want to know more about Scenes and Inputs and how to work with them you can read more here  ","version":"Next","tagName":"h2"},{"title":"Data Orchestration​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#data-orchestration","content":" The Data Orchestration tool is a great way to visualise your uploaded data. Here you can see all of the scenes that you have created. You can filter on certain input types, as well as visualise the actual data in the drawing tool by using the dot-menu on the right as shown below.  Here you can also view in what request a specific scene is annotated in. And if you want to send the scene to a specific request for annotation you can use the Kognic API as explained here.    In the upper right corner of the page you can also upload a scene via the UI. This is currently limited to Cameras and Cameras Sequence inputs, but can be of great help to get started.  ","version":"Next","tagName":"h2"},{"title":"Add team & activate request​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#add-team--activate-request","content":" The last step to setup is to add the team that should work on the request  Two settings determine how a team member gets assigned tasks and what types of tasks they will be assigned - the responsibility and the task allocation type.  ","version":"Next","tagName":"h2"},{"title":"Task allocation type​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#task-allocation-type","content":" You can have manual or automatic task allocation  Automatic task allocation ensures that the team member will automatically get access to tasks of specific types when automatic task allocation has been activated for the request. The team member will be able to work on the tasks as long as there are tasks available of the type/types their responsibility allows them to work on.  Automatically allocated tasks are assigned when the team member clicks &quot;Start a task&quot; on thier “My Tasks” page  If a user have Manual task allocation a project manager must manually assign tasks to the user.  ","version":"Next","tagName":"h3"},{"title":"Responsibility​","type":1,"pageTitle":"Create and setup your project","url":"/docs/workflow/#responsibility","content":" The responsibility determines from which Workflow stage the team member will be assigned tasks during automatic task allocation**.** When manual task allocation is activated for a team member, you can assign them any type of task regardless of their responsibility.  What are workflow stages?  On the input's way toward getting a delivery-ready annotation, it goes through a series of workflow stages. Each stage corresponds to one action in one of the workflow's different phases, such as the action “Correct” in the phase “Review”.  Annotate🖋️The team member will be able to work on available annotation tasks, which are the base annotation task starting from an empty input  Correct🖍️The team member will be to work on available Review correction tasks during automatic task allocation. A Review correction task contains comments and correction requests from a previous Review tasks.  Review 👍👎The team member can work on available Review tasks during automatic task allocation. A Review task is made to quality assure a previous task.  Now lets add your team.  Navigate to the request and the team tab Press add your team members Add a team member Select the Task allocation type and responsibility you want, then search for their name or email to select them. Press Add Members to finish the setup Read more about a request team Team - Kognic: Annotation + Dataset Management for Sensor-Fu… Great job, lets finish and get to work by pressing Activate automatic allocation on the top right. Team members with Automatic task allocation will now have production tasks to work on under their My Tasks view. ","version":"Next","tagName":"h3"},{"title":"Upload your first scene","type":0,"sectionRef":"#","url":"/docs/upload-your-first-scene","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Upload your first scene","url":"/docs/upload-your-first-scene#prerequisites","content":" You have successfully followed the Quickstart guide and have the kognic-io library installed. For users with access to multiple workspaces you need to specify a workspace to upload data too.  ","version":"Next","tagName":"h2"},{"title":"Code examples​","type":1,"pageTitle":"Upload your first scene","url":"/docs/upload-your-first-scene#code-examples","content":" Uploading a 2D scene To upload a 2D scene, you need to have the raw images available on your local machine (or create a callback for remote data). It is a two-step process: Build the scene object in PythonUpload the scene object to the Kognic Platform Below follows examples for a few different cases. One ImageMultiple ImagesSequence from kognic.io.client import KognicIOClient from kognic.io.model.scene.cameras import Cameras, Frame from kognic.io.model.scene.resources import Image # 1. Build scene object scene = Cameras( external_id=&quot;my-first-scene&quot;, frame=Frame(images=[Image(filename=&quot;path/to/image.jpg&quot;)]) ) # 2. Upload scene client = KognicIOClient() scene_uuid = client.cameras.create(scene).scene_uuid print(&quot;Scene uploaded, got uuid:&quot;, scene_uuid)   Uploading a 2D/3D scene To upload a 2D/3D scene, you need to have the raw images and point clouds available on your local machine (or create a callback for remote data). In addition you need to have calibration data available. It is a three-step process: Create a calibrationBuild the scene object in Python, referencing the calibration from the previous stepUpload the scene object to the Kognic Platform Below follows examples for a few different cases. One ImageMultiple ImagesSequence from kognic.io.client import KognicIOClient from kognic.io.model.calibration import SensorCalibration, PinholeCalibration, LidarCalibration from kognic.io.model.scene.lidars_and_cameras import LidarsAndCameras, Frame from kognic.io.model.scene.resources import Image, PointCloud client = KognicIOClient() # 1. Create calibration (see calibration section for more details) sensor_calibration = SensorCalibration( external_id = &quot;my-first-calibration&quot;, calibration = { &quot;CAM&quot;: PinholeCalibration(...), &quot;lidar&quot;: LidarCalibration(...) } ) created_calibration = client.calibration.create_calibration(sensor_calibration) # 2. Build scene object scene = LidarsAndCameras( external_id=f&quot;my-first-scene&quot;, calibration_id = created_calibration.id, frame=Frame( images=[Image(sensor_name = &quot;CAM&quot;, filename=&quot;path/to/image.jpg&quot;)], point_clouds=[PointCloud(sensor_name = &quot;lidar&quot;, filename=&quot;path/to/pointcloud.pcd&quot;)] ) ) # 3. Upload scene scene_uuid = client.lidars_and_cameras.create(scene).scene_uuid print(&quot;Scene uploaded, got uuid:&quot;, scene_uuid) note Multiple point clouds is also supported, but not shown in the examples above since that requires a bit more data. See the Motion Compensation section for more details.  Uploading using ZOD Data We have exemplar code and a tutorial for uploading scenes using Zenseact Open Dataset (ZOD) data, including 2D, 3D, and aggregated 3D scenes. Check out the guide document and exemplar code here! If you have the ZOD data downloaded, and have Kognic API credentials, the examples will run out of the box to create functional scenes! ","version":"Next","tagName":"h2"},{"title":"Pre-annotations","type":0,"sectionRef":"#","url":"/docs/kognic-io/pre_annotations","content":"","keywords":"","version":"Next"},{"title":"Creating pre-annotations using the kognic-io client​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#creating-pre-annotations-using-the-kognic-io-client","content":" There are 3 steps that are needed in order to create pre-annotations in the Kognic platform.  Create a scene by uploading all the needed dataUpload an OpenLabel annotation as a pre-annotationCreate an input from the scene  Note that these steps can be performed in one call with the create_inputs function, see Creating Multiple Inputs With One Call  ","version":"Next","tagName":"h2"},{"title":"1. Creating a scene​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#1-creating-a-scene","content":" Start by creating a scene  # Create Scene but not input since we don't provide project or batch scene_response = client.lidars_and_cameras_sequence.create( lidars_and_cameras_seq, dryrun=dryrun )   Note that you now have to wait for the scene to be created before you can proceed to the next step. More information this can be found Waiting for Scene Creation.  ","version":"Next","tagName":"h3"},{"title":"2. Uploading an OpenLabel annotation​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#2-uploading-an-openlabel-annotation","content":" The pre-annotation can be uploaded to the Kognic platform once the scene has been created successfully.  Load your OpenLabel annotation according to the documentation in kognic-openlabel and upload it to the Kognic platform as such:  client.pre_annotation.create( scene_uuid=scene_response.scene_uuid, # from step 1 pre_annotation=OpenLabelAnnotation(...), dryrun=dryrun )   ","version":"Next","tagName":"h3"},{"title":"3. Create input​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3-create-input","content":" When the scene and pre-annotation have been successfully created, the input can be created. This will add it to the latest open batch in a project, or the specific batch that's specified, and be ready for annotation with the pre-annotation present.  client.lidars_and_cameras_sequence.create_from_scene( scene_uuid=scene_response.scene_uuid, # from step 1 project=project, # Important: this is the external id and not the title dryrun=dryrun )   ","version":"Next","tagName":"h3"},{"title":"OpenLabel support​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#openlabel-support","content":" Pre-annotations use the OpenLabel format/schema but not all OpenLabel features are supported in pre-annotations.  ","version":"Next","tagName":"h2"},{"title":"Unsupported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#unsupported-pre-annotation-features","content":" These features or combinations of features are not currently supported, or only have partial support.  Static geometries: not supported These are bounding boxes, cuboids, etc. declared in the OpenLabel under objects.*.objectData Geometry-specific attributes: not supported on 3D geometry These are attributes declared in the OpenLabel on a single geometric shape, in other words an attribute that only applies to the object as seen by one sensor; a common example is occlusion which is recorded separately for each camera.May also be referred to as source-, stream- or sensor-specific attributes.3D geometry is anything that can be drawn when annotating a pointcloud, e.g. cuboids.Geometry-specific attributes are permitted on 2D geometry e.g. bounding boxesNote that the task definition, must designate a property as source specific before it may be used in this way.The stream attribute is a special case and is excepted from this rule  ","version":"Next","tagName":"h2"},{"title":"Supported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#supported-pre-annotation-features","content":" ","version":"Next","tagName":"h2"},{"title":"Geometries​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#geometries","content":" note Objects cannot have multiple 3D geometries in the same frame  Name\tOpenLABEL field\tDescription\tAttributesCuboid\tcuboid\tCuboid in 3D\t- Bounding box\tbbox\tBounding box in 2D\t- 3D line\tpoly3d\tLine in 3D. Append the first point at the end if you want it to be closed.\t- Polygon\tpoly2d\tPolygon in 2D\tis_hole Multi-polygon\tpoly2d\tMulti-polygon in 2D\tis_hole &amp; polygon_id Curve\tpoly2d\tCurve or line in 2D\tinterpolation_method 2D point\tpoint2d\tPoint\t- Group of 2D points\tpoint2d\tGroup of points\tpoint_class  Note that all geometries should be specified under frames rather than in the root of the pre-annotation. 3D geometries should be expressed in the lidar coordinate system in the single-lidar case, but in the reference coordinate system in the multi-lidar case. The rotation of cuboids should be the same as that in exports. 2D geometries should be expressed in pixel coordinates. See coordinate systems for more information.  ","version":"Next","tagName":"h3"},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes","content":" TextNumBoolean  For 2D geometry, attributes may be specified as geometry specific (aka source/sensor specific), or object specific. Attributes can be static (specified in the objects key) or dynamic (specified in the object_data for the object in the frame) and must be allowed by the task definition, if one exists. Geometry specific attributes (those which appear on a single shape within frames) must also be declared as such in the task definition; arbitrary properties cannot be used in a source-specific way.  ","version":"Next","tagName":"h3"},{"title":"Contexts​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#contexts","content":" Currently not supported. Contact Kognic if you need support for this or use regular attributes instead.  ","version":"Next","tagName":"h3"},{"title":"Frames​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#frames","content":" Every pre-annotation must contain frames with unique timestamps that are among the ones specified in the scene. The reason for this is that the timestamps are used to map the frame in the pre-annotation to the correct frame in the scene. In the static case, one frame should be used with timestamp 0.  ","version":"Next","tagName":"h3"},{"title":"Relations​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#relations","content":" Currently not supported. Contact Kognic if you need support for this or use regular attributes instead.  ","version":"Next","tagName":"h3"},{"title":"Streams​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#streams","content":" Every geometry must have the stream property specified. This property determines which stream (or sensor) that the geometry appears in. It is important that the stream is among the ones specified in the scene and of the same type, for example camera or lidar.  ","version":"Next","tagName":"h3"},{"title":"Sparseness​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness","content":" Pre-annotations can be sparse, meaning that its objects or geometries do not need to be present in every frame. Instead, they can be present in a subset of frames and then interpolated in the frames in between. Utilizing this feature can speed up the annotation process significantly for sequences. Sparseness can be accomplished in two different ways, either by using object data pointers or the boolean property interpolated. The former is the recommended way of doing it in most cases since it will lead to a more compact pre-annotation. The latter is useful when the pre-annotation is created from exported annotations from the Kognic platform.  Interpolation is done by linearly interpolating the geometry values between key frames. This is done in pixel coordinates for 2D geometries. For 3D geometries, the interpolation can be done in either the frame local coordinate system or the world coordinate system (see Coordinate Systems). This is configured in the annotation instruction so reach out to the Kognic team about this if you are unsure. Note that interpolation in the world coordinate system is recommended but requires that the scene contains ego poses.  ","version":"Next","tagName":"h2"},{"title":"Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#object-data-pointers","content":" In OpenLABEL, object data pointers are used to create a specification for objects. For example, you can specify what attributes and geometries that are used for specific objects. In addition, you can specify for which frames that these are present. If a geometry is specified in the object data pointer, it will be present in all frames that the object data pointer is pointing to. If the geometry is not provided in some of these frames, it will be interpolated. Note that geometries mustbe provided for the first and last frame in the object data pointer. Otherwise, the pre-annotation will be rejected.  One limitation is that a geometry must be in the same stream for all frames when using object data pointers. This is because interpolation is done in the stream coordinate system. If you need to use geometries of the same type in different streams, you can simply use different names for the geometries in the different streams.  Sparseness with Object Data Pointers shows an example of how to use object data pointers.  ","version":"Next","tagName":"h3"},{"title":"Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#interpolated-property","content":" The boolean property interpolated can be used to specify that a geometry should be interpolated. Geometries are still required to be present in interpolated frames but their geometry values will be ignored. Note that interpolated geometries must have corresponding geometries (interpolated or not) in the first and last frame of the pre-annotation. Otherwise, the pre-annotation will be rejected.  Using the interpolated property is the recommended way of doing it when the pre-annotation is created from exported annotations from the Kognic platform.  Sparseness with Interpolated Property shows an example of how to use the interpolated property.  ","version":"Next","tagName":"h3"},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes-1","content":" Attributes are handled differently compared to geometries. If an attribute is not present in a frame, its last value will simply be used if the object (or geometry if the property is source-specific) is present in the frame. If the object is not present in the frame, the attribute will be ignored. Dense attributes will be sparsified automatically when the pre-annotation is uploaded to the Kognic platform.  ","version":"Next","tagName":"h3"},{"title":"Kognic Reserved Object Properties​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#kognic-reserved-object-properties","content":" There are certain properties that can be set on an object to toggle various behaviour in the Kognic platform.  ","version":"Next","tagName":"h2"},{"title":"Locked Geometries​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#locked-geometries","content":" If an object and its geometries in the pre-annotation is already of sufficient quality, or should remain unchanged during use of the pre-annotation, you can mark it as locked. The lock is put on an object level, and will affect all the objects geometries.  { &quot;openlabel&quot;: { &quot;objects&quot;: { &quot;object_uuid&quot;: { &quot;name&quot;: &quot;object_uuid&quot;, &quot;object_data&quot;: { &quot;boolean&quot;: [ { &quot;name&quot;: &quot;kognic_locked_geometries&quot;, &quot;val&quot;: true } ] }, &quot;object_data_pointers&quot;: {}, &quot;type&quot;: &quot;Vehicle&quot; } } } }   import kognic.openlabel.models as OLM uuid1 = str(uuid.uuid4()) object = OLM.Object( name=uuid1, type=&quot;car&quot;, object_data=OLM.ObjectData( boolean=[ OLM.Boolean(name=&quot;kognic_locked_geometries&quot;, val=True), ] ), ) openlabel = OLM.Openlabel(objects={uuid1: object}, metadata=OLM.Metadata(schema_version=&quot;1.0.0&quot;)) openlabel_annotation = OLM.OpenLabelAnnotation(openlabel=openlabel)   ","version":"Next","tagName":"h3"},{"title":"Stationary Objects​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#stationary-objects","content":" A stationary object is something that can move, but doesn't. A good example of this is a parked car. This is different from a static object, which can't move, such as a landmark.  Objects can be marked as stationary to enable certain platform features.  { &quot;openlabel&quot;: { &quot;objects&quot;: { &quot;object_uuid&quot;: { &quot;name&quot;: &quot;object_uuid&quot;, &quot;object_data&quot;: { &quot;boolean&quot;: [ { &quot;name&quot;: &quot;kognic_stationary_object&quot;, &quot;val&quot;: true } ] }, &quot;object_data_pointers&quot;: {}, &quot;type&quot;: &quot;Vehicle&quot; } } } }   import kognic.openlabel.models as OLM uuid1 = str(uuid.uuid4()) object = OLM.Object( name=uuid1, type=&quot;car&quot;, object_data=OLM.ObjectData( boolean=[ OLM.Boolean(name=&quot;kognic_stationary_object&quot;, val=True), ] ), ) openlabel = OLM.Openlabel(objects={uuid1: object}, metadata=OLM.Metadata(schema_version=&quot;1.0.0&quot;)) openlabel_annotation = OLM.OpenLabelAnnotation(openlabel=openlabel)   ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#examples","content":" Below follows examples of supported pre-annotations.  ","version":"Next","tagName":"h2"},{"title":"3D cuboid and 2D bounding box with a static property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-cuboid-and-2d-bounding-box-with-a-static-property","content":" { &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot;, &quot;streams&quot;: { &quot;LIDAR1&quot;: {}, &quot;ZFC&quot;: {} } }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;ZFC&quot; }] }, &quot;name&quot;: &quot;Bounding-box-1&quot;, &quot;val&quot;: [1.0, 1.0, 40.0, 30.0] } ], &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;LIDAR1&quot; }] }, &quot;name&quot;: &quot;cuboid-89ac8a2b&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;, &quot;object_data&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; }] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;LIDAR1&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }   ","version":"Next","tagName":"h3"},{"title":"3D line with a dynamic property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-line-with-a-dynamic-property","content":" { &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [{ &quot;frame_end&quot;: 0, &quot;frame_start&quot;: 0 }], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;streams&quot;: { &quot;lidar&quot;: {} }, &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; }] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;line-3d-1&quot;, &quot;val&quot;: [ -5.0, 0.0, 0.0, -5.0, 10.0, 0.0, 5.0, 10.0, 0.0, 5.0, 0.0, 0.0, -5.0, 0.0, 0.0 ] } ], &quot;text&quot;: [{ &quot;name&quot;: &quot;occluded&quot;, &quot;val&quot;: &quot;No&quot; }] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;name&quot;: &quot;cc06aced&quot;, &quot;type&quot;: &quot;Region&quot; } }, &quot;streams&quot;: { &quot;lidar&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }   ","version":"Next","tagName":"h3"},{"title":"Sparseness with Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-object-data-pointers","content":" In the example below the object 1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box called the-bbox-name that is provided in frames 0 and 3. In frames 1 and 2, the bounding box will be interpolated.  { &quot;openlabel&quot;: { &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;car-name&quot;, &quot;type&quot;: &quot;car&quot;, &quot;object_data_pointers&quot;: { &quot;the-bbox-name&quot;: { &quot;type&quot;:&quot;bbox&quot;, &quot;frame_intervals&quot;: [{&quot;frame_start&quot;: 0, &quot;frame_end&quot;: 3}] } } } }, &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } }, &quot;1&quot;: {}, &quot;2&quot;: {}, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } } } } }   ","version":"Next","tagName":"h3"},{"title":"Sparseness with Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-interpolated-property","content":" In the example below sparseness is determined using the interpolated property. The object1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box for which the interpolated property is set to true in frames 1 and 2 but not in frames 0 and 3. The geometry values in frames 1 and 2 are ignored and instead interpolated from the geometry values in frames 0 and 3.  { &quot;openlabel&quot;: { ..., &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } }, &quot;1&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;2&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } } } } }  ","version":"Next","tagName":"h3"},{"title":"OpenLABEL format","type":0,"sectionRef":"#","url":"/docs/openlabel/openlabel-format","content":"","keywords":"","version":"Next"},{"title":"Rotation of Cuboids​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#rotation-of-cuboids","content":" The rotation is such that the y-axis is facing forwards, with a rotation order of XYZ. This means that a cuboid with a heading (yaw) equal to 0 is aligned with the y-axis in the positive direction along the axis. This is somewhat different compared to theISO 8855standard, where the forward direction is along the x-axis. Conversion to ISO 8855 can then be done by applying a rotation around the z-axis and changing sx and sy in the following way  import math from typing import List from scipy.spatial.transform import Rotation def convert_to_iso8855(val: List[float]) -&gt; List[float]: &quot;&quot;&quot; Converts cuboid values to ISO 8855 &quot;&quot;&quot; [x, y, z, qx, qy, qz, qw, sx, sy, sz] = val rotation_1 = Rotation.from_quat([qx, qy, qz, qw]) rotation_2 = Rotation.from_rotvec([0, 0, math.pi / 2]) rot_object = rotation_1 * rotation_2 [qx, qy, qz, qw] = rot_object.as_quat() return [x, y, z, qx, qy, qz, qw, sy, sx, sz]   ","version":"Next","tagName":"h2"},{"title":"Non-sequences are sequences with one frame​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#non-sequences-are-sequences-with-one-frame","content":" Due to reasons of simplicity we have made the choice to treat non-sequences in the same way as sequences. This means that non-sequences are represented as a sequence with only one frame. Only data such as name and type are defined in the top level element keys. All other information is stored under frames, see example below  { &quot;objects&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot; } }, &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;0&quot;: {&quot;object_data&quot;: {...}} } } } }   ","version":"Next","tagName":"h2"},{"title":"Stream is just another text property​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#stream-is-just-another-text-property","content":" The stream property is used to indicate which stream/sensor/source that the geometry och property was annotated in. For example here is an object with a point that has been annotated in a stream with the name Camera. Note that all corresponding attributes for the geometry have also been annotated in the same stream.  { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;name&quot;: &quot;point-4d2d325f&quot;, &quot;val&quot;: [300.5300, 286.4396], &quot;attributes&quot;: { &quot;text&quot;: [ {&quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;Camera&quot;}, {&quot;name&quot;: &quot;Color&quot;, &quot;val&quot;: &quot;Black&quot;} ] } } ] } }   ","version":"Next","tagName":"h2"},{"title":"Relations​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations","content":" Regarding changes on 2022-04-08 Some changes were made regarding how to represent certain types of relations on 2022-04-08. Contact Kognic in case your annotations were produced before this date, but you wish to include these changes anyways.  We consider two types of relations; unidirectional relations between two objects and group relations. In addition to these, there is a need to represent false relations, i.e. relation properties that are not actually pointers to other objects but rather take values such as Inconclusive, Nothing or Unclear.  ","version":"Next","tagName":"h2"},{"title":"Relations are unidirectional​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations-are-unidirectional","content":" Relations are unidirectional, meaning that if an object, object1, has a relation to another object, object2, it does not mean that object2 has a relation to object1. Below follows an example where car-0 is following car-1 and it is unclear whether car-2 is following another car or not.  Deprecated since 2022-04-08 Representing false relations using the relation uid is deprecated and has moved to the use of actions (see the next section)  { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;car-1&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;car-2&quot;, &quot;type&quot;: &quot;Car&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;2&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;Unclear&quot;}] } } }   ","version":"Next","tagName":"h3"},{"title":"Actions are used to represent false relations (new since 2022-04-08)​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#actions-are-used-to-represent-false-relations-new-since-2022-04-08","content":" In the Kognic Platform, there is support for assigning values to relations that are not actually references to other objects. Examples are Inconclusive and Nothing. Actions are used to represent these in the following way, where the name of the action determines the value and the type determines the property name.  { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;Nothing&quot;, &quot;type&quot;: &quot;is_pulling_or_pushing&quot;} } }   ","version":"Next","tagName":"h3"},{"title":"Groups are represented as actions​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#groups-are-represented-as-actions","content":" Deprecated since 2022-04-08 The group concept has been deprecated in favor of single relations between objects. This means that annotations produced after 2022-04-08 will no longer contain the group concept  Group relations are relations where objects can be seen as belonging to a group. There is then a need for an abstract concept that describes the group. OpenLABEL suggests the use of actions for this in such a way that each object in the group has a relation of type isSubjectOfAction to this action. Below follows an example where two lane-0 and lane-1belong to the same road, while it is unclear whether lane-2 belongs to a road.  { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;lane-1&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;lane-2&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;2&quot;: { &quot;name&quot;: &quot;2&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;1&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;Road&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;Unclear&quot;, &quot;type&quot;: &quot;Road&quot;} } }   ","version":"Next","tagName":"h3"},{"title":"Stream specific relations​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#stream-specific-relations","content":" If a relation is stream specific, there will be a property stream_relations denoting which stream the list of relations belong to.  { // frames.0 // ... &quot;frame_properties&quot;: { &quot;streams&quot;: { &quot;CAMERA_FRONT&quot;: { &quot;description&quot;: null, &quot;stream_properties&quot;: { &quot;stream_relations&quot;: { &quot;1&quot;: {} } } } } }, &quot;relations&quot;: { &quot;0&quot;: {} } }   ","version":"Next","tagName":"h3"},{"title":"Representing polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-polygons","content":" Polygons are described by a list of Poly2d objects in OpenLABEL. One of these represents the exterior while the others represent potential holes and this is determined by the boolean property is_hole. Below follows an example of a polygon with one hole.  { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }   The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair).  ","version":"Next","tagName":"h2"},{"title":"Representing multi-polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-multi-polygons","content":" Multi-polygons are simply lists of polygons, so we describe these in a similar way with lists of Poly2d objects with the property is_hole. However, we also add one additional property polygon_id that determines which polygon a Poly2d object belongs to in the multi-polygon. Below follows an example of a multi-polygon with two polygons with one hole each.  { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } }, { &quot;name&quot;: &quot;poly3&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly4&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }   The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair).  ","version":"Next","tagName":"h2"},{"title":"Representing curves​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-curves","content":" caution The name of the interpolation method has changed from interpolation-method to interpolation_method. However, old annotations might still contain the old name  Curves are represented using the poly2d geometry and the interpolation method is specified as a text property in the following way.  { &quot;poly2d&quot;: [ { &quot;closed&quot;: false, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;name&quot;: &quot;curve-d633ca89&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;interpolation_method&quot;, &quot;val&quot;: &quot;natural-cubic-spline&quot; } ] } } ] }   The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). The property interpolation_method is mandatory and determines how the nodes should be associated to each other. The following values are supported:  natural-cubic-splinecatmull-rom-0.5polyline  ","version":"Next","tagName":"h2"},{"title":"Representing 3D lanes​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-3d-lanes","content":" A 3D lane is represented as two lines in 3D (poly3d), one to the right and the other to the left. The text propertylane_edge determines whether the line is to the right or to the left. The lines will always have closed set to false.  { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;left&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.2647494200238287, -51.51747573498745, -2.315540290283199, 1.0807419132566136, -48.91298533071834, -2.313640304199211, -0.0892715141237751, -34.705936676401016, -2.235569814758307, -0.4442893388935316, -29.60917111552865, -2.1894531147766174, -1.0952988968721313, -17.193981050037397, -2.1397902661132875 ] }, { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;right&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.5845765823868767, -51.49487958011918, -2.315540290283199, 1.4004322100638888, -48.888528958803036, -2.313640304199211, 0.23043085215069048, -34.68163859008775, -2.235569814758307, -0.12426061849402326, -29.589636067040036, -2.1894531147766174 ] } ] } }   ","version":"Next","tagName":"h2"},{"title":"Representing 2D points​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-2d-points","content":" A 2D point is represented as a single point2d. Each point2d has an optional point_class attribute. For single points this may be ommited, but if set it must be equal to the type of the object. This attribute is reserved for future use on other point-based geometries.  { &quot;openlabel&quot;: { ..., &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;a940239d-ff27-4480-8294-c482977a1b32&quot;: { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;point_class&quot;, &quot;val&quot;: &quot;APoint&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;stream1&quot; } ] }, ... } ] } }, &quot;e027e626-eb7a-4a8e-a9ae-083464e137d1&quot;: { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;stream1&quot; } ] }, .... } ] } } } } }, &quot;metadata&quot;: {...}, &quot;objects&quot;: { &quot;a940239d-ff27-4480-8294-c482977a1b32&quot;: { ... &quot;type&quot;: &quot;APoint&quot; }, &quot;e027e626-eb7a-4a8e-a9ae-083464e137d1&quot;: { ... &quot;type&quot;: &quot;AnotherPoint&quot; } }, ... } }   ","version":"Next","tagName":"h2"},{"title":"Representing groups of 2d points​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-groups-of-2d-points","content":" A group of points is used when multiple points refere to the same object. The attribute point_class is required for each of the points in the point group, and the point_class has to be different from the object type. The point_class value &quot;line_reference_point&quot; is reserved for future use cases.  ","version":"Next","tagName":"h2"},{"title":"Representing Geometry Collections​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-geometry-collections","content":" note Introduced in kognic_format_version 2.2  Related documentation for the task viewhttps://docs.kognic.com/class-groups  A collection of geometries as described in the link above will be represented as having a reserved relation as type geometry_collection.  { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;name&quot;: &quot;bbox-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;: { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;curve-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;: { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;curve-abc123&quot;, &quot;val&quot;: [ ... ] } ] } }, &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;: { &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane&quot;, &quot;val&quot;: &quot;right&quot; } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot;, &quot;kognic_format_version&quot;: &quot;2.2&quot;, &quot;uuid&quot;: &quot;63698712-b18e-426b-9ad5-1b178cc29838&quot; }, &quot;objects&quot;: { &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;: { &quot;name&quot;: &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_bbox&quot; }, &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;: { &quot;name&quot;: &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_line&quot; }, &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;: { &quot;name&quot;: &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_line&quot; }, &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;: { &quot;name&quot;: &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;, &quot;object_data&quot;: { ... }, &quot;type&quot;: &quot;some_collection&quot; } }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;rdf_objects&quot;: [ { &quot;uid&quot;: &quot;516b6045-87e8-40e4-a104-5eaa600e8e3a&quot;, &quot;type&quot;: &quot;object&quot; }, { &quot;uid&quot;: &quot;fe07e9cf-f42c-4b48-b4d8-bab75b7e9827&quot;, &quot;type&quot;: &quot;object&quot; }, { &quot;uid&quot;: &quot;329508b7-729c-4298-8141-f329dbc32ad0&quot;, &quot;type&quot;: &quot;object&quot; } ], &quot;rdf_subjects&quot;: [ { &quot;uid&quot;: &quot;4c321584-0e88-4578-b0f0-b5e8c974244b&quot;, &quot;type&quot;: &quot;object&quot; } ], &quot;type&quot;: &quot;geometry_collection&quot; } } } }  ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}