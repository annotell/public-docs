"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[477],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>f});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),u=a,f=m["".concat(l,".").concat(u)]||m[u]||d[u]||o;return n?i.createElement(f,r(r({ref:t},p),{},{components:n})):i.createElement(f,r({ref:t},p))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:a,r[1]=s;for(var c=2;c<o;c++)r[c]=n[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7877:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var i=n(7462),a=(n(7294),n(3905));const o={title:"Motion Compensation"},r=void 0,s={unversionedId:"kognic-io/scenes/lidars_with_imu_data",id:"kognic-io/scenes/lidars_with_imu_data",title:"Motion Compensation",description:"An inherent problem with labeling any lidar setup",source:"@site/docs/kognic-io/scenes/lidars_with_imu_data.md",sourceDirName:"kognic-io/scenes",slug:"/kognic-io/scenes/lidars_with_imu_data",permalink:"/docs/kognic-io/scenes/lidars_with_imu_data",draft:!1,editUrl:"https://github.com/annotell/public-docs/docs-src/docs/kognic-io/scenes/lidars_with_imu_data.md",tags:[],version:"current",frontMatter:{title:"Motion Compensation"},sidebar:"docs",previous:{title:"Aggregated Lidars and Cameras Sequence",permalink:"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq"},next:{title:"Scene Feature Flags",permalink:"/docs/kognic-io/feature_flags"}},l={},c=[{value:"Enable/disable motion compensation",id:"enabledisable-motion-compensation",level:2}],p={toc:c};function m(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,i.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"An inherent problem with labeling any lidar setup\nis that the resulting point\ncloud is not a snapshot from a single instant in time but a time interval\nin which the lidar sweep was made. This causes a problem during labeling since\nthe objects can move during the lidar sweep, and if you try to label a car with\ne.g. a 3D box that box would not represent the actual size of that car. This issue can be\nmitigated with the help of motion compensation, where we synchronize the timestamp of all\npoints in the point cloud."),(0,a.kt)("p",null,"By including data from the Inertial Measurement Unit (IMU) of the ego vehicle we get an\nexact trajectory of how the car is moving during the lidar sweeps. This allows us to perform\nmotion compensation, adjusting the points in the point cloud so that they represent the same\ninstant in time."),(0,a.kt)("p",null,"Additionally, each point in the provided point clouds need to have a unix timestamp specified\n(in nanoseconds), so that the motion compensation can work."),(0,a.kt)("p",null,"What instant in time to motion-compensate the points to can be specified\nwith the ",(0,a.kt)("inlineCode",{parentName:"p"},"unix_timestamp")," parameter. If this is not specified then, for each\nframe, the median time of all points in the frame will be used instead."),(0,a.kt)("p",null,"Motion compensation is of particular importance when annotation is to be performed on multiple lidar\nsweeps at once, e.g. in multi-lidar setups and when point clouds are aggregated across frames."),(0,a.kt)("admonition",{title:"All Unix Timestamps need to be in nanoseconds",type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"In order for the motion compensation to work correctly it is important with a consistent\nunit of time. Therefore, all unix timestamps needs to be provided in nanoseconds.")),(0,a.kt)("p",null,"Note that all timestamps (in point clouds and the provided ",(0,a.kt)("inlineCode",{parentName:"p"},"unix_timestamp"),") must be encompassed by\nthe timestamps in the imu data. Otherwise, the scene creation will fail."),(0,a.kt)("p",null,"IMU data is provided as a list of ",(0,a.kt)("inlineCode",{parentName:"p"},"IMUData")," objects in the root of the object in the following way:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\nfrom kognic.io.model.ego import IMUData\nfrom kognic.io.model.calibration import Position, RotationQuaternion\nfrom kognic.io.model.scene.lidars_and_cameras_sequence import LidarsAndCamerasSequence, Frame\n\nfrom kognic.io.client import KognicIOClient\n\nimu_data = [\n    IMUData(\n        position=Position(x=-10.44, y=126.06, z=78.817),\n        rotation_quaternion=RotationQuaternion(x=-1.0, y=0.5, z=1, w=0),\n        timestamp=1665997200597027072 # ns\n    ),\n    ...\n]\n\nframes = [\n    Frame(..., unix_timestamp = 1665997358832901120),\n    Frame(..., unix_timestamp = 1665997503951270144),\n]\n\n\nlidars_and_cam_seq = LidarsAndCamerasSequence(\n    ...,\n    imu_data = imu_data,\n    frames = frames,\n)\n\n\nclient = KognicIOClient()\nclient.lidars_and_cameras_sequence.create(\n    lidars_and_cam_seq,\n    project="project-ext-id",\n    dryrun=True,\n)\n')),(0,a.kt)("admonition",{title:"Use dryrun to validate",type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"Setting ",(0,a.kt)("inlineCode",{parentName:"p"},"dryrun")," parameter to true in the method call, will validate the scene using the API but not create it.")),(0,a.kt)("h2",{id:"enabledisable-motion-compensation"},"Enable/disable motion compensation"),(0,a.kt)("p",null,"By default motion compensation is performed for scenes with LIDAR pointclouds when IMU data is provided."),(0,a.kt)("p",null,"Whether motion compensation is enabled or not is controlled by a ",(0,a.kt)("a",{parentName:"p",href:"../feature_flags"},"feature flag"),". By default it is enabled but it can be disabled by providing an empty feature flag."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from kognic.io.model.scene.feature_flags import FeatureFlags\n\nclient.lidars_and_cameras_sequence.create(\n    ...,\n    feature_flags=FeatureFlags()\n)\n")),(0,a.kt)("p",null,"It may be desirable to disable motion compensation in cases where pointclouds are already motion compensated outside of the Kognic platform."))}m.isMDXComponent=!0}}]);