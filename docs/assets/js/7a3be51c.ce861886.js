"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[670],{2375:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"kognic-io/scenes/lidars_and_cameras_seq","title":"Lidars and Cameras Sequence","description":"A LidarsAndCamerasSeq  consists of a sequence of camera images and lidar point clouds, where each frame consists","source":"@site/docs/kognic-io/scenes/lidars_and_cameras_seq.md","sourceDirName":"kognic-io/scenes","slug":"/kognic-io/scenes/lidars_and_cameras_seq","permalink":"/docs/kognic-io/scenes/lidars_and_cameras_seq","draft":false,"unlisted":false,"editUrl":"https://github.com/annotell/public-docs/edit/master/docs-src/docs/kognic-io/scenes/lidars_and_cameras_seq.md","tags":[],"version":"current","frontMatter":{"title":"Lidars and Cameras Sequence"},"sidebar":"docs","previous":{"title":"Cameras Sequence","permalink":"/docs/kognic-io/scenes/cameras_seq"},"next":{"title":"Aggregated Lidars and Cameras Sequence","permalink":"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq"}}');var s=i(4848),o=i(8453);const a={title:"Lidars and Cameras Sequence"},r=void 0,c={},d=[{value:"Providing Ego Vehicle Motion Information",id:"providing-ego-vehicle-motion-information",level:2},{value:"Shutter timings",id:"shutter-timings",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.code,{children:"LidarsAndCamerasSeq"}),"  consists of a sequence of camera images and lidar point clouds, where each frame consists\non 1-9 camera images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the\n",(0,s.jsx)(n.code,{children:"LidarsAndCamerasSeq"})," object please check the section related to ",(0,s.jsx)(n.a,{href:"/docs/kognic-io/overview",children:"Scene Overview"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"reference",children:"https://github.com/annotell/kognic-io-examples-python/blob/master/examples/lidars_and_cameras_seq.py\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Use dryrun to validate",type:"tip",children:(0,s.jsxs)(n.p,{children:["Setting ",(0,s.jsx)(n.code,{children:"dryrun"})," parameter to true in the method call, will validate the scene using the API but not create it."]})}),"\n",(0,s.jsx)(n.admonition,{title:"reuse calibration",type:"tip",children:(0,s.jsx)(n.p,{children:"Note that you can, and should, reuse the same calibration for multiple s if possible."})}),"\n",(0,s.jsx)(n.h2,{id:"providing-ego-vehicle-motion-information",children:"Providing Ego Vehicle Motion Information"}),"\n",(0,s.jsxs)(n.p,{children:["Ego vehicle motion (i.e. the position and rotation of the ego vehicle) is optional information that can be provided when\ncreating ",(0,s.jsx)(n.code,{children:"LidarsAndCamerasSeq"}),"s. This information can enable a massive reduction in the time it takes to annotate\nstatic objects. Ego vehicle motion information is provided by passing a ",(0,s.jsx)(n.code,{children:"EgoVehicleMotion"})," object to ",(0,s.jsx)(n.strong,{children:"each"})," ",(0,s.jsx)(n.code,{children:"Frame"}),"\nin the scene."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"reference",children:"https://github.com/annotell/kognic-io-examples-python/blob/master/examples/lidars_and_cameras_seq_full.py\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Coordinate Systems",type:"note",children:(0,s.jsxs)(n.p,{children:["Note that both ",(0,s.jsx)(n.code,{children:"position"})," and ",(0,s.jsx)(n.code,{children:"rotation"})," for ego vehicle pose are with respect to the ",(0,s.jsx)(n.em,{children:"local"})," coordinate system."]})}),"\n",(0,s.jsx)(n.h2,{id:"shutter-timings",children:"Shutter timings"}),"\n",(0,s.jsxs)(n.p,{children:["Shutter timings are optional metadata that may be provided when creating an ",(0,s.jsx)(n.code,{children:"Image"})," within a ",(0,s.jsx)(n.code,{children:"Frame"}),". Timings are two\nvalues: shutter start and end timestamp in nanoseconds since unix epoch and are specified for each image in each frame."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"reference",children:"https://github.com/annotell/kognic-io-examples-python/blob/master/examples/lidars_and_cameras_seq_with_imu_and_shutter_times.py\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);