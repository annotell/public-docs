"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[3488],{3703:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>j,contentTitle:()=>v,default:()=>_,frontMatter:()=>x,metadata:()=>b,toc:()=>y});var t=a(4848),s=a(8453),o=a(1470),r=a(9365),i=a(7041),c=a.n(i);const l=e=>`https://github.com/annotell/kognic-io-examples-python/blob/30c725ad38a1e5a163c28f10163022d4d522acc8/${e}`,d=(e,n)=>{const a=n??"";return(0,t.jsx)(c(),{className:"language-python",children:l(e),metastring:a})};function u(e){const n={a:"a",code:"code",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Our example code initialises a Kognic IO Client at the top level, then creates the scene from ZOD\ndata for (potentially) multiple scenes at once using a function."}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L72-L83","{4}"),"\n",(0,t.jsx)(n.p,{children:"The first step in creating the scene is to load and iterate ZOD sequences, picking as many as we are interested in."}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L28-L35","{6}"),"\n",(0,t.jsx)(n.p,{children:"Then we must convert the scene. Given we have the ZOD frames converted, it's very easy to create a single camera sequence."}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L44-L46","{2}"),"\n",(0,t.jsxs)(n.p,{children:["But to convert the frames is more complex. We need to add all the sensors that we are interested in: in this case only the ",(0,t.jsx)(n.code,{children:"FRONT"})," camera.\nWe must also convert timestamps to different precision as we go."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ZOD frame start timestamps are in fractional seconds"}),"\n",(0,t.jsx)(n.li,{children:"Kognic frame relative timestamps are in milliseconds"}),"\n",(0,t.jsx)(n.li,{children:"We use integer nanoseconds as an intermediate."}),"\n"]}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L49-L69","{18}"),"\n",(0,t.jsxs)(n.p,{children:["Converting the camera frame to an image is a simple mapping in this case, which we have abstracted out. Note that we\ndo not know the ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/scenes/lidars_and_cameras_seq#shutter-timings",children:"shutter timing"}),"\nof the ZOD frames, but we set it to 1 ns in this example. This is not a problem in this case where there is no 3D data."]}),"\n",d("examples/zod/conversion.py#L93-L102"),"\n",(0,t.jsx)(n.p,{children:"Going back to the main create function, we move on to creating the scene:"}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L33-L35","{2}"),"\n",(0,t.jsxs)(n.p,{children:["Where we simply hand the scene (",(0,t.jsx)(n.code,{children:"CamerasSequence"}),") to Kognic IO to create for us. If it is not a dry run, we get\nback the UUID of the created scene (if it's a dry run, expect ",(0,t.jsx)(n.code,{children:"None"}),")."]}),"\n",d("examples/zod/upload_cameras_sequence_scene.py#L38-L41")]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}function p(e){const n={a:"a",code:"code",em:"em",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This example follows the same broad structure as the cameras-only sequence with the addition of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A LiDAR sensor"}),"\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/calibrations/overview",children:"calibration"}),", to allow projection between 2D and 3D ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/coordinate_systems#single-lidar-case",children:"coordinate systems"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Conversion of point clouds from ZOD's packed NumPy arrays."}),"\n",(0,t.jsx)(n.li,{children:"Conversion of ego poses for each frame."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"As before we initialise a Kognic IO Client at the top-level, then create the scene from ZOD data for (potentially)\nmultiple scenes at once using a function."}),"\n",d("examples/zod/upload_lcs_scene.py#L93-L130","{4}"),"\n",(0,t.jsx)(n.p,{children:"When using both camera and LiDAR sensors we need a calibrations to relate them to each other in space.\nThe process for converting and creating the scene thus gains a step: calibration conversion."}),"\n",d("examples/zod/upload_lcs_scene.py#L35-L43","{6}"),"\n",(0,t.jsxs)(n.p,{children:["ZOD provides calibrations that we need to map to Kognic's format. We've provided some utility functions in the examples\nrepository that do this work for the sensors used in this example: the ",(0,t.jsx)(n.code,{children:"FRONT"})," camera and the ",(0,t.jsx)(n.code,{children:"VELODYNE"})," lidar."]}),"\n",d("examples/zod/conversion.py#L78-L87"),"\n",(0,t.jsx)(n.p,{children:"Take a look in the example code for the full LiDAR and camera calibration conversion details; suffice to say we must\nunpack the intrinsics & extrinsics from ZOD format and plug them in to Kognic format."}),"\n",(0,t.jsxs)(n.p,{children:["Next we need to create the frames. As with camera-only sequences, each frame consists of a collection of images for each\ncamera sensor, but now also a point cloud per LiDAR. We also specify the ego vehicle pose for each frame, telling us how\nthe vehicle has moved through the world. This is optional but valuable, as it simplifies annotation of static objects\nwhich do not move in world space even though they do move in the ",(0,t.jsxs)(n.a,{href:"/docs/kognic-io/coordinate_systems#the-reference-coordinate-system-and-calibrations",children:[(0,t.jsx)(n.em,{children:"reference"})," coordinate system"]}),",\nby allowing very accurate interpolation across frames."]}),"\n",d("examples/zod/upload_lcs_scene.py#L59-L90","{16,19,20}"),"\n",(0,t.jsxs)(n.p,{children:["We convert the pointclouds from ZOD's packed NumPy arrays (",(0,t.jsx)(n.code,{children:".npy"}),") to one of the formats supported by Kognic IO.\nRefer to the linked ",(0,t.jsx)(n.code,{children:"conversion.py"})," for details of exactly how the data is read and reformatted."]}),"\n",d("examples/zod/conversion.py#L105-L111"),"\n",(0,t.jsx)(n.p,{children:"In the Kognic platform, single LiDAR scenes should have their ego motion data expressed in the LiDAR coordinate system.\nFor multi-lidar scenes we expect the reference coordinate system. Since ZOD uses the reference coordinate system (which\nmoves with the vehidle), we convert to the LiDAR's coordinate system by applying the calibration transform:"}),"\n",d("examples/zod/upload_lcs_scene.py#L77-L78"),"\n",(0,t.jsx)(n.p,{children:"Once we have created the frames, the main conversion function proceeds as it did in the cameras example: build the scene\nobject and post it to Kognic."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}function f(e){const n={a:"a",code:"code",em:"em",li:"li",ol:"ol",p:"p",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This example follows the same structure as the LiDAR-and-camera sequence example."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq",children:"Aggregated scenes"})," are a special case of LiDAR + camera sequence\nscenes where the LiDAR data is aggregated across frames into a single pointcloud. This gives a dense, static pointcloud\nthat represents the entire scene across all frames."]}),"\n",(0,t.jsx)(n.p,{children:"Aggregated scenes may be created by providing a pointcloud on every frame and allowing the Kognic platform to handle\naggregation, or, they may be pre-aggregated and uploaded by specifying a pointcloud on the first frame, then nothing on\nsubsequent frames."}),"\n",(0,t.jsx)(n.p,{children:"In the case of ZOD data, we only have per-frame pointclouds, so the example uploads a pointcloud on every frame and\nleaves aggregation to the platform. As such it is very similar to the LiDAR-and-camera sequence example, except that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The scene type is different: ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSequence"})," instead of ",(0,t.jsx)(n.code,{children:"LidarsAndCamerasSequence"}),"."]}),"\n"]}),"\n",d("examples/zod/upload_alcs_scene.py#L56-L58","{3}"),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"Frame"}),"s are of an aggregated-scene specific type"]}),"\n"]}),"\n",d("examples/zod/upload_alcs_scene.py#L82-L91","{2}"),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Ego pose data is required"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Otherwise the two approaches are very similar - refer to the ",(0,t.jsx)(n.em,{children:"Lidars and Cameras Sequence"})," tab."]})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(f,{...e})}):f(e)}const x={id:"upload-zod-data",title:"Upload ZOD data"},v=void 0,b={id:"upload-data/upload-zod-data",title:"Upload ZOD data",description:"This tutorial will guide you through uploading different scene types using the Zenseact Open Dataset (ZOD).",source:"@site/docs/upload-data/upload-zod-data.mdx",sourceDirName:"upload-data",slug:"/upload-data/upload-zod-data",permalink:"/docs/upload-data/upload-zod-data",draft:!1,unlisted:!1,editUrl:"https://github.com/annotell/public-docs/edit/master/docs-src/docs/upload-data/upload-zod-data.mdx",tags:[],version:"current",frontMatter:{id:"upload-zod-data",title:"Upload ZOD data"},sidebar:"docs",previous:{title:"Aggregated Lidars and Cameras Sequence",permalink:"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq"},next:{title:"Overview",permalink:"/docs/kognic-io/overview"}},j={},y=[];function w(e){const n={a:"a",code:"code",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components},{Details:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["This tutorial will guide you through uploading different ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/overview#different-types-of-scenes",children:"scene types"})," using the ",(0,t.jsx)(n.a,{href:"https://zod.zenseact.com/",children:"Zenseact Open Dataset (ZOD)"}),".\nThe purpose of this page is to show you some of the steps that might be needed to convert recordings into Kognic scenes."]}),"\n",(0,t.jsxs)(a,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("h2",{children:"Prerequisites & Dependencies"})}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["To follow along in this guide you need to download the data from ",(0,t.jsx)(n.a,{href:"https://zod.zenseact.com/download/",children:"Zenseact Open Dataset"}),". The data should be structured like this:"]}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"zod\n\u251c\u2500\u2500 sequences\n\u2502   \u251c\u2500\u2500 000000\n\u2502   \u251c\u2500\u2500 000002\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 trainval-sequences-mini.json\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"You will also need to install the ZOD Python package from PyPI, which provides some abstractions for reading the data:"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip install zod\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["You need to have a Kognic account and the Kognic Python client installed. If you have not done this yet, read the ",(0,t.jsx)(n.a,{href:"/docs/getting-started/quickstart",children:"quickstart guide"}),"."]}),"\n"]})]}),"\n",(0,t.jsxs)(n.p,{children:["This guide follows the process of uploading scenes using ZOD data, using the example code from\n",(0,t.jsx)("a",{href:l("examples/zod"),children:"the Kognic IO ZOD examples repository"})," which contains the complete source files for all\nof the snippets in this page. The examples are runnable, if you have the data available and have Kognic authentication\nset up."]}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"cameras-sequence",label:"Cameras Sequence",children:(0,t.jsx)(h,{})}),(0,t.jsx)(r.A,{value:"lidars-and-cameras-sequence",label:"Lidars and Cameras Sequence",children:(0,t.jsx)(m,{})}),(0,t.jsx)(r.A,{value:"aggregated-lidars-and-cameras-sequence",label:"Aggregated Lidars and Cameras Sequence",children:(0,t.jsx)(g,{})})]})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(w,{...e})}):w(e)}},9365:(e,n,a)=>{a.d(n,{A:()=>r});a(6540);var t=a(8215);const s={tabItem:"tabItem_Ymn6"};var o=a(4848);function r(e){let{children:n,hidden:a,className:r}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,t.A)(s.tabItem,r),hidden:a,children:n})}},1470:(e,n,a)=>{a.d(n,{A:()=>w});var t=a(6540),s=a(8215),o=a(3104),r=a(6347),i=a(205),c=a(7485),l=a(1682),d=a(9466);function u(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:a}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:s}}=e;return{value:n,label:a,attributes:t,default:s}}))}(a);return function(e){const n=(0,l.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function p(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:a}=e;const s=(0,r.W6)(),o=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,c.aZ)(o),(0,t.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(s.location.search);n.set(o,e),s.replace({...s.location,search:n.toString()})}),[o,s])]}function f(e){const{defaultValue:n,queryString:a=!1,groupId:s}=e,o=h(e),[r,c]=(0,t.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o}))),[l,u]=m({queryString:a,groupId:s}),[f,g]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,o]=(0,d.Dv)(a);return[s,(0,t.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:s}),x=(()=>{const e=l??f;return p({value:e,tabValues:o})?e:null})();(0,i.A)((()=>{x&&c(x)}),[x]);return{selectedValue:r,selectValue:(0,t.useCallback)((e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);c(e),u(e),g(e)}),[u,g,o]),tabValues:o}}var g=a(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=a(4848);function b(e){let{className:n,block:a,selectedValue:t,selectValue:r,tabValues:i}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),d=e=>{const n=e.currentTarget,a=c.indexOf(n),s=i[a].value;s!==t&&(l(n),r(s))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=c.indexOf(e.currentTarget)+1;n=c[a]??c[0];break}case"ArrowLeft":{const a=c.indexOf(e.currentTarget)-1;n=c[a]??c[c.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":a},n),children:i.map((e=>{let{value:n,label:a,attributes:o}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:e=>c.push(e),onKeyDown:u,onClick:d,...o,className:(0,s.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":t===n}),children:a??n},n)}))})}function j(e){let{lazy:n,children:a,selectedValue:s}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===s));return e?(0,t.cloneElement)(e,{className:"margin-top--md"}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function y(e){const n=f(e);return(0,v.jsxs)("div",{className:(0,s.A)("tabs-container",x.tabList),children:[(0,v.jsx)(b,{...n,...e}),(0,v.jsx)(j,{...n,...e})]})}function w(e){const n=(0,g.A)();return(0,v.jsx)(y,{...e,children:u(e.children)},String(n))}}}]);