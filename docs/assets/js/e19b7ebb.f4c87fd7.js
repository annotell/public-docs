"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[8208],{4949:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>g,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var t=s(4848),o=s(8453);const a={title:"Aggregated Lidars and Cameras Sequence"},i=void 0,r={id:"kognic-io/scenes/aggregated_lidars_and_cameras_seq",title:"Aggregated Lidars and Cameras Sequence",description:"This feature is new in version 1.1.5",source:"@site/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq.md",sourceDirName:"kognic-io/scenes",slug:"/kognic-io/scenes/aggregated_lidars_and_cameras_seq",permalink:"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq",draft:!1,unlisted:!1,editUrl:"https://github.com/annotell/public-docs/edit/master/docs-src/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq.md",tags:[],version:"current",frontMatter:{title:"Aggregated Lidars and Cameras Sequence"},sidebar:"docs",previous:{title:"Lidars and Cameras Sequence",permalink:"/docs/kognic-io/scenes/lidars_and_cameras_seq"},next:{title:"Motion Compensation",permalink:"/docs/kognic-io/scenes/lidars_with_imu_data"}},c={},d=[];function l(e){const n={a:"a",admonition:"admonition",code:"code",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"This feature is new in version 1.1.5"})}),"\n",(0,t.jsxs)(n.p,{children:["An ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"})," scene consists of a sequence of camera images and lidar point clouds, where each\nframe consists on 1-9 camera images as well as 1-20 point clouds (in the case where you have pre-aggregated your point clouds, the first frame consists of 1-20 point clouds and all other frames 0 point clouds). What differentiates ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"}),"\nfrom ",(0,t.jsx)(n.code,{children:"LidarsAndCamerasSeq"})," is that point clouds are aggregated over time during annotation which results in one big\npoint cloud in the coordinate system of the first frame. Therefore, ego motion data is ",(0,t.jsx)(n.strong,{children:"mandatory"})," for this type of\nscene. For more documentation on what each field corresponds to in the ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"})," object please\ncheck the section related to ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/overview",children:"Scene Overview"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Refer to ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/coordinate_systems",children:"Coordinate Systems"})," for more information about what coordinate systems to use."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"reference",children:"https://github.com/annotell/kognic-io-examples-python/blob/master/examples/agg_lidars_and_cameras_seq.py\n"})}),"\n",(0,t.jsx)(n.admonition,{title:"Use dryrun to validate scene",type:"tip",children:(0,t.jsxs)(n.p,{children:["Setting ",(0,t.jsx)(n.code,{children:"dryrun"})," parameter to true in the method call, will validate the scene using the API but not create it."]})}),"\n",(0,t.jsx)(n.admonition,{title:"reuse calibration",type:"tip",children:(0,t.jsx)(n.p,{children:"Note that you can, and should, reuse the same calibration for multiple s if possible."})})]})}function g(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>r});var t=s(6540);const o={},a=t.createContext(o);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);