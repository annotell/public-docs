"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[6877],{2614:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"category","label":"Getting started","items":[{"type":"link","label":"Quickstart","href":"/docs/getting-started/quickstart","docId":"getting-started/quickstart","unlisted":false},{"type":"category","label":"Indepth theory","items":[{"type":"link","label":"Advanced setup","href":"/docs/kognic-apis","docId":"kognic-apis","unlisted":false},{"type":"link","label":"Key Concepts","href":"/docs/key-concepts","docId":"key_concepts","unlisted":false},{"type":"link","label":"Data requirements","href":"/docs/getting-started/data-reqs","docId":"getting-started/data-reqs","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Upload data","items":[{"type":"category","label":"Guides","items":[{"type":"link","label":"Upload your First Scene","href":"/docs/upload-your-first-scene","docId":"upload-data/upload-your-first-scene","unlisted":false},{"type":"link","label":"View an Uploaded Scene","href":"/docs/upload-data/view-uploaded-scene","docId":"upload-data/view-uploaded-scene","unlisted":false},{"type":"category","label":"More examples","items":[{"type":"link","label":"Cameras","href":"/docs/kognic-io/scenes/cameras","docId":"kognic-io/scenes/cameras","unlisted":false},{"type":"link","label":"Lidars and Cameras","href":"/docs/kognic-io/scenes/lidars_and_cameras","docId":"kognic-io/scenes/lidars_and_cameras","unlisted":false},{"type":"link","label":"Cameras Sequence","href":"/docs/kognic-io/scenes/cameras_seq","docId":"kognic-io/scenes/cameras_seq","unlisted":false},{"type":"link","label":"Lidars and Cameras Sequence","href":"/docs/kognic-io/scenes/lidars_and_cameras_seq","docId":"kognic-io/scenes/lidars_and_cameras_seq","unlisted":false},{"type":"link","label":"Aggregated Lidars and Cameras Sequence","href":"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq","docId":"kognic-io/scenes/aggregated_lidars_and_cameras_seq","unlisted":false},{"type":"link","label":"Upload ZOD data","href":"/docs/upload-data/upload-zod-data","docId":"upload-data/upload-zod-data","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Indepth theory","items":[{"type":"category","label":"Scenes","items":[{"type":"link","label":"Overview","href":"/docs/kognic-io/overview","docId":"kognic-io/overview","unlisted":false},{"type":"link","label":"Motion Compensation","href":"/docs/kognic-io/scenes/lidars_with_imu_data","docId":"kognic-io/scenes/lidars_with_imu_data","unlisted":false},{"type":"link","label":"Scene Feature Flags","href":"/docs/kognic-io/feature_flags","docId":"kognic-io/feature_flags","unlisted":false},{"type":"link","label":"Working with Scenes & Inputs","href":"/docs/kognic-io/working_with_scenes_and_inputs","docId":"kognic-io/working_with_scenes_and_inputs","unlisted":false},{"type":"link","label":"Supported file formats","href":"/docs/kognic-io/file_formats","docId":"kognic-io/file_formats","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Calibrations","items":[{"type":"link","label":"Calibrations Overview","href":"/docs/kognic-io/calibrations/overview","docId":"kognic-io/calibrations/overview","unlisted":false},{"type":"link","label":"Lidar Calibrations","href":"/docs/kognic-io/calibrations/lidars","docId":"kognic-io/calibrations/lidars","unlisted":false},{"type":"link","label":"Standard Camera Calibrations","href":"/docs/kognic-io/calibrations/cameras-standard","docId":"kognic-io/calibrations/cameras-standard","unlisted":false},{"type":"link","label":"Custom Camera Calibrations","href":"/docs/kognic-io/calibrations/cameras-custom","docId":"kognic-io/calibrations/cameras-custom","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"link","label":"Pre-annotations","href":"/docs/kognic-io/pre_annotations","docId":"kognic-io/pre_annotations","unlisted":false},{"type":"link","label":"Coordinate Systems","href":"/docs/kognic-io/coordinate_systems","docId":"kognic-io/coordinate_systems","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Order annotations","items":[{"type":"link","label":"Prerequisites for Annotation","href":"/docs/upload-data/prerequisites-for-annotation","docId":"upload-data/prerequisites-for-annotation","unlisted":false},{"type":"link","label":"Ordering Annotation","href":"/docs/upload-data/ordering-annotation","docId":"upload-data/ordering-annotation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Download annotations","items":[{"type":"category","label":"Guides","items":[{"type":"link","label":"Download annotations","href":"/docs/download-annotations/","docId":"download-annotations/download-annotations","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Indepth theory","items":[{"type":"link","label":"OpenLABEL format","href":"/docs/openlabel/openlabel-format","docId":"openlabel/openlabel-format","unlisted":false},{"type":"link","label":"kognic-openlabel","href":"/docs/openlabel/python-client","docId":"openlabel/python-client","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Project management","items":[{"type":"link","label":"Projects","href":"/docs/kognic-io/project","docId":"kognic-io/project","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Dataset exploration","items":[{"type":"link","label":"Introduction","href":"/docs/dataset-exploration/introduction","docId":"dataset-exploration/introduction","unlisted":false},{"type":"link","label":"The prediction format","href":"/docs/dataset-exploration/prediction-format","docId":"dataset-exploration/prediction-format","unlisted":false},{"type":"link","label":"Uploading predictions","href":"/docs/dataset-exploration/uploading-predictions","docId":"dataset-exploration/uploading-predictions","unlisted":false},{"type":"link","label":"Understand what your dataset contains","href":"/docs/dataset-exploration/understand-dataset-content","docId":"dataset-exploration/understand-dataset-content","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Annotation integration","items":[{"type":"link","label":"Introduction","href":"/docs/annotation-integration/introduction","docId":"annotation-integration/introduction","unlisted":false},{"type":"link","label":"Review","href":"/docs/annotation-integration/review","docId":"annotation-integration/review","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Support","items":[{"type":"link","label":"Errors & Troubleshooting","href":"/docs/kognic-io/error_handling","docId":"kognic-io/error_handling","unlisted":false},{"type":"link","label":"FAQ","href":"/docs/kognic-io/FAQ","docId":"kognic-io/FAQ","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"annotation-integration/introduction":{"id":"annotation-integration/introduction","title":"Introduction","description":"In order to enable a programmatic interface to the ground truth production process, an HTTP API is made available. This","sidebar":"docs"},"annotation-integration/review":{"id":"annotation-integration/review","title":"Review","description":"A review is a decision that determines whether an annotation is of sufficient quality or if it needs","sidebar":"docs"},"dataset-exploration/introduction":{"id":"dataset-exploration/introduction","title":"Introduction","description":"The dataset API is in an early release stage and might be subject to changes.","sidebar":"docs"},"dataset-exploration/prediction-format":{"id":"dataset-exploration/prediction-format","title":"The prediction format","description":"Predictions use the OpenLabel format, which is expressed in JSON. This is the same format as the one used","sidebar":"docs"},"dataset-exploration/understand-dataset-content":{"id":"dataset-exploration/understand-dataset-content","title":"Understand what your dataset contains","description":"This page will help you comprehend the contents of your dataset.","sidebar":"docs"},"dataset-exploration/uploading-predictions":{"id":"dataset-exploration/uploading-predictions","title":"Uploading predictions","description":"Introduction","sidebar":"docs"},"dataset-refinement/introduction":{"id":"dataset-refinement/introduction","title":"Introduction","description":"This page has moved here"},"dataset-refinement/prediction-format":{"id":"dataset-refinement/prediction-format","title":"The prediction format","description":"This page has moved here"},"dataset-refinement/uploading-predictions":{"id":"dataset-refinement/uploading-predictions","title":"Uploading predictions","description":"This page has moved here"},"download-annotations/download-annotations":{"id":"download-annotations/download-annotations","title":"Download annotations","description":"After annotations are ordered for a scene they will be produced, reviewed and quality-controlled within the Kognic Platform.","sidebar":"docs"},"getting-started/data-reqs":{"id":"getting-started/data-reqs","title":"Data requirements","description":"Kognic Platform Data requirements","sidebar":"docs"},"getting-started/quickstart":{"id":"getting-started/quickstart","title":"Quickstart","description":"Kognic provides a client called kognic-io to simplify calling our APIs using Python.","sidebar":"docs"},"key_concepts":{"id":"key_concepts","title":"Key Concepts","description":"Kognic Platform Concepts","sidebar":"docs"},"kognic-apis":{"id":"kognic-apis","title":"Advanced setup","description":"Overview of Kognic API usage including authentication","sidebar":"docs"},"kognic-io/calibrations/cameras-custom":{"id":"kognic-io/calibrations/cameras-custom","title":"Custom Camera Calibrations","description":"This feature is new in version 1.8.0 of kognic-io and some parts require optional dependencies. Run pip install kognic-io[wasm] to install it.","sidebar":"docs"},"kognic-io/calibrations/cameras-standard":{"id":"kognic-io/calibrations/cameras-standard","title":"Standard Camera Calibrations","description":"The Camera calibration format is based on OpenCVs format and","sidebar":"docs"},"kognic-io/calibrations/lidars":{"id":"kognic-io/calibrations/lidars","title":"Lidar Calibrations","description":"A LIDAR calibration is represented as a LidarCalibration object and consists of a position expressed with three coordinates and a rotation","sidebar":"docs"},"kognic-io/calibrations/overview":{"id":"kognic-io/calibrations/overview","title":"Calibrations Overview","description":"Scenes with 2D and 3D data across various coordinate systems need calibration to align sensors by location and orientation.","sidebar":"docs"},"kognic-io/coordinate_systems":{"id":"kognic-io/coordinate_systems","title":"Coordinate Systems","description":"Scenes such as lidarsandcameras_sequence need to be able to combine the data from different sensors and from","sidebar":"docs"},"kognic-io/error_handling":{"id":"kognic-io/error_handling","title":"Errors & Troubleshooting","description":"Dealing with Errors","sidebar":"docs"},"kognic-io/FAQ":{"id":"kognic-io/FAQ","title":"FAQ","description":"FAQ","sidebar":"docs"},"kognic-io/feature_flags":{"id":"kognic-io/feature_flags","title":"Scene Feature Flags","description":"To make the scene creation process more flexible we support optional feature flags that can be passed at scene creation time.","sidebar":"docs"},"kognic-io/file_formats":{"id":"kognic-io/file_formats","title":"Supported file formats","description":"The API allows uploading different file formats of images and point clouds. In this section we describe the supported formats for each type.","sidebar":"docs"},"kognic-io/overview":{"id":"kognic-io/overview","title":"Overview","description":"Different types of scenes","sidebar":"docs"},"kognic-io/pre_annotations":{"id":"kognic-io/pre_annotations","title":"Pre-annotations","description":"Pre-annotations have many uses in ground-truth production. The pre-annotations feature allows information about the objects already known to be present in an input to be specified. Please reach out to our Advisory Services team to see how they can best be used for your use-case.","sidebar":"docs"},"kognic-io/project":{"id":"kognic-io/project","title":"Projects","description":"Project","sidebar":"docs"},"kognic-io/scenes/aggregated_lidars_and_cameras_seq":{"id":"kognic-io/scenes/aggregated_lidars_and_cameras_seq","title":"Aggregated Lidars and Cameras Sequence","description":"This feature is new in version 1.1.5","sidebar":"docs"},"kognic-io/scenes/cameras":{"id":"kognic-io/scenes/cameras","title":"Cameras","description":"A Cameras  consists of a single frame of camera images, where the frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the Cameras object please check the section related to Scene Overview.","sidebar":"docs"},"kognic-io/scenes/cameras_seq":{"id":"kognic-io/scenes/cameras_seq","title":"Cameras Sequence","description":"A CamerasSeq  consists of a sequence of camera images, where each frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the CamerasSeq object please check the section related to Scene Overview.","sidebar":"docs"},"kognic-io/scenes/lidars_and_cameras":{"id":"kognic-io/scenes/lidars_and_cameras","title":"Lidars and Cameras","description":"A LidarsAndCameras  consists of a single frame which contains 1-9 cameras images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the LidarsAndCameras object please check the section related to Scene Overview.","sidebar":"docs"},"kognic-io/scenes/lidars_and_cameras_seq":{"id":"kognic-io/scenes/lidars_and_cameras_seq","title":"Lidars and Cameras Sequence","description":"A LidarsAndCamerasSeq  consists of a sequence of camera images and lidar point clouds, where each frame consists","sidebar":"docs"},"kognic-io/scenes/lidars_with_imu_data":{"id":"kognic-io/scenes/lidars_with_imu_data","title":"Motion Compensation","description":"An inherent problem with labeling any lidar setup","sidebar":"docs"},"kognic-io/working_with_scenes_and_inputs":{"id":"kognic-io/working_with_scenes_and_inputs","title":"Working with Scenes & Inputs","description":"For detailed information about different scene modalities, check the Scene Types section.","sidebar":"docs"},"openlabel/openlabel-format":{"id":"openlabel/openlabel-format","title":"OpenLABEL format","description":"OpenLABEL is a standardized annotation format developed by ASAM.","sidebar":"docs"},"openlabel/python-client":{"id":"openlabel/python-client","title":"kognic-openlabel","description":"Using this OpenLABEL json schema we have created a python package kognic-openlabel","sidebar":"docs"},"upload-data/ordering-annotation":{"id":"upload-data/ordering-annotation","title":"Ordering Annotation","description":"As outlined on the previous page, annotation requires various configuration to have been completed in order for scenes to be turned into annotations.","sidebar":"docs"},"upload-data/prerequisites-for-annotation":{"id":"upload-data/prerequisites-for-annotation","title":"Prerequisites for Annotation","description":"Before scenes can be annotated a few things need to be in place. From a developer perspective there is not too much to do;","sidebar":"docs"},"upload-data/upload-your-first-scene":{"id":"upload-data/upload-your-first-scene","title":"Upload your First Scene","description":"When uploading raw data to the Kognic Platform, you need to do so in the form of a scene.","sidebar":"docs"},"upload-data/upload-zod-data":{"id":"upload-data/upload-zod-data","title":"Upload ZOD data","description":"This tutorial will guide you through uploading different scene types using the Zenseact Open Dataset (ZOD).","sidebar":"docs"},"upload-data/view-uploaded-scene":{"id":"upload-data/view-uploaded-scene","title":"View an Uploaded Scene","description":"Once a scene has been created, and we have processed it, it is possible to view it in the annotation tool (without annotation capabilities).","sidebar":"docs"}}}}')}}]);