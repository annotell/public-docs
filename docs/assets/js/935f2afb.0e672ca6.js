"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"category","label":"Kognic Auth","items":[{"type":"link","label":"Kognic Auth","href":"/docs/kognic-auth","docId":"kognic-auth"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Key Concepts","items":[{"type":"link","label":"Key Concepts","href":"/docs/","docId":"key_concepts"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Kognic IO","items":[{"type":"link","label":"Projects","href":"/docs/kognic-io/project","docId":"kognic-io/project"},{"type":"category","label":"Inputs","items":[{"type":"link","label":"Overview","href":"/docs/kognic-io/overview","docId":"kognic-io/overview"},{"type":"link","label":"Annotation Types","href":"/docs/kognic-io/annotation_types","docId":"kognic-io/annotation_types"},{"type":"category","label":"Input Types","items":[{"type":"link","label":"Cameras","href":"/docs/kognic-io/inputs/cameras","docId":"kognic-io/inputs/cameras"},{"type":"link","label":"Lidars and Cameras","href":"/docs/kognic-io/inputs/lidars_and_cameras","docId":"kognic-io/inputs/lidars_and_cameras"},{"type":"link","label":"Cameras Sequence","href":"/docs/kognic-io/inputs/cameras_seq","docId":"kognic-io/inputs/cameras_seq"},{"type":"link","label":"Lidars and Cameras Sequence","href":"/docs/kognic-io/inputs/lidars_and_cameras_seq","docId":"kognic-io/inputs/lidars_and_cameras_seq"},{"type":"link","label":"Aggregated Lidars and Cameras Sequence","href":"/docs/kognic-io/inputs/aggregated_lidars_and_cameras_seq","docId":"kognic-io/inputs/aggregated_lidars_and_cameras_seq"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Working with Inputs","href":"/docs/kognic-io/working_with_inputs","docId":"kognic-io/working_with_inputs"},{"type":"link","label":"Pre-annotations","href":"/docs/kognic-io/pre_annotations","docId":"kognic-io/pre_annotations"},{"type":"link","label":"Motion Compensation","href":"/docs/kognic-io/inputs/lidars_with_imu_data","docId":"kognic-io/inputs/lidars_with_imu_data"},{"type":"link","label":"Input Feature Flags","href":"/docs/kognic-io/feature_flags","docId":"kognic-io/feature_flags"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Downloading Annotations","href":"/docs/kognic-io/annotations","docId":"kognic-io/annotations"},{"type":"link","label":"Calibrations","href":"/docs/kognic-io/calibrations","docId":"kognic-io/calibrations"},{"type":"link","label":"Coordinate Systems","href":"/docs/kognic-io/coordinate_systems","docId":"kognic-io/coordinate_systems"},{"type":"link","label":"Errors","href":"/docs/kognic-io/error_handling","docId":"kognic-io/error_handling"},{"type":"link","label":"FAQ","href":"/docs/kognic-io/FAQ","docId":"kognic-io/FAQ"},{"type":"category","label":"Supported File Formats","items":[{"type":"link","label":"Images","href":"/docs/kognic-io/resources/images","docId":"kognic-io/resources/images"},{"type":"link","label":"Point clouds","href":"/docs/kognic-io/resources/pointclouds","docId":"kognic-io/resources/pointclouds"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"OpenLABEL","items":[{"type":"link","label":"OpenLABEL format","href":"/docs/openlabel/openlabel-format","docId":"openlabel/openlabel-format"},{"type":"link","label":"The Python client","href":"/docs/openlabel/python-client","docId":"openlabel/python-client"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Migration Guide","items":[{"type":"link","label":"annotell-input-api to kognic-io","href":"/docs/a2k_migration_guide","docId":"a2k_migration_guide"}],"collapsed":true,"collapsible":true}]},"docs":{"a2k_migration_guide":{"id":"a2k_migration_guide","title":"annotell-input-api to kognic-io","description":"Overview","sidebar":"docs"},"key_concepts":{"id":"key_concepts","title":"Key Concepts","description":"Kognic Platform Concepts","sidebar":"docs"},"kognic-auth":{"id":"kognic-auth","title":"Kognic Auth","description":"How to authenticate with Kognic APIs","sidebar":"docs"},"kognic-io/annotation_types":{"id":"kognic-io/annotation_types","title":"Annotation Types","description":"Annotation Types are something that can be configured for a project either during or after","sidebar":"docs"},"kognic-io/annotations":{"id":"kognic-io/annotations","title":"Downloading Annotations","description":"Annotations are made available for each input and annotation type as soon as they are quality assured by the Kognic platform. Information about the format can be found in the Key Concepts section.","sidebar":"docs"},"kognic-io/calibrations":{"id":"kognic-io/calibrations","title":"Calibrations","description":"Scenes including both a 2D and 3D representation such as lidarsandcameras deal with data in a number of different","sidebar":"docs"},"kognic-io/coordinate_systems":{"id":"kognic-io/coordinate_systems","title":"Coordinate Systems","description":"Scenes such as lidarsandcameras_sequence need to be able to combine the data from different sensors and from","sidebar":"docs"},"kognic-io/error_handling":{"id":"kognic-io/error_handling","title":"Errors","description":"Dealing with Errors","sidebar":"docs"},"kognic-io/FAQ":{"id":"kognic-io/FAQ","title":"FAQ","description":"FAQ","sidebar":"docs"},"kognic-io/feature_flags":{"id":"kognic-io/feature_flags","title":"Input Feature Flags","description":"To make the input creation process more flexible we support optional feature flags that are passed at input creation time.","sidebar":"docs"},"kognic-io/inputs/aggregated_lidars_and_cameras_seq":{"id":"kognic-io/inputs/aggregated_lidars_and_cameras_seq","title":"Aggregated Lidars and Cameras Sequence","description":"This feature is new in version 1.1.5","sidebar":"docs"},"kognic-io/inputs/cameras":{"id":"kognic-io/inputs/cameras","title":"Cameras","description":"A Cameras input consists of a single frame of camera images, where the frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the Cameras object please check the section related to Input Overview.","sidebar":"docs"},"kognic-io/inputs/cameras_seq":{"id":"kognic-io/inputs/cameras_seq","title":"Cameras Sequence","description":"A CamerasSeq input consists of a sequence of camera images, where each frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the CamerasSeq object please check the section related to Input Overview.","sidebar":"docs"},"kognic-io/inputs/lidars_and_cameras":{"id":"kognic-io/inputs/lidars_and_cameras","title":"Lidars and Cameras","description":"A LidarsAndCameras input consists of a single frame which contains 1-9 cameras images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the LidarsAndCameras object please check the section related to Input Overview.","sidebar":"docs"},"kognic-io/inputs/lidars_and_cameras_seq":{"id":"kognic-io/inputs/lidars_and_cameras_seq","title":"Lidars and Cameras Sequence","description":"A LidarsAndCamerasSeq input consists of a sequence of camera images and lidar point clouds, where each frame consists","sidebar":"docs"},"kognic-io/inputs/lidars_with_imu_data":{"id":"kognic-io/inputs/lidars_with_imu_data","title":"Motion Compensation","description":"An inherent problem with labeling any lidar setup","sidebar":"docs"},"kognic-io/overview":{"id":"kognic-io/overview","title":"Overview","description":"Different types of inputs","sidebar":"docs"},"kognic-io/pre_annotations":{"id":"kognic-io/pre_annotations","title":"Pre-annotations","description":"This feature is in an early release stage and might be subject to changes","sidebar":"docs"},"kognic-io/project":{"id":"kognic-io/project","title":"Projects","description":"Project","sidebar":"docs"},"kognic-io/resources/images":{"id":"kognic-io/resources/images","title":"Images","description":"The API allows uploading of annotation project related data such as images and point clouds. For images, we currently support two formats: png and jpg.","sidebar":"docs"},"kognic-io/resources/pointclouds":{"id":"kognic-io/resources/pointclouds","title":"Point clouds","description":"The API allows uploading of annotation project related data such as images and point clouds. Kognic uses a potree format internally to represent and present point clouds, this means that uploaded point cloud data needs to be converted into this format before it can be used as input in the system. We currently support automatic conversion of three formats: pcd, csv and las. The converter does not however exhaustively support all possible versions of these formats, see below for details of each format.","sidebar":"docs"},"kognic-io/working_with_inputs":{"id":"kognic-io/working_with_inputs","title":"Working with Inputs","description":"Creating Inputs","sidebar":"docs"},"openlabel/openlabel-format":{"id":"openlabel/openlabel-format","title":"OpenLABEL format","description":"OpenLABEL is a standardized annotation format developed by ASAM.","sidebar":"docs"},"openlabel/python-client":{"id":"openlabel/python-client","title":"The Python client","description":"Using this schema we have developed a python client kognic-openlabel which makes it easier","sidebar":"docs"}}}')}}]);