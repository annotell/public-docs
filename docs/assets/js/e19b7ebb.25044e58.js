"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[8208],{4949:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var t=s(4848),a=s(8453);const o={title:"Aggregated Lidars and Cameras Sequence"},r=void 0,i={id:"kognic-io/scenes/aggregated_lidars_and_cameras_seq",title:"Aggregated Lidars and Cameras Sequence",description:"This feature is new in version 1.1.5",source:"@site/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq.md",sourceDirName:"kognic-io/scenes",slug:"/kognic-io/scenes/aggregated_lidars_and_cameras_seq",permalink:"/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq",draft:!1,unlisted:!1,editUrl:"https://github.com/annotell/public-docs/edit/master/docs-src/docs/kognic-io/scenes/aggregated_lidars_and_cameras_seq.md",tags:[],version:"current",frontMatter:{title:"Aggregated Lidars and Cameras Sequence"},sidebar:"docs",previous:{title:"Lidars and Cameras Sequence",permalink:"/docs/kognic-io/scenes/lidars_and_cameras_seq"},next:{title:"Upload ZOD data",permalink:"/docs/upload-data/upload-zod-data"}},d={},c=[];function l(e){const n={a:"a",admonition:"admonition",code:"code",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"This feature is new in version 1.1.5"})}),"\n",(0,t.jsxs)(n.p,{children:["An ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"})," scene consists of a sequence of camera images and lidar point clouds, where each\nframe consists on 1-9 camera images as well as 1-20 point clouds (in the case where you have pre-aggregated your point clouds, the first frame consists of 1-20 point clouds and all other frames 0 point clouds). What differentiates ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"}),"\nfrom ",(0,t.jsx)(n.code,{children:"LidarsAndCamerasSeq"})," is that point clouds are aggregated over time during annotation which results in one big\npoint cloud in the coordinate system of the first frame. Therefore, ego motion data is ",(0,t.jsx)(n.strong,{children:"mandatory"})," for this type of\nscene. For more documentation on what each field corresponds to in the ",(0,t.jsx)(n.code,{children:"AggregatedLidarsAndCamerasSeq"})," object please\ncheck the section related to ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/overview",children:"Scene Overview"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Refer to ",(0,t.jsx)(n.a,{href:"/docs/kognic-io/coordinate_systems",children:"Coordinate Systems"})," for more information about what coordinate systems to use."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"reference",children:"https://github.com/annotell/kognic-io-examples-python/blob/master/examples/agg_lidars_and_cameras_seq.py\n"})}),"\n",(0,t.jsx)(n.admonition,{title:"Use dryrun to validate scene",type:"tip",children:(0,t.jsxs)(n.p,{children:["Setting ",(0,t.jsx)(n.code,{children:"dryrun"})," parameter to true in the method call, will validate the scene using the API but not create it."]})}),"\n",(0,t.jsx)(n.admonition,{title:"reuse calibration",type:"tip",children:(0,t.jsx)(n.p,{children:"Note that you can, and should, reuse the same calibration for multiple s if possible."})})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>i});var t=s(6540);const a={},o=t.createContext(a);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);