"use strict";(self.webpackChunkkognic_sdk_docs=self.webpackChunkkognic_sdk_docs||[]).push([[3958],{3800:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"getting-started/data-reqs","title":"Data requirements","description":"Kognic Platform Data requirements","source":"@site/docs/getting-started/data-reqs.mdx","sourceDirName":"getting-started","slug":"/getting-started/data-reqs","permalink":"/docs/getting-started/data-reqs","draft":false,"unlisted":false,"editUrl":"https://github.com/annotell/public-docs/edit/master/docs-src/docs/getting-started/data-reqs.mdx","tags":[],"version":"current","frontMatter":{"title":"Data requirements","id":"data-reqs","description":"Kognic Platform Data requirements"},"sidebar":"docs","previous":{"title":"Key Concepts","permalink":"/docs/key-concepts"},"next":{"title":"Upload your First Scene","permalink":"/docs/upload-your-first-scene"}}');var o=n(4848),s=n(8453);const r={title:"Data requirements",id:"data-reqs",description:"Kognic Platform Data requirements"},a=void 0,l={},c=[{value:"Images",id:"images",level:2},{value:"Point clouds",id:"point-clouds",level:2},{value:"Calibrations",id:"calibrations",level:2},{value:"Ego vehicle poses",id:"ego-vehicle-poses",level:2},{value:"FAQ",id:"faq",level:2},{value:"How do I check the if the calibration is correct",id:"how-do-i-check-the-if-the-calibration-is-correct",level:3}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components},{Details:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.p,{children:["The Kognic platform supports multiple types of data and information to enable an efficient annotation process, mainly ",(0,o.jsx)(t.strong,{children:"images"}),", ",(0,o.jsx)(t.strong,{children:"point clouds"}),", ",(0,o.jsx)(t.strong,{children:"calibrations"})," and ",(0,o.jsx)(t.strong,{children:"ego vehicle poses/IMU data"}),". In this section we describe the supported formats for each type."]}),"\n",(0,o.jsx)(t.h2,{id:"images",children:"Images"}),"\n",(0,o.jsxs)(t.p,{children:["We currently support the following image formats: ",(0,o.jsx)(t.strong,{children:"png"}),", ",(0,o.jsx)(t.strong,{children:"jpg"}),", ",(0,o.jsx)(t.strong,{children:"jpeg"}),", ",(0,o.jsx)(t.strong,{children:"webp"})," and ",(0,o.jsx)(t.strong,{children:"avif"}),"."]}),"\n",(0,o.jsx)(t.h2,{id:"point-clouds",children:"Point clouds"}),"\n",(0,o.jsxs)(t.p,{children:["Kognic uses a potree format internally to represent and present point clouds, this means that uploaded point cloud data needs to be converted into this format before it can be used as scene in the system.\nWe currently support automatic conversion of the following formats: ",(0,o.jsx)(t.strong,{children:"pcd"}),", ",(0,o.jsx)(t.strong,{children:"csv"})," and ",(0,o.jsx)(t.strong,{children:"las"}),". The converter does not however exhaustively support all possible versions of these formats, see below for details of each format."]}),"\n",(0,o.jsxs)(t.p,{children:["A timestamp field must always be present in point clouds, both in single-frame and sequence scenes, but the values are irrelevant ",(0,o.jsx)(t.a,{href:"./scenes/lidars_with_imu_data.md#enabledisable-motion-compensation",children:"if motion compensation is not enabled"}),".\nAn intensity field may be provided in point clouds and will be preserved during conversion. If omitted, the intensity for all points will be zero.\nColor and other auxiliary data that is not used in the platform is currently discarded in the conversion to potree."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.a,{href:"/docs/kognic-io/file_formats",children:"A more detailed description of the point cloud formats is found here"})}),"\n",(0,o.jsx)(t.h2,{id:"calibrations",children:"Calibrations"}),"\n",(0,o.jsxs)(t.p,{children:["Scenes with 2D and 3D data across various ",(0,o.jsx)(t.a,{href:"/docs/kognic-io/coordinate_systems",children:"coordinate systems"})," need ",(0,o.jsx)(t.strong,{children:"calibrations"})," to align sensors by location and orientation.\nBoth an extrinsic calibration that maps the position and rotation in 3D relative to the reference system and an intrinsic camera calibration that projects the 3D points to camera's image plane.\nAll extrinsic calibrations shall represent the transformation from the sensor to the reference system."]}),"\n",(0,o.jsxs)(n,{children:[(0,o.jsx)("summary",{children:(0,o.jsx)("h2",{style:{margin:"0px"},children:"Types of calibrations"})}),(0,o.jsx)(t.p,{children:"All calibrations detail a sensor\u2019s 3D position and orientation relative to the reference system, the calibrations shall map the transformation from the sensor to the reference system.\nThey also map 3D points to the camera\u2019s image plane."}),(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["For LiDAR/RADAR, there is only one type of calibration available, ",(0,o.jsx)(t.a,{href:"/docs/kognic-io/calibrations/lidars",children:"read more here"})]}),"\n",(0,o.jsxs)(t.li,{children:["For cameras, we support different types of ",(0,o.jsx)(t.a,{href:"/docs/kognic-io/calibrations/cameras-standard",children:"standard camera calibrations"}),", where you have to provide the intrinsic parameters of the camera.\nAll camera calibration are implemented using the ",(0,o.jsx)(t.strong,{children:"OpenCV coordinate system"}),"."]}),"\n"]}),(0,o.jsx)(t.admonition,{title:"Unsupported camera model",type:"tip",children:(0,o.jsxs)(t.p,{children:["If your camera model is not supported, you can also provide\na ",(0,o.jsx)(t.a,{href:"/docs/kognic-io/calibrations/cameras-custom",children:"custom camera calibration"})," where you provide the implementation in the form of a WebAssembly module."]})})]}),"\n",(0,o.jsx)(t.h2,{id:"ego-vehicle-poses",children:"Ego vehicle poses"}),"\n",(0,o.jsx)(t.p,{children:"An ego vehicle pose can optionally be added to each frame which describes the relative pose.\nIt is highly recommended for 3D sequence annotations as it enables more efficient workflows and functions in the Kognic platform, especially for static objects."}),"\n",(0,o.jsx)(t.p,{children:"The pose is represented using a 3D position and a quaternion in the local coordinate system.\nFor a single lidar input the poses shall be on the lidar coordinate system. For multi-lidar inputs the poses shall be on the reference coordinate system"}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{style:{textAlign:"left"},children:"Key"}),(0,o.jsx)(t.th,{style:{textAlign:"left"},children:"Value"}),(0,o.jsx)(t.th,{style:{textAlign:"left"},children:"Parameters"})]})}),(0,o.jsxs)(t.tbody,{children:[(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{style:{textAlign:"left"},children:(0,o.jsx)(t.code,{children:"rotation_quaternion"})}),(0,o.jsxs)(t.td,{style:{textAlign:"left"},children:["A ",(0,o.jsx)(t.code,{children:"RotationQuaternion"})," object"]}),(0,o.jsxs)(t.td,{style:{textAlign:"left"},children:[(0,o.jsx)(t.code,{children:"w"}),", ",(0,o.jsx)(t.code,{children:"x"}),", ",(0,o.jsx)(t.code,{children:"y"}),", ",(0,o.jsx)(t.code,{children:"z"})]})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{style:{textAlign:"left"},children:(0,o.jsx)(t.code,{children:"position"})}),(0,o.jsxs)(t.td,{style:{textAlign:"left"},children:["A ",(0,o.jsx)(t.code,{children:"Position"})," object"]}),(0,o.jsxs)(t.td,{style:{textAlign:"left"},children:[(0,o.jsx)(t.code,{children:"x"}),", ",(0,o.jsx)(t.code,{children:"y"}),", ",(0,o.jsx)(t.code,{children:"z"})]})]})]})]}),"\n",(0,o.jsxs)(t.p,{children:["In addition to the frame poses, there is also the option to upload higher frequency IMU data to enable motion compensation.\nMore details on motion compensation can be found ",(0,o.jsx)(t.a,{href:"/docs/kognic-io/scenes/lidars_with_imu_data",children:"here"})]}),"\n",(0,o.jsx)(t.h2,{id:"faq",children:"FAQ"}),"\n",(0,o.jsx)(t.h3,{id:"how-do-i-check-the-if-the-calibration-is-correct",children:"How do I check the if the calibration is correct"}),"\n",(0,o.jsxs)(t.p,{children:["Follow the instructions in the ",(0,o.jsx)(t.a,{href:"/docs/upload-data/view-uploaded-scene",children:"link"})," to check your calibration.\nYou can also quickly check the orientation of your camera quickly by opening a scene and go to the 3D viewer.\nAt the ego vehicle there is a small yellow circle representing the position and a red arrow representing the orientation of the currently selected camera. Check that the arrow is pointing in the direction that you expect, for example in front of the ego vehicle."]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var i=n(6540);const o={},s=i.createContext(o);function r(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);