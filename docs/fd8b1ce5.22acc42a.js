(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{115:function(e,n,t){"use strict";t.r(n),t.d(n,"MDXContext",(function(){return m})),t.d(n,"MDXProvider",(function(){return u})),t.d(n,"mdx",(function(){return b})),t.d(n,"useMDXComponents",(function(){return p})),t.d(n,"withMDXComponents",(function(){return c}));var a=t(0),i=t.n(a);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(){return(o=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e}).apply(this,arguments)}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function d(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var m=i.a.createContext({}),c=function(e){return function(n){var t=p(n.components);return i.a.createElement(e,o({},n,{components:t}))}},p=function(e){var n=i.a.useContext(m),t=n;return e&&(t="function"==typeof e?e(n):d(d({},n),e)),t},u=function(e){var n=p(e.components);return i.a.createElement(m.Provider,{value:n},e.children)},h={inlineCode:"code",wrapper:function(e){var n=e.children;return i.a.createElement(i.a.Fragment,{},n)}},f=i.a.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,o=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=p(t),c=a,u=m["".concat(o,".").concat(c)]||m[c]||h[c]||r;return t?i.a.createElement(u,d(d({ref:n},s),{},{components:t})):i.a.createElement(u,d({ref:n},s))}));function b(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,o=new Array(r);o[0]=f;var s={};for(var d in n)hasOwnProperty.call(n,d)&&(s[d]=n[d]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var l=2;l<r;l++)o[l]=t[l];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,t)}f.displayName="MDXCreateElement"},60:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return o})),t.d(n,"metadata",(function(){return s})),t.d(n,"rightToc",(function(){return d})),t.d(n,"default",(function(){return m}));var a=t(3),i=t(8),r=(t(0),t(115)),o={title:"Overview"},s={unversionedId:"kognic-io/overview",id:"kognic-io/overview",isDocsHomePage:!1,title:"Overview",description:"Different types of inputs",source:"@site/docs/kognic-io/overview.md",slug:"/kognic-io/overview",permalink:"/public-docs/docs/kognic-io/overview",editUrl:"https://github.com/annotell/public-docs/docs-src/docs/kognic-io/overview.md",version:"current",sidebar:"docs",previous:{title:"Projects",permalink:"/public-docs/docs/kognic-io/project"},next:{title:"Annotation Types",permalink:"/public-docs/docs/kognic-io/annotation_types"}},d=[{value:"Different types of inputs",id:"different-types-of-inputs",children:[{value:"Sequential vs non-sequential",id:"sequential-vs-non-sequential",children:[]}]},{value:"Input Fields",id:"input-fields",children:[{value:"External Id",id:"external-id",children:[]},{value:"Sensor Specification",id:"sensor-specification",children:[]},{value:"Calibration Id",id:"calibration-id",children:[]},{value:"Metadata",id:"metadata",children:[]},{value:"Frame (non-sequential inputs)",id:"frame-non-sequential-inputs",children:[]},{value:"Frames (sequential inputs)",id:"frames-sequential-inputs",children:[]}]},{value:"Image &amp; Pointcloud Resources",id:"image--pointcloud-resources",children:[]},{value:"IMU Data",id:"imu-data",children:[]},{value:"Input Feature Flags",id:"input-feature-flags",children:[]}],l={rightToc:d};function m(e){var n=e.components,t=Object(i.a)(e,["components"]);return Object(r.mdx)("wrapper",Object(a.default)({},l,t,{components:n,mdxType:"MDXLayout"}),Object(r.mdx)("h2",{id:"different-types-of-inputs"},"Different types of inputs"),Object(r.mdx)("p",null,"An Input represents a grouping of sensor data (e.g. camera images, lidar pointclouds) that should be annotated together. Any information necessary to express the relationship between the sensors and their captured data is also present, be it camera resolution, sensor name or the frequency at which the data was recorded at."),Object(r.mdx)("p",null,"There are different input types depending on what kind of sensor(s) are used to represent the contents of the input. For example, if we want to create an input only consisting of data from camera sensors then we would use the input type ",Object(r.mdx)("inlineCode",{parentName:"p"},"Cameras"),". Similarly, if we want to create an input consisting of lidar sensors and camera sensors then we would use the input type ",Object(r.mdx)("inlineCode",{parentName:"p"},"LidarsAndCameras"),". Additionally, inputs can either be ",Object(r.mdx)("strong",{parentName:"p"},"sequential")," or ",Object(r.mdx)("strong",{parentName:"p"},"non-sequential"),"."),Object(r.mdx)("h3",{id:"sequential-vs-non-sequential"},"Sequential vs non-sequential"),Object(r.mdx)("p",null,"Sequential inputs represent a ",Object(r.mdx)("em",{parentName:"p"},"sequence")," of sensor data, whereas non-sequential inputs only contain a single snapshot of sensor data. The sequential relationship is expressed via a sequence of ",Object(r.mdx)("strong",{parentName:"p"},"Frames"),", where each ",Object(r.mdx)("strong",{parentName:"p"},"Frame")," object contains information related to what kind of sensor data constitues the frame (e.g. which image and/or point cloud is a part of the Frame) as well as a ",Object(r.mdx)("em",{parentName:"p"},"relative timestamp")," that captures where in time (relative to the other frames) the Frame is located."),Object(r.mdx)("p",null,"Non-sequential inputs only express a single snapshot of sensor data. As such, these kinds of inputs only contain a single Frame object and do not require any relative timestamp information."),Object(r.mdx)("p",null,"Sequential input types are easily identified by the suffix ",Object(r.mdx)("inlineCode",{parentName:"p"},"Seq")," present in their name."),Object(r.mdx)("p",null,"The following input types are currently supported"),Object(r.mdx)("ul",null,Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"Cameras")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"LidarsAndCameras")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"CamerasSeq")),Object(r.mdx)("li",{parentName:"ul"},Object(r.mdx)("inlineCode",{parentName:"li"},"LidarAndCamerasSeq"))),Object(r.mdx)("h2",{id:"input-fields"},"Input Fields"),Object(r.mdx)("p",null,"All non-sequential inputs have the following structure"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class Input(BaseModel):\n    external_id: str\n    frame: Frame\n    sensor_specification: SensorSpecification\n    calibration_id: Optional[str] # Required if using lidar sensors\n    metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)\n")),Object(r.mdx)("p",null,"Sequential inputs are similarly represented, except that they instead contain a list of Frames"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class InputSeq(BaseModel):\n    external_id: str\n    frames: List[Frame]\n    sensor_specification: SensorSpecification\n    calibration_id: Optional[str] # Required if using lidar sensors\n    metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)\n")),Object(r.mdx)("p",null,"The fields contain all of the information required to create the input."),Object(r.mdx)("h3",{id:"external-id"},"External Id"),Object(r.mdx)("p",null,"Whenever an input is uploaded it automatically gets an UUID, this is used as the primary identifier by Kognic and by all of our internal systems. However, in order to make communication around specific inputs easier we also allow for clients to include any kind of identifier to the input via the external id."),Object(r.mdx)("h3",{id:"sensor-specification"},"Sensor Specification"),Object(r.mdx)("p",null,"The sensor specification contains information related to the different camera and/or lidar sensors\nused for capturing the data present on the input."),Object(r.mdx)("p",null,'The additional fields are optional and relate to specifying the order of the camera sensors and\nhuman readable variants of the sensor name (e.g. "Front Camera" instead of "FC").'),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"class SensorSpecification(BaseModel):\n    sensor_to_pretty_name: Optional[Dict[str, str]] = None\n    sensor_order: Optional[List[str]] = None\n")),Object(r.mdx)("p",null,"As an example, let's say we have three different camera sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". The ",Object(r.mdx)("inlineCode",{parentName:"p"},"R")," sensor is mounted on the right side of the ego vehicle, the ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," sensor at the front and the ",Object(r.mdx)("inlineCode",{parentName:"p"},"L")," sensor to the left. Creating a sensor specification for this scenario would correspond to"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'sensor_spec = SensorSpecification(\n    sensor_to_pretty_name={\n        "R": "Right Camera",\n        "F": "Front Camera",\n        "L": "Left Camera"\n    },\n    sensor_order=["L", "F", "R"]\n)\n')),Object(r.mdx)("p",null,"The specified ",Object(r.mdx)("inlineCode",{parentName:"p"},"sensor_order")," will cause the different camera sensors to be presented in a clockwise manner in the annotation tool (Left -> Front -> Right), while the ",Object(r.mdx)("inlineCode",{parentName:"p"},"sensor_to_pretty_name")," parameter will result in the annotation tool showing the human readable version of all the sensor names when changing sensor."),Object(r.mdx)("h3",{id:"calibration-id"},"Calibration Id"),Object(r.mdx)("p",null,"Any input consisting of lidar and camera sensors requires a calibration. The calibration captures the spatial relationship (position and rotation) between the different sensors, as well as different camera specific parameters."),Object(r.mdx)("p",null,"This information is used by the annotation tool to highlight regions in the point cloud visible in the selected camera sensors as well as for projecting information from the pointcloud onto the different camera sensors (points, cuboids etc)."),Object(r.mdx)("p",null,"Detailed documentation on how to create calibrations via the API is present in the ",Object(r.mdx)("a",{parentName:"p",href:"calibration"},"Calibration section"),"."),Object(r.mdx)("p",null,"When including calibration id make sure that all of the sensors present on the input are also present in the calibration as well. If this is not the case the input will not be created and a validation error will be returned by the API."),Object(r.mdx)("p",null,"Inputs without a lidar sensor do not require a calibration."),Object(r.mdx)("h3",{id:"metadata"},"Metadata"),Object(r.mdx)("p",null,"Metadata can be added to inputs via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"metadata")," field. It consists of ",Object(r.mdx)("em",{parentName:"p"},"flat")," key-value pairs, which means that nested data structures are not allowed. Metadata can be used to include additional information about an input.\nNothing specified in the metadata can be seen by the annotators, but there are some reserved keywords which can alter the annotation tools behaviour, and can be found here:"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/kognic-io/kognic/io/model/input/metadata/metadata.py\n")),Object(r.mdx)("h3",{id:"frame-non-sequential-inputs"},"Frame (non-sequential inputs)"),Object(r.mdx)("p",null,"The Frame object specifies the binary data to be annotated (.jpg, .png, .las etc) as well as which sensor the data originated from."),Object(r.mdx)("p",null,"The Frame object is different for each input type since they all support different kinds of sensors, even though the overall structure is the same."),Object(r.mdx)("p",null,"As an example, let's say we want to create an input consiting of images from three different camera sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"F")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". The corresponding binary data is present in the files ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_R.jpg"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_F.jpg")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"img_cam_F.jpg"),". This would correspond to creating a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Cameras")," input."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'cameras_input = Cameras(\n    ...,\n    frame=Frame(\n        images=[\n            Image("img_cam_R.jpg", sensor_name="R"),\n            Image("img_cam_F.jpg", sensor_name="F"),\n            Image("img_cam_L.jpg", sensor_name="L"),\n        ]\n    )\n)\n')),Object(r.mdx)("p",null,"Similarly, if we also had an associated lidar pointcloud from the sensor ",Object(r.mdx)("inlineCode",{parentName:"p"},"VDL-64")," and a corresponding binary file ",Object(r.mdx)("inlineCode",{parentName:"p"},"scan_vdl_64.las")," we would instead express this as a ",Object(r.mdx)("inlineCode",{parentName:"p"},"LidarsAndCameras")," input instead."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'lidars_and_cameras = LidarsAndCameras(\n    ...,\n    frame=Frame(\n        images=[\n            Image("img_cam_R.jpg", sensor_name="R"),\n            Image("img_cam_F.jpg", sensor_name="F"),\n            Image("img_cam_L.jpg", sensor_name="L"),\n        ],\n        point_clouds=[\n            PointCloud("scan_vdl_64.las", sensor_name="VDL-64")\n        ]\n    )\n\n)\n')),Object(r.mdx)("h3",{id:"frames-sequential-inputs"},"Frames (sequential inputs)"),Object(r.mdx)("p",null,"Sequential inputs deal with a list of Frame objects instead of a single Frame object. In addition, Frame objects associated with sequential inputs have three additional parameters not present in their non-sequential Frame counterparts: ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id"),", ",Object(r.mdx)("inlineCode",{parentName:"p"},"relative_timestamp")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"metadata"),"."),Object(r.mdx)("p",null,"The sequential relationship is expressed via the ordering of the Frame objects in the ",Object(r.mdx)("inlineCode",{parentName:"p"},"frames")," list"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = Frame(...)\nframe_2 = Frame(...)\nframe_3 = Frame(...)\nframes = [frame_1, frame_2, frame_3]\n")),Object(r.mdx)("p",null,"This representation captures that ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_1")," comes first, then ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_2")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_3"),", but it does not express how much time has passed between the different frames. This information is encoded via the ",Object(r.mdx)("inlineCode",{parentName:"p"},"relative_timestamp")," parameter present on each Frame object. The relative timestamp is expressed in milliseconds and describes the relative time between the Frame and the start of the input."),Object(r.mdx)("p",null,"For example, let's say that the sensor data is collected and aggregated at 2Hz. That would then be expressed as"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = Frame(..., relative_timestamp=0)\nframe_2 = Frame(..., relative_timestamp=500)\nframe_3 = Frame(..., relative_timestamp=1000)\nframes = [frame_1, frame_2, frame_3]\n")),Object(r.mdx)("p",null,"The ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," is expressed as a string and is used to produce a unique identifier for each frame in the list of frames.\nThe ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," is used as a top-level key in the produced annotations, indicating which parts of the complete annotation\nbelong to this specific frame."),Object(r.mdx)("p",null,"A common use case is to use uuids for each ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id"),", or a combination of ",Object(r.mdx)("inlineCode",{parentName:"p"},"external_id")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_index"),".\nFor example, if the ",Object(r.mdx)("inlineCode",{parentName:"p"},"external_id")," of the input is ",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101")," then the ",Object(r.mdx)("inlineCode",{parentName:"p"},"frame_id")," could be encoded as\n",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101:0")," for the first frame, ",Object(r.mdx)("inlineCode",{parentName:"p"},"shanghai_20200101:1")," for the second frame and so on."),Object(r.mdx)("p",null,"Similarly to the metadata capability available on an input-level, it's also possible to provide metadata on a ",Object(r.mdx)("em",{parentName:"p"},"frame")," level as well.\nIt behaves the same way, i.e. consists of ",Object(r.mdx)("em",{parentName:"p"},"flat")," key-value pairs and is not exposed to annotators during the production of annotators."),Object(r.mdx)("p",null,"As an example, let's say we want to create an input of type ",Object(r.mdx)("inlineCode",{parentName:"p"},"CamerasSeq")," consisting of 2 frames, each with camera data\nfrom two different sensors ",Object(r.mdx)("inlineCode",{parentName:"p"},"R")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"L"),". If we have individual images for each frame and sensor, this would correspond\nto the following list of frames"),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'frames = [\n    Frame(\n        frame_id="1",\n        relative_timestamp=0,\n        images=[\n            Image("img_L_1.jpg", sensor_name=\'L\'),\n            Image("img_R_1.jpg", sensor_name=\'R\')\n        ]),\n    Frame(\n        frame_id="2",\n        relative_timestamp=500,\n        images=[\n            Image("img_L_2.jpg", sensor_name=\'L\'),\n            Image("img_R_2.jpg", sensor_name=\'R\')\n        ])\n]\n\ncameras_sequence = CamerasSequence(\n        ...,\n        frames=frames,\n    )\n')),Object(r.mdx)("h2",{id:"image--pointcloud-resources"},"Image & Pointcloud Resources"),Object(r.mdx)("p",null,"Every single file containing binary sensor data (e.g. image or pointcloud files) is represented as a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Resource"),", with\n",Object(r.mdx)("inlineCode",{parentName:"p"},"Image")," and ",Object(r.mdx)("inlineCode",{parentName:"p"},"PointCloud")," both being subclasses of it."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/kognic-io/kognic/io/model/input/resources/resource.py#L7-L12\n")),Object(r.mdx)("p",null,"When specifying a ",Object(r.mdx)("inlineCode",{parentName:"p"},"Resource")," object (like ",Object(r.mdx)("inlineCode",{parentName:"p"},"Image")," or ",Object(r.mdx)("inlineCode",{parentName:"p"},"PointCloud"),") it's possible to either:"),Object(r.mdx)("ol",null,Object(r.mdx)("li",{parentName:"ol"},"Refer to ",Object(r.mdx)("em",{parentName:"li"},"local")," files, these will be uploaded (synchronously) to the Kognic platform."),Object(r.mdx)("li",{parentName:"ol"},"Refer to ",Object(r.mdx)("em",{parentName:"li"},"remote")," files via URI, these will only be uploaded (asynchronously) and stored in the Kognic platform if\nmandatory file conversion is necessary. Otherwise, they will be served to annotators via the URI.")),Object(r.mdx)("p",null,Object(r.mdx)("strong",{parentName:"p"},"Alternative 1")," is achieved by setting the parameter ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename")," to the path of the local file and leaving the\nparameter ",Object(r.mdx)("inlineCode",{parentName:"p"},"resource_id")," set the default value of ",Object(r.mdx)("inlineCode",{parentName:"p"},"None"),", e.g."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},"Image(\n    filename=\"/Users/johndoe/images/img_FC.png\",\n    sensor_name='FC'\n)\n")),Object(r.mdx)("p",null,"This file will automatically be uploaded to the Kognic Platform in a synchronous manner when the corresponding ",Object(r.mdx)("inlineCode",{parentName:"p"},"create"),"\nmethod is called for creating the input."),Object(r.mdx)("p",null,Object(r.mdx)("strong",{parentName:"p"},"Alternative 2")," is achieved by setting the parameter ",Object(r.mdx)("inlineCode",{parentName:"p"},"filename")," to just be the filename, and setting the parameter\n",Object(r.mdx)("inlineCode",{parentName:"p"},"resource_id")," to the corresponding URI of the file, e.g."),Object(r.mdx)("pre",null,Object(r.mdx)("code",{parentName:"pre",className:"language-python"},'Image(\n    filename="img_FC.png",\n    sensor_name=\'FC\',\n    resource_id="gs://data-collection/4fcc30af/img_FC.png"\n)\n')),Object(r.mdx)("p",null,"With this approach the file resources will be served to the platform via the URI, which also means that the files will\nnot be stored in Kognic's cloud. However, ",Object(r.mdx)("strong",{parentName:"p"},"an exception to this")," is for pointcloud files that are not in potree format.\nIn cases like this the Kognic plattform will perform an asynchronous download of the files and convert them to potree\nformat. The potree versions of the files will both be stored and served from Kognic's cloud."),Object(r.mdx)("div",{className:"admonition admonition-note alert alert--secondary"},Object(r.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(r.mdx)("h5",{parentName:"div"},Object(r.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(r.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(r.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),Object(r.mdx)("div",{parentName:"div",className:"admonition-content"},Object(r.mdx)("p",{parentName:"div"},"In order to supply ",Object(r.mdx)("inlineCode",{parentName:"p"},"resource_id")," configuration around access to the storage-provider is required. Contact Kognic before\ncreating any inputs using the ",Object(r.mdx)("inlineCode",{parentName:"p"},"resource_id")," approach."))),Object(r.mdx)("h2",{id:"imu-data"},"IMU Data"),Object(r.mdx)("p",null,"Intertial Measurement Unit (IMU) data may be provided for inputs containing LIDAR pointclouds. This can be used to\nperform motion compensation in multi-lidar setups, and by default if any IMU data is provided this will be done.\nMotion compensation may be disabled via an ",Object(r.mdx)("a",{parentName:"p",href:"feature_flags"},"input feature flag"),", for cases where motion compensation has\nalready been performed prior to upload."),Object(r.mdx)("p",null,"Refer to ",Object(r.mdx)("a",{parentName:"p",href:"inputs/lidars_with_imu_data"},"Motion Compensation for Multi-Lidar Setups"),"."),Object(r.mdx)("h2",{id:"input-feature-flags"},"Input Feature Flags"),Object(r.mdx)("p",null,"Control over optional parts of the input creation process is possible via ",Object(r.mdx)("inlineCode",{parentName:"p"},"FeatureFlags")," that are passed when invoking\nthe create operation on the input. Refer to ",Object(r.mdx)("a",{parentName:"p",href:"feature_flags"},"the feature flags documentation")," for details."))}m.isMDXComponent=!0}}]);