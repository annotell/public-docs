[{"title":"annotell-input-api to kognic-io","type":0,"sectionRef":"#","url":"/docs/a2k_migration_guide","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#overview","content":"In September 2022 Annotell rebranded as Kognic. We took opportunity to make some changes to our public Python libraries during the transition. Some of these changes are incompatible with code written against the last version of annotell-input-api (1.3.1 at time of writing). annotell-input-api will now only be updated to fix critical bugs. All users are encouraged to migrate to kognic-io as soon as possible. "},{"title":"Breaking changes​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#breaking-changes","content":""},{"title":"New packages​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#new-packages","content":"Packages under annotell.input_api were moved to kognic.io. In most cases, updating imports is all that will be needed for client code to continue working. "},{"title":"Packaging and imports​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#packaging-and-imports","content":"Previously some classes could be imported via multiple packages due to how we had used * imports internally. The kognic.io.model.* subpackages are now stricter about which internal classes they export than their equivalents in annotell.input_api.model.* It is no longer possible to, for example, use from kognic.io.model.scene.lidars_and_cameras import * or from kognic.io.model.lidars_and_cameras import IMUData to import IMUData along with all other classes related to Lidar+Camera scenes. info Replace instances of from annotell.input_api.model.input.xxx import * with imports for specific classes from kognic.io.model.xxx packages, or with imports from other packages in cases where the import has broken. "},{"title":"Removal of legacy code​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#removal-of-legacy-code","content":"Legacy calibration models were removed, having been deprecated since 2021-11. info Use the typed calibrations from kognic.io.model.calibration.camera and kognic.io.model.calibration.lidar instead of the generic annotell.input_api.model.sensors.LidarCalibration and CameraCalibration. The get_annotations method was removed, having been deprecated since 2021-09. info Use get_project_annotations instead. "},{"title":"Named arguments​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#named-arguments","content":"Many scene model objects such as Image and PointCloud were converted to use pydantic for validation. These classes must now be instanced by passing all non-defaulted properties as named arguments (kwargs). e.g. an Image now requires both filename and sensor_name to be specified. info Ensure named constructor args are present for all non-defaulted fields for model classes like Image and PointCloud. For instance filename was not previously required to be a named arg, now it is. "},{"title":"Non-breaking changes​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#non-breaking-changes","content":""},{"title":"Environment variables​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#environment-variables","content":"Environment variables used by our libraries which are named with the prefix ANNOTELL_ now have a KOGNIC_ equivalent, e.g. ANNOTELL_CREDENTIALS. The old names will continue to work for the time being, with the library code preferring the newer name if both are set. Future changes are not guaranteed to be backwards compatible (e.g. introduction of new variables, or renaming, may not be applied to both prefixes). We therefore recommend that any scripts are updated at the first opportunity. "},{"title":"Validation​","type":1,"pageTitle":"annotell-input-api to kognic-io","url":"/docs/a2k_migration_guide#validation","content":"Many scene model objects (e.g. Image, PointCloud, LidarsAndCameras) have been converted from Python @dataclass to Pydantic models. Client-side validation errors will now be reported differently for these classes. Validation is stricter but the error messages are also more descriptive. Typically validation will be failed because of significant problems: using the wrong datatype for a field, or missing required parameters for an object. Server-side validation is still used to check that the content/values of the input are sound. "},{"title":"Key Concepts","type":0,"sectionRef":"#","url":"/docs/","content":"","keywords":""},{"title":"Project​","type":1,"pageTitle":"Key Concepts","url":"/docs/#project","content":"Project is the top-most concept when interfacing with the Kognic Platform. It is possible to have multiple ongoing projects, and they act as a container for other Kognic resources. Project setup is performed by the Kognic Professional Services team during the Guideline Agreement Process (GAP) of a new client engagement. To use projects within the Kognic APIs, they can be identified using an external identifier. "},{"title":"Batch​","type":1,"pageTitle":"Key Concepts","url":"/docs/#batch","content":"Input batches allow grouping of inputs into smaller batches within a project. By default, every project has at least one input batch. Ongoing projects can benefit from using batches in two ways Group inputs collected at the same timePerform guideline or task definition changes without the need for retroactive changes. "},{"title":"Batch Status​","type":1,"pageTitle":"Key Concepts","url":"/docs/#batch-status","content":"Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed. "},{"title":"Request​","type":1,"pageTitle":"Key Concepts","url":"/docs/#request","content":"During GAP, projects are divided into different annotation types. E.g. a project consisting of images can be divided into lane annotation and object detection. Within Kognic this is represented as a Request. A Request can be viewed as a drawing tool - we divide big and complex projects into several independent annotation types. This makes it possible to: Reduce the mental strain on annotatorsHigher bandwidth - more annotators can work on the same data in parallelBuild simple user interfaces All of these contribute to a high level of quality while also reducing the total time needed for producing an annotation. "},{"title":"Guideline​","type":1,"pageTitle":"Key Concepts","url":"/docs/#guideline","content":"In order for us to produce annotations we need to know what and how to annotate. This type of information is found in something that's called a Guideline. A guideline contains information on what to mark (e.g. vehicles and pedestrians) as well as how (e.g. bounding box). A guideline also includes information about how to interpret the data, i.e. what does it mean that a vehicle is &quot;heavily occluded&quot;? "},{"title":"Task Definition​","type":1,"pageTitle":"Key Concepts","url":"/docs/#task-definition","content":"Task Definition - Describes what we’re annotating. How many objects? Bounding box, semantic segmentation or lines/splines? What are the properties? Task Definitions are json documents that the Kognic Professional Services team generates from the guideline. The task definition is used by the Kognic App to construct the drawing tool, and can be viewed as the machine readable quivalent of a guideline. "},{"title":"Scene​","type":1,"pageTitle":"Key Concepts","url":"/docs/#scene","content":"Before dealing with different annotation setups, the data to be annotated needs to be uploaded to the Kognic Platform. The scene specifies how different types of data are tied together and includes resources such as images and point clouds as well as metadata and calibrations (how sensors relate to each other). We have support for annotating different types of data, e.g: One (or more) images of the same scene from different camerasImages from different cameras together with lidar point clouds One important concept related to scenes is that of the frame. A frame is a discrete moment in time, consisting of data from different sensors. Scenes can be divided into two categories; single frame and sequences(multiple frames). The latter is represented by the scene types ending with Seq. These should be used when temporal information is important for producing the annotation. "},{"title":"Scene Types​","type":1,"pageTitle":"Key Concepts","url":"/docs/#scene-types","content":"Type\tDescriptionCameras\tA single frame consisting of images from 1-9 cameras LidarsAndCameras\tA single frame consisting of 1-20 lidar point clouds and images from 1-9 cameras CamerasSeq\tSequence of frames, each frame consisting of images from 1-9 cameras LidarsAndCamerasSeq\tSequence of frames, consisting of 1-20 lidar point clouds and images from 1-9 cameras AggregatedLidarsAndCamerasSeq\tSequence of frames, consisting of 1-20 lidar point clouds and images from 1-9 cameras. Point clouds are aggregated over time, producing one point cloud during annotation. "},{"title":"Input​","type":1,"pageTitle":"Key Concepts","url":"/docs/#input","content":"Once a scene has been uploaded to the Kognic Platform, you can create annotation tasks for it. We call these annotation tasks inputs, where each input is added to a request. Separating the input from the scene in this way enables efficient reuse of the uploaded data. For instance, multiple inputs can easily be created from the same scene in the same batch. In the future, this will also be possible across batches. Note that you can specify that inputs should also be created when creating the scene by providing the project/batch to create them in. "},{"title":"Annotation​","type":1,"pageTitle":"Key Concepts","url":"/docs/#annotation","content":"Inputs are annotated in requests, producing annotations. Annotations are provided by kognic-io as json objects on the ASAM OpenLABEL format. More information on how to download these annotations along with some examples of how to work with them is available in the Downloading Annotations chapter. Apart from kognic-io, Kognic also provides a library called kognic-openlabel, which makes it easy to parse and work with the OpenLABEL json. Any conversion away from the OpenLABEL format will have to occur client-side. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/introduction","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-refinement/introduction#prerequisites","content":"Before you begin, make sure you have: Access to the refinement toolAn account with permissions to use our APIGenerated API credentials. See API CredentialsInstalled our Python 3 SDK for authentication - kognic-auth "},{"title":"No API Client Available​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-refinement/introduction#no-api-client-available","content":"At the moment we do not provide an API client for the refinement tool. Instead, we'll provide examples of how you can interact with our API. "},{"title":"Endpoints​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-refinement/introduction#endpoints","content":"You can discover the list of accessible endpoints within our swagger documentation. "},{"title":"Request Example​","type":1,"pageTitle":"Introduction","url":"/docs/dataset-refinement/introduction#request-example","content":"Here's an example using the kognic-auth library to list all datasets available to the user: import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession() try: response = client.session.get(base_url + &quot;datasets&quot;) response.raise_for_status() data = response.json() print(data) except requests.exceptions.RequestException as e: print(f&quot;Request error: {e}&quot;)  "},{"title":"Uploading predictions","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/uploading-predictions","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-refinement/uploading-predictions#introduction","content":"In this example, we'll walk you through how to upload predictions using our API into an already existing dataset. Before you begin: See Prerequisites and learn about the prediction format. "},{"title":"Steps​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-refinement/uploading-predictions#steps","content":""},{"title":"1. Get the UUID of the dataset​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-refinement/uploading-predictions#1-get-the-uuid-of-the-dataset","content":"You can either access the tool and copy the UUID following dataset/ in the URL, or utilize the datasets endpoint to get the uuid of the dataset: client.session.get(base_url + &quot;datasets&quot;)  "},{"title":"2. Get the UUID of the predictions group​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-refinement/uploading-predictions#2-get-the-uuid-of-the-predictions-group","content":"In order to upload predictions, a prediction group needs to exist. Predictions can be organized into groups for any purpose imaginable. The UUID of an existing prediction group can be found in the URL after predictions/ or by using the endpoint client.session.get(base_url + f&quot;/datasets/{datasetUuid}/predictions-groups&quot;)  "},{"title":"3. Upload predictions​","type":1,"pageTitle":"Uploading predictions","url":"/docs/dataset-refinement/uploading-predictions#3-upload-predictions","content":"For a small amount of predictions, synchronous calls might work import requests from kognic.auth.requests.auth_session import RequestsAuthSession base_url = &quot;https://dataset.app.kognic.com/v1/&quot; client = RequestsAuthSession() predictions_group_uuid = &quot;...&quot; openlabel_content = {&quot;openlabel&quot;: ...} data = { &quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content, } try: response = client.session.post( base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot;, json=data ) response.raise_for_status() response_json = response.json() print(f&quot;Created prediction with uuid {response_json['data']}&quot;) except requests.exceptions.RequestException as e: msg = e.response.text print(f&quot;Request error: {e}. {msg}&quot;)  For larger amounts of predictions, asynchronous calls are recommended. The following example uses the async client from the kognic-auth library to make 100 asynchronous calls: import asyncio from kognic.auth.httpx.async_client import HttpxAuthAsyncClient base_url = &quot;https://dataset.app.kognic.com/v1/&quot; predictions_group_uuid = &quot;...&quot; url = base_url + f&quot;predictions-groups/{predictions_group_uuid}/predictions&quot; openlabel_content = {&quot;openlabel&quot;: ...} MAX_CONNECTIONS = 10 async def upload_prediction(payload, session, sem): async with sem: response = await session.post(url, json=payload) response.raise_for_status() return response.json().get(&quot;data&quot;) async def main(n_runs: int): client = HttpxAuthAsyncClient() session = await client.session sem = asyncio.Semaphore(MAX_CONNECTIONS) tasks = [] for i in range(n_runs): payload = {&quot;sceneUuid&quot;: &quot;...&quot;, &quot;openlabelContent&quot;: openlabel_content} task = upload_prediction(payload, session, sem) tasks.append(task) responses = await asyncio.gather(*tasks) await session.aclose() print(responses) if __name__ == '__main__': asyncio.run(main(100))  Setting MAX_CONNECTIONS to something bigger than 10 might not work and is not recommended. "},{"title":"Kognic Auth","type":0,"sectionRef":"#","url":"/docs/kognic-auth","content":"","keywords":""},{"title":"Generating Credentials​","type":1,"pageTitle":"Kognic Auth","url":"/docs/kognic-auth#generating-credentials","content":"The credentials file, including the Kognic client ID and the Kognic client secret, can be generated in the Kognic web application by clicking on &quot;Api Credentials...&quot; in the user menu, followed by clicking on the &quot;Generate Credentials&quot; button.  "},{"title":"The prediction format","type":0,"sectionRef":"#","url":"/docs/dataset-refinement/prediction-format","content":"","keywords":""},{"title":"Supported prediction features​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-refinement/prediction-format#supported-prediction-features","content":"note Only one type of geometry, e.g. cuboid, per prediction is supported The current API for uploading predictions supports the following geometries: Name\tOpenLABEL field\tDescriptionCuboid\tcuboid\tCuboid in 3D Bounding box\tbbox\tBounding box in 2D Note that all geometries should be specified under frames rather than in the root of the pre-annotation. The rotation of cuboids should be the same as that in exports. 2D geometries should be expressed in pixel coordinates. See coordinate systems for more information. frame_properties is required, same as for uploading pre annotations. For non-video data, frame_properties.external_id will be resolved automatically if it is left as an empty string.frame_properties.timestamp will be ignored for non-video data and can therefore be set to 0. frame_properties.streamcan be left as an empty dict. Existence confidence can be provided by specifying an attribute called confidence. It is not required and will be set to 1.0 if it is left empty. If provided, it must be defined as a numeric value between 0.0 and 1.0. Existence confidence is set to 0.85 in the examples below. The camera_id and lidar_id in the examples below must match the id of the sensor in already existing annotations. The object_data.type will show up as the class name in the tool. "},{"title":"Prediction examples​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-refinement/prediction-format#prediction-examples","content":""},{"title":"2D bounding box with a static property​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-refinement/prediction-format#2d-bounding-box-with-a-static-property","content":"In OpenLabel, a bounding box is represented as a list of 4 values: [x, y, width, height], where x and y are the center coordinates of the bounding box. The width and height are the width and height of the bounding box. The xand y coordinates are relative to the upper left corner of the image. { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;camera_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;val&quot;: [ 1.0, 1.0, 40.0, 30.0 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-bounding-box-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;camera_id&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"3D cuboid with a static property​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-refinement/prediction-format#3d-cuboid-with-a-static-property","content":"Cuboids are represented as a list of 10 values: [x, y, z, qx, qy, qz, qw, width, length, height], where x, y, and z are the center coordinates of the cuboid. x, y, z, width, length, and height are in meters.qx, qy, qz, and qw are the quaternion values for the rotation of the cuboid. Read more about coordinate systems and quaternions here. { &quot;openlabel&quot;: { &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;&quot;, &quot;streams&quot;: {} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;num&quot;: [ { &quot;val&quot;: 0.85, &quot;name&quot;: &quot;confidence&quot; } ], &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar_id&quot; } ] }, &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;any-human-readable-cuboid-name&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;lidar_id&quot;: { &quot;type&quot;: &quot;lidar&quot; } } } }  "},{"title":"Using kognic-openlabel to validate the format​","type":1,"pageTitle":"The prediction format","url":"/docs/dataset-refinement/prediction-format#using-kognic-openlabel-to-validate-the-format","content":"See kognic-openlabel for more information. "},{"title":"Downloading Annotations","type":0,"sectionRef":"#","url":"/docs/kognic-io/annotations","content":"","keywords":""},{"title":"v1.1.x​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#v11x","content":"This section describes how you can fetch annotations on the OpenLABEL format. These annotations are automatically available as soon as they are finished and can be downloaded either for an entire project/batch or individually via the methods listed below. All methods return either a single Annotation object or a generator yielding Annotation objects, which contains indentifiers as well as a dictionary containing the OpenLABEL json: class Annotation(BaseSerializer): input_uuid: str annotation_type: str created: datetime content: Optional[Dict]  As the Annotation model shows, an annotation is unique for each input and annotation-type. The OpenLABEL json can be used as it is or be converted into a pythonic object using the kognic-openlabel library, describedhere. "},{"title":"Get Single Annotation​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#get-single-annotation","content":"Using input and annotation type​ examples/get_annotation.py loading... See full example on GitHub This method returns a single Annotation object, containing the OpenLABEL json, using an input uuid and an annotation type. "},{"title":"Get Annotations for a Project or Batch​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#get-annotations-for-a-project-or-batch","content":"examples/get_project_annotations.py loading... See full example on GitHub This example fetches annotations for an entire project or batch. The run() method returns a generator which will yield Annotation objects for all finished annotations , for the given project, batch and annotation_type, and in the end prints all of these annotations. "},{"title":"Common use cases​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#common-use-cases","content":""},{"title":"Download and convert a single annotation​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#download-and-convert-a-single-annotation","content":"This example shows a common workflow where an annotation is fetched, parsed into an OpenLabelAnnotation and then converted into a custom annotation format. from kognic.io.client import KognicIOClient from kognic.openlabel.models import OpenLabelAnnotation from pydantic import BaseModel class CustomAnnotationFormat(BaseModel): ... @staticmethod def from_openlabel(openlabel_annotation: OpenLabelAnnotation): pass client = KognicIOClient() annotation = client.annotation.get_annotation( input_uuid='&lt;input-uuid-identifier&gt;', annotation_type='&lt;annotation-type&gt;' ) openlabel_annotation = OpenLabelAnnotation.parse_obj(annotation.content) # Create pydantic object converted_annotation = CustomAnnotationFormat.from_openlabel(openlabel_annotation=openlabel_annotation) # Convert annotation converted_dict = converted_annotation.dict(exclude_none=True) # Serialize to dict (or json)  "},{"title":"Download and save annotations to a zip file​","type":1,"pageTitle":"Downloading Annotations","url":"/docs/kognic-io/annotations#download-and-save-annotations-to-a-zip-file","content":"In this example, all annotations are fetch for a project batch and then converted saved into a zip file. Note that the save_file must have the extension .zip. import io, json, zipfile import kognic.io.client as KognicIOClient client = KognicIOClient() zip_buffer = io.BytesIO() with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file: for annotation in client.annotation.get_project_annotations( project=&quot;Project-identifier&quot;, batch=&quot;Batch-identifier&quot;, annotation_type=&quot;Annotation-Type-identifier&quot; ): encoded_annotation = io.BytesIO(json.dumps(annotation.content, indent=4).encode()) zip_file.writestr(f&quot;{annotation.input_uuid}.json&quot;, encoded_annotation.getvalue()) with open('path/to/annotations.zip', 'wb') as f: f.write(zip_buffer.getvalue())  "},{"title":"Annotation Types","type":0,"sectionRef":"#","url":"/docs/kognic-io/annotation_types","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#examples","content":"For the following examples we will be creating cameras_sequence inputs, however the procedure would be identical for any other input type. We will also assume that the project example_project_id is configured with the Annotation Types:static_objects, and dynamic_objects, and that they are also available in the batchexample_batch_id. "},{"title":"Get Annotation Types for Project​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#get-annotation-types-for-project","content":"from kognic.io.client import KognicIOClient client = KognicIOClient() project_annotation_types = client.project.get_annotation_types(project=&quot;example_project_id&quot;)  This will return a list of all Annotation Types available in the project. "},{"title":"Get Annotation Types for a specified Project Batch​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#get-annotation-types-for-a-specified-project-batch","content":"from kognic.io.client import KognicIOClient client = KognicIOClient() batch_annotation_types = client.project.get_annotation_types(project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;)  This will return a list of all Annotation Types available in the specified batch. Note that this list does not need to contain all Annotation Types in the project. "},{"title":"Create inputs for specific Annotation Types​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#create-inputs-for-specific-annotation-types","content":"from kognic.io.client import KognicIOClient from kognic.io.model.scene.cameras_sequence import CamerasSequence client = KognicIOClient() camera_input = CamerasSequence(external_id=..., frames=...) client.cameras_sequence.create(camera_input, project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;, annotation_types=[&quot;static_objects&quot;, &quot;dynamic_objects&quot;])  The above example will create a new input which will be annotated for the annotation types specified. If one or more of the specified annotation types would not be available in the specified batch the validation in the API would fail. Specifying batch is optional In these examples we have specified which batch the inputs should be created for, but this is optional. If no batch is specified the inputs will be created in the latest batch with status open. "},{"title":"Create inputs for all Annotation Types in batch​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#create-inputs-for-all-annotation-types-in-batch","content":"from kognic.io.client import KognicIOClient from kognic.io.model.scene.cameras_sequence import CamerasSequence client = KognicIOClient() camera_input = CamerasSequence(external_id=..., frames=...) client.cameras_sequence.create(camera_input, project=&quot;example_project_id&quot;, batch=&quot;example_batch_id&quot;)  The above example will create a new input which will be annotated for all Annotation Types available in the batch example_batch_id. However, this way it is not explicit what Annotation Types that the inputs will be annotated with, and you would not get an error if e.g. static_objects was missing from the specified batch. Always specify Annotation Types In order to get the best possible validation it is recommended that you always specify annotation types when you create inputs. "},{"title":"Add/remove annotation types for an input​","type":1,"pageTitle":"Annotation Types","url":"/docs/kognic-io/annotation_types#addremove-annotation-types-for-an-input","content":"Adding an annotation type to an input means that an annotation will be produced for that input with the specified annotation type. In the same way, removing annotation types from an input means that annotations will not be produced for that input with the specified annotation types. In the case when multiple annotation types are annotated in the same task, it is enough to specify one annotation type when adding but all annotation types must be specified when removing. Note that it is currently not possible to add an annotation type that has already been removed from an input. from kognic.io.client import KognicIOClient client = KognicIOClient() input_uuid = 'cca60a67-cb68-4645-8bae-00c6e6415555' # Add an annotation type to an input client.input.add_annotation_type(input_uuid=input_uuid, annotation_type=&quot;annotation-type&quot;) # Remove annotation types from an input annotation_types = [&quot;annotation-type-1&quot;, &quot;annotation-type-2&quot;, ...] client.input.remove_annotation_types(input_uuid=input_uuid, annotation_type=annotation_types)  "},{"title":"Calibrations","type":0,"sectionRef":"#","url":"/docs/kognic-io/calibrations","content":"","keywords":""},{"title":"Lidar​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#lidar","content":"Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z field_of_view (optional)\tA LidarFieldOfView object\tstart_angle_deg, stop_angle_deg and optionally depth A LIDAR calibration is represented as a LidarCalibration object and consists of a position expressed with three coordinates and a rotation in the form of a Quaternion. Optionally, the sensor's field of view may be specified by providing an object that has a sweep start angle and sweep stop angle. The field of view may also optionally include the depth to which the field extends. See the code example below for creating a base LidarCalibration object. examples/calibration/create_lidar_calibration.py loading... See full example on GitHub "},{"title":"Camera​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#camera","content":"The Camera calibration format is based on OpenCVs format and this paper. The different camera types supported are: PINHOLE, FISHEYE, KANNALA, PRINCIPALPOINTDIST, and FUSEDCYLINDRICAL.  "},{"title":"Common​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#common","content":"All camera calibrations have the following attributes Key\tValue\tParametersrotation_quaternion\tA RotationQuaternion object\tw, x, y, z position\tA Position object\tx, y, z camera_matrix\tA CameraMatrix object\tfx, fy, cx, cy image_width\tInteger\tNA image_height\tInteger\tNA field_of_view\tFloat\tNA "},{"title":"Pinhole​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#pinhole","content":"The PINHOLE camera model expands the common model with: Key\tValue\tParametersdistortion_coefficients\tA DistortionCoefficients object\tk1, k2, p1, p2, k3 examples/calibration/create_pinhole_calibration.py loading... See full example on GitHub "},{"title":"Fisheye​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#fisheye","content":"The Fisheye camera model expands the PINHOLE model with the following Key\tValue\tParametersxi\tFloat\tNA examples/calibration/create_fisheye_calibration.py loading... See full example on GitHub "},{"title":"Kannala​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#kannala","content":"The KANNALA camera model changes and expands the PINHOLE with the following Key\tValue\tParametersdistortion_coefficients\tA KannalaDistortionCoefficients object. The distortion parameters k3, k4, if available, can be assigned to p1 and p2 respectively. That is p1=k3 and p2=k4.\tk1, k2, p1, p2 undistortion_coefficients\tA UndistortionCoefficients object.\tl1, l2, l3, l4 examples/calibration/create_kannala_calibration.py loading... See full example on GitHub "},{"title":"Principal point distortion​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#principal-point-distortion","content":"The principal point distortion model consists of the common attributes plus Key\tValue\tParametersprincipal_point_distortion_coefficients\tA PrincipalPointDistortionCoefficients object\tk1, k2 lens_projection_coefficients (optional. Default to values for model SF806)\tA LensProjectionCoefficients object\tc1, c2,c3, c4,c5, c6 distortion_center\tA DistortionCenter object\tx, y principal_point\tA PrincipalPoint object\tx, y examples/calibration/create_principal_point_distortion_calibration.py loading... See full example on GitHub "},{"title":"Fused cylindrical​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#fused-cylindrical","content":"The fused cylindrical model consists of the common attributes plus Key\tValue\tParameterscut_angles_degree\tA CutAngles object. Note these angles should be expressed in degrees.\tupper, lower vertical_fov_degree (optional. Default 72.5 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA horizontal_fov_degree (optional. Default 93 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA max_altitude_angle_degree (optional. Default 90 degrees)\tFloat. Note this angle should be expressed in degrees.\tNA examples/calibration/create_fused_cylindrical_calibration.py loading... See full example on GitHub "},{"title":"Cylindrical​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#cylindrical","content":"The cylindrical model consists only of the common attributes. There are no extra attributes to set for this model. examples/calibration/create_cylindrical_calibration.py loading... See full example on GitHub "},{"title":"Principal point fisheye​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#principal-point-fisheye","content":"The principal point fisheye model consists of the common attributes plus Key\tValue\tParametersprincipal_point_fisheye_coefficients\tA PrincipalPointFisheyeCoefficients object\talpha_l, alpha_r, beta_u, beta_l examples/calibration/create_principal_point_fisheye_calibration.py loading... See full example on GitHub "},{"title":"Example: Creating a calibration​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#example-creating-a-calibration","content":"The following example code shows how you can create a unity (i.e. we assume that all sensors are placed at origin and have no rotation) calibration for a LIDAR sensor and several camera sensors of type PINHOLE. examples/calibration/calibration.py loading... See full example on GitHub "},{"title":"Listing existing calibrations​","type":1,"pageTitle":"Calibrations","url":"/docs/kognic-io/calibrations#listing-existing-calibrations","content":"As a final step we can fetch the calibration via the external id. This can either be done via the client, or via the CLI kognicutil tool. client.calibration.get_calibration(external_id=&quot;Collection 2020-06-16&quot;)  $ kognicutil calibration --external-id &quot;Collection 2020-06-16&quot;  "},{"title":"Errors","type":0,"sectionRef":"#","url":"/docs/kognic-io/error_handling","content":"Errors When the client sends a http request to the API and waits until it receives a response. If the response code is 2xx (the status code for a successful call) the client converts the received message into a python object which can be viewed or used. However if the API responds with an error code (4xx or 5xx) the python client will raise an error. It's up to the user to decide if and how they want to handle this error.","keywords":""},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/kognic-io/FAQ","content":"","keywords":""},{"title":"Receiving requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: ... when trying to create inputs​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#receiving-requestsexceptionshttperror-403-client-error-forbidden-for-url--when-trying-to-create-inputs","content":"This implies that the authenticated user does not have access to the endpoint being called. Make sure you're authenticating correctly. If a Kognic user, make sure client_organization_id is specified on the KognicIOClient. "},{"title":"How do I know that my input was created successfully?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-do-i-know-that-my-input-was-created-successfully","content":"Whenever a .create(...) call for an input has been successfully made it's (asynchronously) submitted for pre-processing in the Kognic platform. The input is available only once the pre-processing has been successfully executed. However, pre-processing can also fail, for example if the pointcloud or image files are poorly formatted or corrupt. The easiest way to check the status of an input is the input status field present on inputs returned by the methods get_inputs(...) and get_inputs_by_uuids(...). The input is successfully created and available in the platform once the status is set to created. note Since pre-processing is an asynchronous process it might take a while before the input changes status from processing to either created or failed. # Example code of how to check if an input has been successfully created resp = client.cameras.create(...) uuid = resp.uuid [i] = client.input.get_inputs_by_uuids(input_uuids=[uuid]) # Successfully created and available once status is `created` print(f'Input {uuid} status:', i.status)  "},{"title":"How can I view my input?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#how-can-i-view-my-input","content":"Successfully created inputs can be viewed in the Kognic platform via their view-link. The view-link can be accessed via the view_link field present on inputs returned by the methods get_inputs(...) and get_inputs_by_uuids(...). # Example code of how to access view-links for all inputs in a project inputs = client.input.get_inputs(project=&quot;project-identifier&quot;) for i in inputs: print(f&quot;Input {i.external_id} view-link: {i.view_link}&quot;)  "},{"title":"Why are the cuboids rotated by 90 degrees?​","type":1,"pageTitle":"FAQ","url":"/docs/kognic-io/FAQ#why-are-the-cuboids-rotated-by-90-degrees","content":"The coordinate system is defined by the uploaded data, but the rotation is defined by Kognic. This is somewhat different (90-degree rotation) compared to the ISO 8855 standard. See Rotation of Cuboids for more about this and how you can convert to ISO 8855. "},{"title":"Input Feature Flags","type":0,"sectionRef":"#","url":"/docs/kognic-io/feature_flags","content":"","keywords":""},{"title":"Supported Features​","type":1,"pageTitle":"Input Feature Flags","url":"/docs/kognic-io/feature_flags#supported-features","content":"Individual flags are found on the enums found within FeatureFlags. "},{"title":"PointCloudFeatures​","type":1,"pageTitle":"Input Feature Flags","url":"/docs/kognic-io/feature_flags#pointcloudfeatures","content":"Flag\tDefault state\tDescriptionMOTION_COMPENSATION\tEnabled\tCauses motion compensation of point clouds using IMU data. "},{"title":"Coordinate Systems","type":0,"sectionRef":"#","url":"/docs/kognic-io/coordinate_systems","content":"","keywords":""},{"title":"The reference coordinate system and calibrations​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-reference-coordinate-system-and-calibrations","content":"Each sensor has its own coordinate system in 3D space that depends on its location and orientation on the ego vehicle. Being able to transform measurements between these sensor coordinate systems is important. To do this, a reference coordinate system is defined which works as a middle man between the sensor coordinate systems. The reference coordinate system can be chosen arbitrarily relative to the ego vehicle. By defining a calibration function CiC_iCi​ for sensor iiiwe can map a point xi⃗\\vec{x_i}xi​​ to the reference coordinate system in the following way xR⃗=Ci(xi⃗)\\vec{x_R} = C_i(\\vec{x_i})xR​​=Ci​(xi​​) In the same way we can map points from all other sensors to the reference coordinate system. Subsequently, we can also map a point from coordinate system iii to coordinate system jjj by applying the inverse of the calibration xj⃗=Cj−1(Ci(xi⃗))\\vec{x_j} = C_j^{-1}(C_i(\\vec{x_i}))xj​​=Cj−1​(Ci​(xi​​)) "},{"title":"The world coordinate system and ego motion data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#the-world-coordinate-system-and-ego-motion-data","content":"With this, we can now express points in coordinate systems local to the ego vehicle. This is great, but sometimes it is also valuable to express points recorded at different times in the same coordinate system. We call this the world coordinate system since it is static in time. We can transform a point to the world coordinate system using ego motion data, which describes the location and orientation of the ego vehicle at any given time. With the ego motion data we can transform a point xt⃗\\vec{x_t}xt​​ to the world coordinate system with xw⃗=Et(xt⃗)\\vec{x_w} = E_t(\\vec{x_t})xw​​=Et​(xt​​) Subsequently, we can also transform a point recorded at time ttt to the coordinate system at time t′t't′ by applying the inverse of the ego transformation function xt′⃗=Et′−1(Et(xt⃗))\\vec{x_{t'}} = E_{t'}^{-1}(E_t(\\vec{x_t}))xt′​​=Et′−1​(Et​(xt​​)) This can be used to compensate each lidar point for the motion of the ego vehicle, a process also known asmotion compensation. It is highly recommended to motion compensate point clouds since lidar points are recorded at different instants in time. This can be done by providing high frequency ego motion data (IMU data) when creating a scene. "},{"title":"Single-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#single-lidar-case","content":"The image below displays how the different sensors relate to each other in 3D space in the single-lidar case. Note that the ego motion data should be expressed in the lidar coordinate system.  "},{"title":"Multi-lidar case​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#multi-lidar-case","content":"In the multi-lidar case (see image below) there are multiple point clouds, each in their own lidar coordinate system. These are merged into one point cloud in the reference coordinate system during scene creation since it's more efficient to annotate one point cloud rather than several. If IMU data is available, we can also compensate for the ego motion so that each point is transformed to the reference coordinate system at the frame timestamp. This is done by applying xw⃗=Et(Ci(xi,t⃗))xt′⃗=Et′−1(xw⃗)\\vec{x_w} = E_t(C_i(\\vec{x_{i,t}})) \\\\ \\vec{x_{t'}} = E_{t'}^{-1}(\\vec{x_{w}})xw​​=Et​(Ci​(xi,t​​))xt′​​=Et′−1​(xw​​) where xi,t⃗\\vec{x_{i,t}}xi,t​​ is the point expressed in the lidar coordinate system of lidar iii at time ttt and xt′⃗\\vec{x_{t'}}xt′​​is the point expressed in the reference coordinate system at the frame time t′t't′. It is recommended to provide IMU data so that motion compensation can be utilized. Since the merged point cloud is expressed in the reference coordinate system we also expect any ego motion data to be expressed in the reference coordinate system.  "},{"title":"Different coordinate systems for different kinds of data​","type":1,"pageTitle":"Coordinate Systems","url":"/docs/kognic-io/coordinate_systems#different-coordinate-systems-for-different-kinds-of-data","content":"Different kinds of data are expressed in different coordinate systems depending on whether it's single-lidar or multi-lidar. This is summarized in the table below where we can see that ego motion data should be expressed in the lidar coordinate system in the single-lidar case but in the reference coordinate system in the multi-lidar case for example. Type of data\tSingle-lidar\tMulti-lidarEgo poses &amp; IMU data\tLidar\tReference OpenLABEL export 3D geometries\tLidar\tReference OpenLABEL export 2D geometries\tPixel\tPixel Pre-annotations 3D geometries\tLidar\tReference Pre-annotations 2D geometries\tPixel\tPixel "},{"title":"Aggregated Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/aggregated_lidars_and_cameras_seq","content":"Aggregated Lidars and Cameras Sequence note This feature is new in version 1.1.5 An AggregatedLidarsAndCamerasSeq scene consists of a sequence of camera images and lidar point clouds, where each frame consists on 1-9 camera images as well as 1-20 point clouds. What differentiates AggregatedLidarsAndCamerasSeqfrom LidarsAndCamerasSeq is that point clouds are aggregated over time during annotation which results in one big point cloud in the coordinate system of the first frame. Therefore, ego motion data is mandatory for this type of scene. For more documentation on what each field corresponds to in the AggregatedLidarsAndCamerasSeq object please check the section related to Input Overview. Refer to Coordinate Systems for more information about what coordinate systems to use. examples/agg_lidars_and_cameras_seq.py loading... See full example on GitHub Use dryrun to validate input Setting dryrun parameter to true in the method call, will validate the input using the API but not create any inputs. reuse calibration Note that you can, and should, reuse the same calibration for multiple inputs if possible.","keywords":""},{"title":"Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/cameras","content":"Cameras A Cameras input consists of a single frame of camera images, where the frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the Cameras object please check the section related to Input Overview. examples/cameras.py loading... See full example on GitHub Use dryrun to validate input Setting dryrun parameter to true in the method call, will validate the input using the API but not create any inputs.","keywords":""},{"title":"Lidars and Cameras","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/lidars_and_cameras","content":"Lidars and Cameras A LidarsAndCameras input consists of a single frame which contains 1-9 cameras images as well as 1-20 point clouds. For more documentation on what each field corresponds to in the LidarsAndCameras object please check the section related to Input Overview. examples/lidars_and_cameras.py loading... See full example on GitHub Use dryrun to validate input Setting dryrun parameter to true in the method call, will validate the input using the API but not create any inputs.","keywords":""},{"title":"Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/cameras_seq","content":"Cameras Sequence A CamerasSeq input consists of a sequence of camera images, where each frame can contain between 1-9 images from different sensors. For more documentation on what each field corresponds to in the CamerasSeq object please check the section related to Input Overview. examples/cameras_seq_images.py loading... See full example on GitHub Use dryrun to validate input Setting dryrun parameter to true in the method call, will validate the input using the API but not create any inputs.","keywords":""},{"title":"Lidars and Cameras Sequence","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/lidars_and_cameras_seq","content":"","keywords":""},{"title":"Providing Ego Vehicle Motion Information​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/inputs/lidars_and_cameras_seq#providing-ego-vehicle-motion-information","content":"Ego vehicle motion (i.e. the position and rotation of the ego vehicle) is optional information that can be provided when creating LidarsAndCamerasSeq inputs. This information can enable a massive reduction in the time it takes to annotate static objects. Ego vehicle motion information is provided by passing a EgoVehicleMotion object to each Framein the input. examples/lidars_and_cameras_seq_full.py loading... See full example on GitHub Coordinate Systems Note that both position and rotation for ego vehicle pose are with respect to the local coordinate system. "},{"title":"Shutter timings​","type":1,"pageTitle":"Lidars and Cameras Sequence","url":"/docs/kognic-io/inputs/lidars_and_cameras_seq#shutter-timings","content":"Shutter timings are optional metadata that may be provided when creating an Image within a Frame. Timings are two values: shutter start and end timestamp in nanoseconds since unix epoch and are specified for each image in each frame. examples/lidars_and_cameras_seq_with_imu_and_shutter_times.py loading... See full example on GitHub "},{"title":"Motion Compensation","type":0,"sectionRef":"#","url":"/docs/kognic-io/inputs/lidars_with_imu_data","content":"","keywords":""},{"title":"Enable/disable motion compensation​","type":1,"pageTitle":"Motion Compensation","url":"/docs/kognic-io/inputs/lidars_with_imu_data#enabledisable-motion-compensation","content":"By default motion compensation is performed for inputs with LIDAR pointclouds when IMU data is provided. Whether motion compensation is enabled is controlled by an input feature flag, the default is enabled. To disable motion compensation you must provide a different set of flags from the default, and not include the motion compensation flag. Calling the create method for an input, and not specifying feature flags at all is equivalent to using the defaults, and motion compensation will be performed. It may be desirable to disable motion compensation in cases where pointclouds are already motion compensated outside of the Kognic platform. "},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/kognic-io/overview","content":"","keywords":""},{"title":"Different types of inputs​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#different-types-of-inputs","content":"An Input represents a grouping of sensor data (e.g. camera images, lidar pointclouds) that should be annotated together. Any information necessary to express the relationship between the sensors and their captured data is also present, be it camera resolution, sensor name or the frequency at which the data was recorded at. There are different input types depending on what kind of sensor(s) are used to represent the contents of the input. For example, if we want to create an input only consisting of data from camera sensors then we would use the input type Cameras. Similarly, if we want to create an input consisting of lidar sensors and camera sensors then we would use the input type LidarsAndCameras. Additionally, inputs can either be sequential or non-sequential. "},{"title":"Sequential vs non-sequential​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sequential-vs-non-sequential","content":"Sequential inputs represent a sequence of sensor data, whereas non-sequential inputs only contain a single snapshot of sensor data. The sequential relationship is expressed via a sequence of Frames, where each Frame object contains information related to what kind of sensor data constitues the frame (e.g. which image and/or point cloud is a part of the Frame) as well as a relative timestamp that captures where in time (relative to the other frames) the Frame is located. Non-sequential inputs only express a single snapshot of sensor data. As such, these kinds of inputs only contain a single Frame object and do not require any relative timestamp information. Sequential input types are easily identified by the suffix Seq present in their name. The following input types are currently supported CamerasLidarsAndCamerasCamerasSeqLidarsAndCamerasSeqAggregatedLidarsAndCamerasSeq "},{"title":"Input Fields​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#input-fields","content":"All non-sequential inputs have the following structure class Input(BaseModel): external_id: str frame: Frame sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)  Sequential inputs are similarly represented, except that they instead contain a list of Frames class InputSeq(BaseModel): external_id: str frames: List[Frame] sensor_specification: SensorSpecification calibration_id: Optional[str] # Required if using lidar sensors metadata: Mapping[str, Union[int, float, str, bool]] = field(default_factory=dict)  The fields contain all the information required to create the input. "},{"title":"External Id​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#external-id","content":"Whenever an input is uploaded it automatically gets a UUID, this is used as the primary identifier by Kognic and by all of our internal systems. However, in order to make communication around specific inputs easier we also allow for clients to include any kind of identifier to the input via the external id. "},{"title":"Sensor Specification​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#sensor-specification","content":"The sensor specification contains information related to the different camera and/or lidar sensors used for capturing the data present on the input. The additional fields are optional and relate to specifying the order of the camera sensors and human readable variants of the sensor name (e.g. &quot;Front Camera&quot; instead of &quot;FC&quot;). class SensorSpecification(BaseModel): sensor_to_pretty_name: Optional[Dict[str, str]] = None sensor_order: Optional[List[str]] = None  As an example, let's say we have three different camera sensors R, F and L. The R sensor is mounted on the right side of the ego vehicle, the F sensor at the front and the L sensor to the left. Creating a sensor specification for this scenario would correspond to sensor_spec = SensorSpecification( sensor_to_pretty_name={ &quot;R&quot;: &quot;Right Camera&quot;, &quot;F&quot;: &quot;Front Camera&quot;, &quot;L&quot;: &quot;Left Camera&quot; }, sensor_order=[&quot;L&quot;, &quot;F&quot;, &quot;R&quot;] )  The specified sensor_order will cause the different camera sensors to be presented in a clockwise manner in the annotation tool (Left -&gt; Front -&gt; Right), while the sensor_to_pretty_name parameter will result in the annotation tool showing the human readable version of all the sensor names when changing sensor. "},{"title":"Calibration Id​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#calibration-id","content":"Any input consisting of lidar and camera sensors requires a calibration. The calibration captures the spatial relationship (position and rotation) between the different sensors, as well as different camera specific parameters. This information is used by the annotation tool to highlight regions in the point cloud visible in the selected camera sensors as well as for projecting information from the pointcloud onto the different camera sensors (points, cuboids etc). Detailed documentation on how to create calibrations via the API is present in the Calibration section. When including calibration id make sure that all of the sensors present on the input are also present in the calibration as well. If this is not the case the input will not be created and a validation error will be returned by the API. Inputs without a lidar sensor do not require a calibration. "},{"title":"Metadata​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#metadata","content":"Metadata can be added to inputs via the metadata field. It consists of flat key-value pairs, which means that nested data structures are not allowed. Metadata can be used to include additional information about an input. Nothing specified in the metadata can be seen by the annotators, but there are some reserved keywords that can alter the behaviour of the annotation tool. Reserved keywords can be found on the MetaData object in the python client. "},{"title":"Frame (non-sequential inputs)​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#frame-non-sequential-inputs","content":"The Frame object specifies the binary data to be annotated (.jpg, .png, .las etc) as well as which sensor the data originated from. The Frame object is different for each input type since they all support different kinds of sensors, even though the overall structure is the same. As an example, let's say we want to create an input consiting of images from three different camera sensors R, F and L. The corresponding binary data is present in the files img_cam_R.jpg, img_cam_F.jpg and img_cam_F.jpg. This would correspond to creating a Cameras input. cameras_input = Cameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ] ) )  Similarly, if we also had an associated lidar pointcloud from the sensor VDL-64 and a corresponding binary file scan_vdl_64.las we would instead express this as a LidarsAndCameras input instead. lidars_and_cameras = LidarsAndCameras( ..., frame=Frame( images=[ Image(&quot;img_cam_R.jpg&quot;, sensor_name=&quot;R&quot;), Image(&quot;img_cam_F.jpg&quot;, sensor_name=&quot;F&quot;), Image(&quot;img_cam_L.jpg&quot;, sensor_name=&quot;L&quot;), ], point_clouds=[ PointCloud(&quot;scan_vdl_64.las&quot;, sensor_name=&quot;VDL-64&quot;) ] ) )  "},{"title":"Frames (sequential inputs)​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#frames-sequential-inputs","content":"Sequential inputs deal with a list of Frame objects instead of a single Frame object. In addition, Frame objects associated with sequential inputs have three additional parameters not present in their non-sequential Frame counterparts: frame_id, relative_timestamp and metadata. The sequential relationship is expressed via the ordering of the Frame objects in the frames list frame_1 = Frame(...) frame_2 = Frame(...) frame_3 = Frame(...) frames = [frame_1, frame_2, frame_3]  This representation captures that frame_1 comes first, then frame_2 and frame_3, but it does not express how much time has passed between the different frames. This information is encoded via the relative_timestamp parameter present on each Frame object. The relative timestamp is expressed in milliseconds and describes the relative time between the Frame and the start of the input. For example, let's say that the sensor data is collected and aggregated at 2Hz. That would then be expressed as frame_1 = Frame(..., relative_timestamp=0) frame_2 = Frame(..., relative_timestamp=500) frame_3 = Frame(..., relative_timestamp=1000) frames = [frame_1, frame_2, frame_3]  The frame_id is expressed as a string and is used to produce a unique identifier for each frame in the list of frames. The frame_id is used as a top-level key in the produced annotations, indicating which parts of the complete annotation belong to this specific frame. A common use case is to use uuids for each frame_id, or a combination of external_id and frame_index. For example, if the external_id of the input is shanghai_20200101 then the frame_id could be encoded asshanghai_20200101:0 for the first frame, shanghai_20200101:1 for the second frame and so on. Similarly to the metadata capability available on an input-level, it's also possible to provide metadata on a frame level as well. It behaves the same way, i.e. consists of flat key-value pairs and is not exposed to annotators during the production of annotators. As an example, let's say we want to create an input of type CamerasSeq consisting of 2 frames, each with camera data from two different sensors R and L. If we have individual images for each frame and sensor, this would correspond to the following list of frames frames = [ Frame( frame_id=&quot;1&quot;, relative_timestamp=0, images=[ Image(&quot;img_L_1.jpg&quot;, sensor_name='L'), Image(&quot;img_R_1.jpg&quot;, sensor_name='R') ]), Frame( frame_id=&quot;2&quot;, relative_timestamp=500, images=[ Image(&quot;img_L_2.jpg&quot;, sensor_name='L'), Image(&quot;img_R_2.jpg&quot;, sensor_name='R') ]) ] cameras_sequence = CamerasSequence( ..., frames=frames, )  "},{"title":"Image & Pointcloud Resources​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#image--pointcloud-resources","content":"Every file containing sensor data (image or pointcloud files) is represented as a Resource, withImage and PointCloud being the concrete subclasses. class Resource(ABC, BaseSerializer): filename: str resource_id: Optional[str] = None sensor_name: str file_data: Optional[FileData] = Field(default=None, exclude=True)  Resources ultimately describe how to obtain some binary or textual sensor data, which can be done in different ways: Indirectly: by refering to a local filename that contains the dataDirectly: provide some bytes-like object at creation timeLazily: provide a callback function which can provide the bytes later in the process Resources must always be given a filename. For alternative 1 this must point to the local file to upload. For alternatives 2 &amp; 3 the filename is treated more as an identifier; it is used to name the uploaded file but does not have to correspond to anything in the filesystem. Resources also always have a sensor_name which identifies the sensor they were captured from. In sequential inputs, each Frame will have a Resource for each sensor. Resources take their actual data (bytes) from bytes, a BinaryIO or an IOBase-compatible object. These are referred to with the type alias UploadableData = Union[bytes, BinaryIO, IOBase]. For alternatives 2 &amp; 3 listed above, a FileData object is attached to the Image or PointCloud to capture the source of data. It is created with either data: UploadableData or a callback: Callable[[str], UploadableData], as well as a format which identifies the type of data contained in the bytes. info Previous API client releases advertised support for ingesting files from external URIs, such as gs://bucket/path/file. Please contact Kognic if you believe you require this functionality going forward. "},{"title":"Local File​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#local-file","content":"Set filename to the path of the local file and do not provide data via other means (directly or callback). The content is uploaded using a content type inferred from the filename suffix. Image(filename=&quot;/Users/johndoe/images/img_FC.png&quot;, sensor_name=&quot;FC&quot;)  "},{"title":"Data in Memory​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-in-memory","content":"In addition to filename, provide a FileData object via the file_data attribute, which in turn has an UploadableData as its own data attribute. This example uses raw bytes: png_blob = FileData(data=b'some PNG bytes', format=FileData.Format.PNG) Image(filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=png_blob)  "},{"title":"Data from Callback​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#data-from-callback","content":"In addition to filename, provide a FileData object via the file_data attribute, with a callback function that produces an UploadableData, e.g. png_from_callback = FileData(callback=get_png, format=FileData.Format.PNG) Image(filename=&quot;FC-frame15&quot;, sensor_name=&quot;FC&quot;, file_data=png_from_callback)  The callback function (get_png) is a unary function with the following signature. def get_png(file: str) -&gt; UploadableData: pass  The callback function is invoked with the Resource.filename as its argument when it is time to upload that single file. If the callback requires extra arguments then we recommend creating a closure over the additional arguments like this: def get_callback(arg1, arg2, **kwargs): def callback(filename) -&gt; bytes: # ... use arg1, arg2, filename and kwargs return callback FileData(callback=get_callback(&quot;foo&quot;, &quot;bar&quot;, extra1=&quot;baz&quot;, extra2=&quot;qux&quot;), format=FileData.Format.JPG)  "},{"title":"IMU Data​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#imu-data","content":"Intertial Measurement Unit (IMU) data may be provided for inputs containing LIDAR pointclouds. This can be used to perform motion compensation in multi-lidar setups, and by default if any IMU data is provided this will be done. Motion compensation may be disabled via an input feature flag, for cases where motion compensation has already been performed prior to upload. Refer to Motion Compensation for Multi-Lidar Setups. "},{"title":"Input Feature Flags​","type":1,"pageTitle":"Overview","url":"/docs/kognic-io/overview#input-feature-flags","content":"Control over optional parts of the input creation process is possible via FeatureFlags that are passed when invoking the create operation on the input. Refer to the feature flags documentation for details. "},{"title":"Pre-annotations","type":0,"sectionRef":"#","url":"/docs/kognic-io/pre_annotations","content":"","keywords":""},{"title":"Creating pre-annotations using the kognic-io client​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#creating-pre-annotations-using-the-kognic-io-client","content":"There are 3 steps that are needed in order to create pre-annotations in the Kognic platform. Create a scene by uploading all the needed dataUpload an OpenLabel annotation as a pre-annotationCreate an input from the scene Note that these steps can be performed in one call with the create_inputs function, see Creating Multiple Inputs With One Call "},{"title":"1. Creating a scene​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#1-creating-a-scene","content":"note The scene is a subset of the input, specifically referring to the data, such as images or point clouds. An input is what is created when this data is ready to be annotated. The interface for creating just a scene, without an input, is the same as we are familiar with. The exception is that by not providing a project or a batch in the function call, the scene will be &quot;dangling&quot; until deemed ready for annotation examples/lidars_and_cameras_seq_with_pre_annotations.py loading... See full example on GitHub "},{"title":"2. Uploading an OpenLabel annotation​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#2-uploading-an-openlabel-annotation","content":"The pre-annotation can be uploaded to the Kognic platform once the scene has been created successfully. Load your OpenLabel annotation according to the documentation in kognic-openlabel and upload it to the Kognic platform as such: client.pre_annotation.create( scene_uuid=scene_response.input_uuid, # from step 1 pre_annotation=OpenLabelAnnotation(...), dryrun=dryrun )  "},{"title":"3. Create the input​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3-create-the-input","content":"When the scene and pre-annotation have been successfully created, the input can be created. This will add it to the latest open batch in a project, or the specific batch that's specified, and be ready for annotation with the pre-annotation present. client.lidars_and_cameras_sequence.create_from_scene( scene_uuid=scene_response.input_uuid, # from step 1 annotation_types=annotation_types, project=project, dryrun=dryrun )  "},{"title":"OpenLabel support​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#openlabel-support","content":"Pre-annotations use the OpenLabel format/schema but not all OpenLabel features are supported in pre-annotations. "},{"title":"Unsupported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#unsupported-pre-annotation-features","content":"These features or combinations of features are not currently supported, or only have partial support. Static geometries: not supported These are bounding boxes, cuboids, etc. declared in the OpenLabel under objects.*.objectData Geometry-specific attributes: not supported on 3D geometry These are attributes declared in the OpenLabel on a single geometric shape, in other words an attribute that only applies to the object as seen by one sensor; a common example is occlusion which is recorded separately for each camera.May also be referred to as source-, stream- or sensor-specific attributes.3D geometry is anything that can be drawn when annotating a pointcloud, e.g. cuboids.Geometry-specific attributes are permitted on 2D geometry e.g. bounding boxesNote that the task definition, must designate a property as source specific before it may be used in this way.The stream attribute is a special case and is excepted from this rule "},{"title":"Supported pre-annotation features​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#supported-pre-annotation-features","content":""},{"title":"Geometries​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#geometries","content":"note Objects cannot have multiple 3D geometries in the same frame Name\tOpenLABEL field\tDescriptionCuboid\tcuboid\tCuboid in 3D Bounding box\tbbox\tBounding box in 2D 3D line\tpoly3d\tLine in 3D. Append the first point at the end if you want it to be closed. Polygon\tpoly2d\tPolygon in 2D. The property is_hole is mandatory. Multi-polygon\tpoly2d\tMulti-polygon in 2D. The properties is_hole and polygon_id are mandatory. Note that all geometries should be specified under frames rather than in the root of the pre-annotation. 3D geometries should be expressed in the lidar coordinate system in the single-lidar case, but in the reference coordinate system in the multi-lidar case. The rotation of cuboids should be the same as that in exports. 2D geometries should be expressed in pixel coordinates. See coordinate systems for more information. "},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes","content":"TextNumBoolean For 2D geometry, attributes may be specified as geometry specific (aka source/sensor specific), or object specific. Attributes can be static (specified in the objects key) or dynamic (specified in the object_data for the object in the frame) and must be allowed by the task definition, if one exists. Geometry specific attributes (those which appear on a single shape within frames) must also be declared as such in the task definition; arbitrary properties cannot be used in a source-specific way. "},{"title":"Contexts​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#contexts","content":"Currently not supported. Contact Kognic if you need support for this or use regular attributes instead. "},{"title":"Frames​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#frames","content":"Every pre-annotation must contain frames with unique timestamps that are among the ones specified in the scene. The reason for this is that the timestamps are used to map the frame in the pre-annotation to the correct frame in the scene. In the static case, one frame should be used with timestamp 0. "},{"title":"Relations​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#relations","content":"Currently not supported. Contact Kognic if you need support for this or use regular attributes instead. "},{"title":"Streams​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#streams","content":"Every geometry must have the stream property specified. This property determines which stream (or sensor) that the geometry appears in. It is important that the stream is among the ones specified in the scene and of the same type, for example camera or lidar. "},{"title":"Sparseness​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness","content":"Pre-annotations can be sparse, meaning that its objects or geometries do not need to be present in every frame. Instead, they can be present in a subset of frames and then interpolated in the frames in between. Utilizing this feature can speed up the annotation process significantly for sequences. Sparseness can be accomplished in two different ways, either by using object data pointers or the boolean property interpolated. The former is the recommended way of doing it in most cases since it will lead to a more compact pre-annotation. The latter is useful when the pre-annotation is created from exported annotations from the Kognic platform. Interpolation is done by linearly interpolating the geometry values between key frames. This is done in pixel coordinates for 2D geometries. For 3D geometries, the interpolation can be done in either the frame local coordinate system or the world coordinate system (see Coordinate Systems). This is configured in the annotation instruction so reach out to the Kognic team about this if you are unsure. Note that interpolation in the world coordinate system is recommended but requires that the scene contains ego poses. "},{"title":"Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#object-data-pointers","content":"In OpenLABEL, object data pointers are used to create a specification for objects. For example, you can specify what attributes and geometries that are used for specific objects. In addition, you can specify for which frames that these are present. If a geometry is specified in the object data pointer, it will be present in all frames that the object data pointer is pointing to. If the geometry is not provided in some of these frames, it will be interpolated. Note that geometries mustbe provided for the first and last frame in the object data pointer. Otherwise, the pre-annotation will be rejected. One limitation is that a geometry must be in the same stream for all frames when using object data pointers. This is because interpolation is done in the stream coordinate system. If you need to use geometries of the same type in different streams, you can simply use different names for the geometries in the different streams. Sparseness with Object Data Pointers shows an example of how to use object data pointers. "},{"title":"Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#interpolated-property","content":"The boolean property interpolated can be used to specify that a geometry should be interpolated. Geometries are still required to be present in interpolated frames but their geometry values will be ignored. Note that interpolated geometries must have corresponding geometries (interpolated or not) in the first and last frame of the pre-annotation. Otherwise, the pre-annotation will be rejected. Using the interpolated property is the recommended way of doing it when the pre-annotation is created from exported annotations from the Kognic platform. Sparseness with Interpolated Property shows an example of how to use the interpolated property. "},{"title":"Attributes​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#attributes-1","content":"Attributes are handled differently compared to geometries. If an attribute is not present in a frame, its last value will simply be used if the object (or geometry if the property is source-specific) is present in the frame. If the object is not present in the frame, the attribute will be ignored. Dense attributes will be sparsified automatically when the pre-annotation is uploaded to the Kognic platform. "},{"title":"Examples​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#examples","content":"Below follows examples of supported pre-annotations. "},{"title":"3D cuboid and 2D bounding box with a static property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-cuboid-and-2d-bounding-box-with-a-static-property","content":"{ &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot;, &quot;streams&quot;: {&quot;LIDAR1&quot;: {}, &quot;ZFC&quot;: {}} }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;ZFC&quot; } ] }, &quot;name&quot;: &quot;Bounding-box-1&quot;, &quot;val&quot;: [1.0, 1.0, 40.0, 30.0] } ], &quot;cuboid&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;LIDAR1&quot; } ] }, &quot;name&quot;: &quot;cuboid-89ac8a2b&quot;, &quot;val&quot;: [ 2.079312801361084, -18.919870376586914, 0.3359137773513794, -0.002808041640852679, 0.022641949116037438, 0.06772797660868829, 0.9974429197838155, 1.767102435869269, 4.099334155319101, 1.3691029802958168 ] } ] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;, &quot;object_data&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;color&quot;, &quot;val&quot;: &quot;red&quot; } ] }, &quot;type&quot;: &quot;PassengerCar&quot; } }, &quot;streams&quot;: { &quot;LIDAR1&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"3D line with a dynamic property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#3d-line-with-a-dynamic-property","content":"{ &quot;openlabel&quot;: { &quot;frame_intervals&quot;: [{ &quot;frame_end&quot;: 0, &quot;frame_start&quot;: 0 }], &quot;frames&quot;: { &quot;0&quot;: { &quot;frame_properties&quot;: { &quot;streams&quot;: { &quot;lidar&quot;: {} }, &quot;timestamp&quot;: 0, &quot;external_id&quot;: &quot;0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; }] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;line-3d-1&quot;, &quot;val&quot;: [ -5.0, 0.0, 0.0, -5.0, 10.0, 0.0, 5.0, 10.0, 0.0, 5.0, 0.0, 0.0, -5.0, 0.0, 0.0 ] } ], &quot;text&quot;: [{ &quot;name&quot;: &quot;occluded&quot;, &quot;val&quot;: &quot;No&quot; }] } } } } }, &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; }, &quot;objects&quot;: { &quot;cc06aced-d7dc-4638-a6e9-dc7f5e215340&quot;: { &quot;name&quot;: &quot;cc06aced&quot;, &quot;type&quot;: &quot;Region&quot; } }, &quot;streams&quot;: { &quot;lidar&quot;: { &quot;type&quot;: &quot;lidar&quot; }, &quot;ZFC&quot;: { &quot;type&quot;: &quot;camera&quot; } } } }  "},{"title":"Sparseness with Object Data Pointers​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-object-data-pointers","content":"In the example below the object 1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box called the-bbox-name that is provided in frames 0 and 3. In frames 1 and 2, the bounding box will be interpolated. { &quot;openlabel&quot;: { &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;name&quot;: &quot;car-name&quot;, &quot;type&quot;: &quot;car&quot;, &quot;object_data_pointers&quot;: { &quot;the-bbox-name&quot;: { &quot;type&quot;:&quot;bbox&quot;, &quot;frame_intervals&quot;: [{&quot;frame_start&quot;: 0, &quot;frame_end&quot;: 3}] } } } }, &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } }, &quot;1&quot;: {}, &quot;2&quot;: {}, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [{&quot;name&quot;: &quot;the-bbox-name&quot;,...}] } } } } } } }  "},{"title":"Sparseness with Interpolated Property​","type":1,"pageTitle":"Pre-annotations","url":"/docs/kognic-io/pre_annotations#sparseness-with-interpolated-property","content":"In the example below sparseness is determined using the interpolated property. The object1232b4f4-e3ca-446a-91cb-d8d403703df7 has a bounding box for which the interpolated property is set to true in frames 1 and 2 but not in frames 0 and 3. The geometry values in frames 1 and 2 are ignored and instead interpolated from the geometry values in frames 0 and 3. { &quot;openlabel&quot;: { ..., &quot;frames&quot;: { &quot;0&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } }, &quot;1&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;2&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: true }] }, ... } ] } } } }, &quot;3&quot;: { ..., &quot;objects&quot;: { &quot;1232b4f4-e3ca-446a-91cb-d8d403703df7&quot;: { &quot;object_data&quot;: { &quot;bbox&quot;: [ { &quot;attributes&quot;: { &quot;stream&quot;: [{ &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;CAM&quot; }], &quot;boolean&quot;: [{ &quot;name&quot;: &quot;interpolated&quot;, &quot;val&quot;: false }] }, ... } ] } } } } } } }  "},{"title":"Projects","type":0,"sectionRef":"#","url":"/docs/kognic-io/project","content":"","keywords":""},{"title":"Project​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#project","content":"In order to create inputs via the API, a Kognic project needs to exist. Projects are configured by the Kognic Professional Services team, during the Guideline Agreement Process (GAP) of a new client engagement. "},{"title":"List Projects​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#list-projects","content":"Projects in your organization can be listed with the KognicIOClient projects = client.project.get_projects()  or with the kognicutil CLI kognicutil projects  Returns all projects. "},{"title":"Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch","content":"Input batches allow further grouping of inputs into smaller batches within a project. Specifying input batch during input creation is optional, and will otherwise default to the latest open batch. Ongoing projects can benefit from using batches in two ways Group inputs collected at the same timePerform guideline or task definition changes without the need for retroactive changes. "},{"title":"Batch Status​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#batch-status","content":"Status\tDescriptionpending\tBatch has been created but is still not fully configured by Kognic. Either during project setup or requested changes open\tBatch is open for new inputs ready\tBatch has been published and no longer open for new inputs. in-progress\tKognic has started annotation of inputs within the batch. completed\tAnnotations has been completed. "},{"title":"Listing Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#listing-batches","content":"project_batches = client.project.get_project_batches(&quot;project_external_id&quot;)  Or via kognicutil CLI kognicutil projects &lt;project-external-id&gt; --get-batches  Returns all batches for the project "},{"title":"Creating Batches​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#creating-batches","content":"project_batch = client.project.create_batch(&quot;project_external_id&quot;, &quot;batch_external_id&quot;)  Creates a new batch in the open state. The new batch will contain the same Annotation Types (see Annotation Types) as the latest previous batch, which means that the process of uploading inputs will be identical between batches. This method has an optional flag publish_previous_batches which defaults to False. By setting this flag toTrue, as shown in the example below, all previous batches in the open state would be published and you would no longer be able to upload inputs to those batches. You should therefore be certain that you no longer need to upload more inputs to the previous batches if you use this flag. project_batch = client.project.create_batch(&quot;project_external_id&quot;, &quot;batch_external_id&quot;, publish_previous_batches=True)  Contact Kognic before use Kognic usually helps with creating batches before a client becomes autonomous, in order to avoid any confusion please contact Kognic before you start using this feature. "},{"title":"Publish Batch​","type":1,"pageTitle":"Projects","url":"/docs/kognic-io/project#publish-batch","content":"project_batch = client.project.publish_batch(&quot;project_external_id&quot;, &quot;batch_external_id&quot;)  Publishes the input batch, setting the state of the batch to ready. Published batches are not open for new inputs. A project with multiple open batches will require you to specify which open batch to target when creating inputs, whereas a project with a single open batch will allow you omit batch when creating inputs. "},{"title":"Images","type":0,"sectionRef":"#","url":"/docs/kognic-io/resources/images","content":"Images The API allows uploading of annotation project related data such as images and point clouds. For images, we currently support two formats: png and jpg.","keywords":""},{"title":"Point clouds","type":0,"sectionRef":"#","url":"/docs/kognic-io/resources/pointclouds","content":"","keywords":""},{"title":"PCD​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#pcd","content":"The currently supported format includes the following header: VERSION .7 FIELDS x y z intensity timestamp SIZE 4 4 4 4 8 TYPE F F F U U COUNT 1 1 1 1 1 WIDTH &lt;w&gt; HEIGHT &lt;h&gt; VIEWPOINT 0 0 0 1 0 0 0 POINTS &lt;n&gt; DATA ascii  Apart from ascii as DATA type, we also support binary and binary_compressed. Note that we currently don't support organized point clouds in the binary_compressed case, i.e. when HEIGHT is not equal to 1. "},{"title":"CSV​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#csv","content":"We currently only support the following exact header and using , as separation character (where intensity is uint8,ts_gps is an uint64 and x, y, z are all float32): ts_gps,x,y,z,intensity  All other formats will fail. "},{"title":"LAS​","type":1,"pageTitle":"Point clouds","url":"/docs/kognic-io/resources/pointclouds#las","content":"We currently support version 1.2 and point format id 3, as defined in the las 1.2 specification. All other formats will cause the conversion to fail. "},{"title":"Working with Inputs","type":0,"sectionRef":"#","url":"/docs/kognic-io/working_with_inputs","content":"","keywords":""},{"title":"Creating Inputs​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#creating-inputs","content":"note For detailed information about different input modalities, check the Input Types section. Kognic Users As a Kognic user, it is possible to specify client_organization_id to KognicIOClient constructor to create inputs on behalf of a client organization In order to create inputs, they need to be associated with a project and an input batch. Consider the following project setup: organization # root for projects └── projects ├── project-a ├── batch-1 - completed ├── batch-2 - open ├── input 0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3 ├── input 37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36 └── ... └── batch-3 - pending └── project-b ├── batch-1 └── ...  There are 2 ways to associate inputs with a project and batch: Adding inputs to the latest open batch for a projectAdding inputs to specified batch for a project The following examples all use an input of type Cameras, however the interface applies to all input types. Note that we also provide a wrapper function create_inputs to help with this process, see Creating Multiple Inputs With One Call. "},{"title":"Adding inputs to the latest open batch for a project​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#adding-inputs-to-the-latest-open-batch-for-a-project","content":"client.cameras.create( ..., project=&quot;project-a&quot;)  Will add inputs to project-a batch-2 because it's the latest open batch. "},{"title":"Adding inputs to specified batch in a project​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#adding-inputs-to-specified-batch-in-a-project","content":"client.cameras.create( ..., project=&quot;project-a&quot;, batch=&quot;batch-3&quot;)  Will add inputs to project-a batch-3. "},{"title":"Input Status​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#input-status","content":"Once an input has been created, it might be preprocessed before being made available for annotation. Also, postprocessing such as conversion to the client-specific format might take place after annotation has been performed. During this process, the status property of an input can be used to keep track of progress. Status\tDescriptionpending\tInput has been validated but the server is waiting for the associated data to be uploaded processing\tAssociated data has been uploaded and is currently being processed by the Kognic Platform, potentially performing conversion of file formats created\tInput is created and available for annotation failed\tConversion of input failed invalidated:broken-input\tInput was invalidated since it did not load invalidated:duplicate\tInput was invalidated due to being uploaded several times invalidated:incorrectly-created\tInput was invalidated because it was incorrectly created "},{"title":"List Inputs​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#list-inputs","content":"Inputs can be retrieved from the API in two ways: Filtering on a project using the get_inputs method. Additional filter parameters are also available (see table below) for querying inputs.Providing the input uuids of the inputs to be retrieved using the get_inputs_by_uuids method # List all inputs for a project client.input.get_inputs(project=&quot;project-identifier&quot;) # List all inputs for a project and a batch client.input.get_inputs(project=&quot;project-identifier&quot;, batch=&quot;batch-identifier&quot;) # List all inputs for specific input uuids input_uuids = ['cca60a67-cb68-4645-8bae-00c6e6415555', 'cc8776d0-f537-4094-8b11-8c2111741e2f', ...] client.input.get_inputs_by_uuids(input_uuids=input_uuids)  Additional filter parameters for querying inputs using get_inputs are listed below. Parameter\tDescriptionproject\tProject identifier to filter by batch\tWhich batch in the project to return inputs for external_ids\tReturn inputs matching the external_ids supplied include_invalidated\tFilters inputs based on their status, defaults to False "},{"title":"Response​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#response","content":"The response is a list of Input objects containing the following properties Property\tDescriptionuuid\tID used to identify the input within the Kognic Platform external_id\tExternal ID supplied during input creation batch\tWhich batch does the input belong to view_link\tA url to view the input in the Kognic Platform input_type\tType of input (see Input Types) status\tInput status (see Input Statuses) error_message\tIf there is an error during input creation the error message will be included, otherwise it's None annotation_types\tList of annotation types for the input "},{"title":"Invalidate Inputs​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#invalidate-inputs","content":"Invalidation of an input means that it will be removed for all annotation types. See Annotation Typeson how to remove a specific annotation types for an input. from kognic.io.model.input.invalidated_reason_input import InvalidatedReasonInput invalid_uuids = [&quot;0edb8f59-a8ea-4c9b-aebb-a3caaa6f2ba3&quot;, &quot;37d9dda4-3a29-4fcb-8a71-6bf16d5a9c36&quot;] reason = InvalidatedReasonInput.BAD_CONTENT client.input.invalidate_inputs(invalid_uuids, reason)  If issues are detected upstream related to inputs created, it is possible to invalidate inputs. Invalidated inputs will not produce annotations and any completed annotations of the input will be invalidated. Reason\tDescriptionbad-content\tInput does not load, or has erroneous metadata such as invalid calibration duplicate\tIf same input has been created several times incorrectly-created\tIf the input was unintentionally created. "},{"title":"List Invalidated Inputs​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#list-invalidated-inputs","content":"If errors are detected by Kognic, inputs will be invalidated and a reason will be supplied. project = &quot;project-identifier&quot; client.input.get_inputs(project=project, include_invalidated=True)  "},{"title":"Creating Multiple Inputs With One Call​","type":1,"pageTitle":"Working with Inputs","url":"/docs/kognic-io/working_with_inputs#creating-multiple-inputs-with-one-call","content":"note This feature is new in version 1.1.9 Since the input creation process is asynchronous, it is sometimes useful to wait for the inputs to be created before continuing. In order to do this, we provide a wrapper function create_inputs which can create multiple scenes and inputs, wait for them to be created (or failed) and yield the results. The function will block until it has a result to yield or all of the inputs have completed in one way or another. The function takes a list of SceneWithPreannotation(a new wrapper object containing a scene and optionally a pre-annotation) along with the normal input creation parameters. from kognic.io.tools.input_creation import create_inputs, SceneWithPreAnnotation, InputCreationStatus from kognic.io.model.scene import LidarsAndCamerasSequence from kognic.openlabel.models import OpenLabelAnnotation scenes_with_pre_annotations: List[SceneWithPreAnnotation] = [ SceneWithPreAnnotation( scene=LidarsAndCamerasSequence(...), preannotation=OpenLabelAnnotation(...) # Optional ), ... ] for input_result in create_inputs(client, scenes_with_pre_annotations, &quot;project-identifier&quot;, batch=&quot;batch-identifier&quot;): # Do something with the result if input_result.status == InputCreationStatus.CREATED: print(f&quot;Input {input_result.external_id} was created, got uuid {input_result.input_uuid}&quot;) elif input_result.status == InputCreationStatus.FAILED: print(f&quot;Input {input_result.external_id} failed to be created at stage {input_result.error.stage} with error {input_result.error.message}&quot;) else: print(f&quot;Input {input_result.external_id} is in status {input_result.status}&quot;)  Note that the functions also accepts the parameters wait_timeout and sleep_time which can be used to control the wait-behavior. The wait_timeout parameter specifies the maximum time to wait for the inputs to be created/failed, whilesleep_time specifies the time to sleep between each check. Units are in seconds. The time it takes for inputs to be created depends on their size and the number of inputs to be created so the wait_timeout should be set accordingly. The default value is 30 minutes, starting from the time when all scene jobs have been committed. "},{"title":"The Python client","type":0,"sectionRef":"#","url":"/docs/openlabel/python-client","content":"The Python client Using this schema we have developed a python client kognic-openlabel which makes it easier to work with annotations. The python client models the OpenLABEL format as pydantic models. It is publicly availablehere and can be installed with pip install kognic-openlabel Since pydantic is used, the model contains validation as well as methods for serialization and deserialition. Below are examples of how you can easily change between different formats openlabel_dict = { &quot;openlabel&quot;: { &quot;metadata&quot;: { &quot;schema_version&quot;: &quot;1.0.0&quot; } } } from kognic.openlabel.models import OpenLabelAnnotation # Deserialize dict openlabel_annotation = OpenLabelAnnotation.parse_obj(openlabel_dict) # Serialize to json openlabel_json = openlabel_annotation.json(exclude_none=True) # Deserialize json openlabel_annotation = OpenLabelAnnotation.parse_raw(openlabel_json) # Serialize to dict openlabel_dict = openlabel_annotation.dict(exclude_none=True) ","keywords":""},{"title":"OpenLABEL format","type":0,"sectionRef":"#","url":"/docs/openlabel/openlabel-format","content":"","keywords":""},{"title":"Rotation of Cuboids​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#rotation-of-cuboids","content":"The rotation is such that the y-axis is facing forwards, with a rotation order of XYZ. This means that a cuboid with a heading (yaw) equal to 0 is aligned with the y-axis in the positive direction along the axis. This is somewhat different compared to theISO 8855standard, where the forward direction is along the x-axis. Conversion to ISO 8855 can then be done by applying a rotation around the z-axis and changing sx and sy in the following way import math from typing import List from scipy.spatial.transform import Rotation def convert_to_iso8855(val: List[float]) -&gt; List[float]: &quot;&quot;&quot; Converts cuboid values to ISO 8855 &quot;&quot;&quot; [x, y, z, qx, qy, qz, qw, sx, sy, sz] = val rotation_1 = Rotation.from_quat([qx, qy, qz, qw]) rotation_2 = Rotation.from_rotvec([0, 0, math.pi / 2]) rot_object = rotation_1 * rotation_2 [qx, qy, qz, qw] = rot_object.as_quat() return [x, y, z, qx, qy, qz, qw, sy, sx, sz]  "},{"title":"Non-sequences are sequences with one frame​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#non-sequences-are-sequences-with-one-frame","content":"Due to reasons of simplicity we have made the choice to treat non-sequences in the same way as sequences. This means that non-sequences are represented as a sequence with only one frame. Only data such as name and type are defined in the top level element keys. All other information is stored under frames, see example below { &quot;objects&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot; } }, &quot;frames&quot;: { &quot;0&quot;: { &quot;objects&quot;: { &quot;0&quot;: {&quot;object_data&quot;: {...}} } } } }  "},{"title":"Stream is just another text property​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#stream-is-just-another-text-property","content":"The stream property is used to indicate which stream/sensor/source that the geometry och property was annotated in. For example here is an object with a point that has been annotated in a stream with the name Camera. Note that all corresponding attributes for the geometry have also been annotated in the same stream. { &quot;object_data&quot;: { &quot;point2d&quot;: [ { &quot;name&quot;: &quot;point-4d2d325f&quot;, &quot;val&quot;: [300.5300, 286.4396], &quot;attributes&quot;: { &quot;text&quot;: [ {&quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;Camera&quot;}, {&quot;name&quot;: &quot;Color&quot;, &quot;val&quot;: &quot;Black&quot;} ] } } ] } }  "},{"title":"Relations​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations","content":"Regarding changes on 2022-04-08 Some changes were made regarding how to represent certain types of relations on 2022-04-08. Contact Kognic in case your annotations were produced before this date, but you wish to include these changes anyways. We consider two types of relations; unidirectional relations between two objects and group relations. In addition to these, there is a need to represent false relations, i.e. relation properties that are not actually pointers to other objects but rather take values such as Inconclusive, Nothing or Unclear. "},{"title":"Relations are unidirectional​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#relations-are-unidirectional","content":"Relations are unidirectional, meaning that if an object, object1, has a relation to another object, object2, it does not mean that object2 has a relation to object1. Below follows an example where car-0 is following car-1 and it is unclear whether car-2 is following another car or not. Deprecated since 2022-04-08 Representing false relations using the relation uid is deprecated and has moved to the use of actions (see the next section) { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;car-0&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;car-1&quot;, &quot;type&quot;: &quot;Car&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;car-2&quot;, &quot;type&quot;: &quot;Car&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isFollowing&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;2&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;Unclear&quot;}] } } }  "},{"title":"Actions are used to represent false relations (new since 2022-04-08)​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#actions-are-used-to-represent-false-relations-new-since-2022-04-08","content":"In the Kognic Platform, there is support for assigning values to relations that are not actually references to other objects. Examples are Inconclusive and Nothing. Actions are used to represent these in the following way, where the name of the action determines the value and the type determines the property name. { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;Nothing&quot;, &quot;type&quot;: &quot;is_pulling_or_pushing&quot;} } }  "},{"title":"Groups are represented as actions​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#groups-are-represented-as-actions","content":"Deprecated since 2022-04-08 The group concept has been deprecated in favor of single relations between objects. This means that annotations produced after 2022-04-08 will no longer contain the group concept Group relations are relations where objects can be seen as belonging to a group. There is then a need for an abstract concept that describes the group. OpenLABEL suggests the use of actions for this in such a way that each object in the group has a relation of type isSubjectOfAction to this action. Below follows an example where two lane-0 and lane-1belong to the same road, while it is unclear whether lane-2 belongs to a road. { &quot;objects&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;lane-0&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;lane-1&quot;, &quot;type&quot;: &quot;Lane&quot;}, &quot;2&quot;: {&quot;name&quot;: &quot;lane-2&quot;, &quot;type&quot;: &quot;Lane&quot;} }, &quot;relations&quot;: { &quot;0&quot;: { &quot;name&quot;: &quot;0&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;0&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;1&quot;: { &quot;name&quot;: &quot;1&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;0&quot;}] }, &quot;2&quot;: { &quot;name&quot;: &quot;2&quot;, &quot;type&quot;: &quot;isSubjectOfAction&quot;, &quot;rdf_subjects&quot;: [{&quot;type&quot;: &quot;object&quot;, &quot;uid&quot;: &quot;1&quot;}], &quot;rdf_objects&quot;: [{&quot;type&quot;: &quot;action&quot;, &quot;uid&quot;: &quot;1&quot;}] } }, &quot;actions&quot;: { &quot;0&quot;: {&quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;Road&quot;}, &quot;1&quot;: {&quot;name&quot;: &quot;Unclear&quot;, &quot;type&quot;: &quot;Road&quot;} } }  "},{"title":"Representing polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-polygons","content":"Polygons are described by a list of Poly2d objects in OpenLABEL. One of these represents the exterior while the others represent potential holes and this is determined by the boolean property is_hole. Below follows an example of a polygon with one hole. { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). "},{"title":"Representing multi-polygons​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-multi-polygons","content":"Multi-polygons are simply lists of polygons, so we describe these in a similar way with lists of Poly2d objects with the property is_hole. However, we also add one additional property polygon_id that determines which polygon a Poly2d object belongs to in the multi-polygon. Below follows an example of a multi-polygon with two polygons with one hole each. { &quot;object_data&quot;: { &quot;poly2d&quot;: [ { &quot;name&quot;: &quot;poly1&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly2&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;1&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } }, { &quot;name&quot;: &quot;poly3&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: false}] } }, { &quot;name&quot;: &quot;poly4&quot;, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [{&quot;name&quot;: &quot;polygon_id&quot;, &quot;val&quot;: &quot;2&quot;}], &quot;boolean&quot;: [{&quot;name&quot;: &quot;is_hole&quot;, &quot;val&quot;: true}] } } ] } }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). "},{"title":"Representing curves​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-curves","content":"Curves are represented using the poly2d geometry and the interpolation method is specified as a text property in the following way. { &quot;poly2d&quot;: [ { &quot;closed&quot;: false, &quot;mode&quot;: &quot;MODE_POLY2D_ABSOLUTE&quot;, &quot;name&quot;: &quot;curve-d633ca89&quot;, &quot;val&quot;: [...], &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;interpolation-method&quot;, &quot;val&quot;: &quot;natural-cubic-spline&quot; } ] } } ] }  The value MODE_POLY2D_ABSOLUTE is the only supported value for mode. Absolute mode means that the values in val are interpreted as pixel coordinates (not as values relative to the first coordinate pair). "},{"title":"Representing 3D lanes​","type":1,"pageTitle":"OpenLABEL format","url":"/docs/openlabel/openlabel-format#representing-3d-lanes","content":"A 3D lane is represented as two lines in 3D (poly3d), one to the right and the other to the left. The text propertylane_edge determines whether the line is to the right or to the left. The lines will always have closed set to false. { &quot;object_data&quot;: { &quot;poly3d&quot;: [ { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;left&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.2647494200238287, -51.51747573498745, -2.315540290283199, 1.0807419132566136, -48.91298533071834, -2.313640304199211, -0.0892715141237751, -34.705936676401016, -2.235569814758307, -0.4442893388935316, -29.60917111552865, -2.1894531147766174, -1.0952988968721313, -17.193981050037397, -2.1397902661132875 ] }, { &quot;attributes&quot;: { &quot;text&quot;: [ { &quot;name&quot;: &quot;lane_edge&quot;, &quot;val&quot;: &quot;right&quot; }, { &quot;name&quot;: &quot;stream&quot;, &quot;val&quot;: &quot;lidar&quot; } ] }, &quot;closed&quot;: false, &quot;name&quot;: &quot;&quot;, &quot;val&quot;: [ 1.5845765823868767, -51.49487958011918, -2.315540290283199, 1.4004322100638888, -48.888528958803036, -2.313640304199211, 0.23043085215069048, -34.68163859008775, -2.235569814758307, -0.12426061849402326, -29.589636067040036, -2.1894531147766174 ] } ] } }  "}]